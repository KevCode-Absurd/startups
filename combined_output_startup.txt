Introduction Stop me if you’ve heard this one before. Brilliant college kids sitting in a dorm are inventing the future. Heedless of boundaries, possessed of new technology and youthful enthusiasm, they build a new company from scratch. Their early success allows them to raise money and bring an amazing new product to market. They hire their friends, assemble a superstar team, and dare the world to stop them.

Ten years and several startups ago, that was me, building my first company. I particularly remember a moment from back then: the moment I realized my company was going to fail. My cofounder and I were at our wits’ end. The dot-com bubble had burst, and we had spent all our money. We tried desperately to raise more capital, and we could not. It was like a breakup scene from a Hollywood movie: it was raining, and we were arguing in the street. We couldn’t even agree on where to walk next, and so we parted in anger, heading in opposite directions. As a metaphor for our company’s failure, this image of the two of us, lost in the rain and drifting apart, is perfect. It remains a painful memory. The company limped along for months afterward, but our situation was hopeless. At the time, it had seemed we were doing everything right: we had a great product, a brilliant team, amazing technology, and the right idea at the right time. And we really were on to something. We were building a way for college kids to create online pro les for the purpose of sharing … with employers. Oops. But despite a promising idea, we were nonetheless doomed from day one, because we did not know the process we would need to use to turn our product insights into a great company.

If you’ve never experienced a failure like this, it is hard to describe the feeling. It’s as if the world were falling out from under you. You realize you’ve been duped. The stories in the magazines are lies: hard work and perseverance don’t lead to success. Even worse, the many, many, many promises you’ve made to employees, friends, and family are not going to come true. Everyone who thought you were foolish for stepping out on your own will be proven right. It wasn’t supposed to turn out that way. In magazines and newspapers, in blockbuster movies, and on countless blogs, we hear the mantra of the successful entrepreneurs: through determination, brilliance, great timing, and—above all—a great product, you too can achieve fame and fortune. There is a mythmaking industry hard at work to sell us that story, but I have come to believe that the story is false, the product of selection bias and after-the-fact rationalization. In fact, having worked with hundreds of entrepreneurs, I have seen rsthand how often a promising start leads to failure. 

The grim reality is that most startups fail. Most new products are not successful. Most new ventures do not live up to their potential. Yet the story of perseverance, creative genius, and hard work persists. Why is it so popular? I think there is something deeply appealing about this modern-day rags-to-riches story. It makes success seem inevitable if you just have the right stu . It means that the mundane details, the boring stu , the small individual choices don’t matter. If we build it, they will come. When we fail, as so many of us do, we have a ready-made excuse: we didn’t have the right stu . We weren’t visionary enough or weren’t in the right place at the right time. After more than ten years as an entrepreneur, I came to reject that line of thinking. I have learned from both my own successes and failures and those of many others that it’s the boring stu that matters the most. Startup success is not a consequence of good genes or being in the right place at the right time. 

Startup success can be engineered by following the right process, which means it can be engineered by following the right process, which means it can be learned, which means it can be taught. Entrepreneurship is a kind of management. No, you didn’t read that wrong. We have wildly divergent associations with these two words, entrepreneurship and management. Lately, it seems that one is cool, innovative, and exciting and the other is dull, serious, and bland. It is time to look past these preconceptions. Let me tell you a second startup story. It’s 2004, and a group of founders have just started a new company. 

Their previous company had failed very publicly. Their credibility is at an all-time low. They have a huge vision: to change the way people communicate by using a new technology called avatars (remember, this was before James Cameron’s blockbuster movie). They are following a visionary named Will Harvey, who paints a compelling picture: people connecting with their friends, hanging out online, using avatars to give them a combination of intimate connection and safe anonymity. Even better, instead of having to build all the clothing, furniture, and accessories these avatars would need to accessorize their digital lives, the customers would be enlisted to build those things and sell them to one another.

The engineering challenge before them is immense: creating virtual worlds, user-generated content, an online commerce engine, micropayments, and—last but not least—the three-dimensional avatar technology that can run on anyone’s PC. I’m in this second story, too. I’m a cofounder and chief technology o cer of this company, which is called IMVU. At this point in our careers, my cofounders and I are determined to make new mistakes. We do everything wrong: instead of spending years perfecting our technology, we build a minimum viable product, an early product that is terrible, full of bugs and crash-your-computer-yes-really stability problems. Then we ship it to customers way before it’s ready. And we charge money for it. After securing initial customers, we change the product constantly—much too fast by traditional standards—shipping new versions of our product dozens of times every single day. We really did have customers in those early days—true visionary early adopters—and we often talked to them and asked for their early adopters—and we often talked to them and asked for their feedback. But we emphatically did not do what they said. We viewed their input as only one source of information about our product and overall vision. In fact, we were much more likely to run experiments on our customers than we were to cater to their whims. Traditional business thinking says that this approach shouldn’t work, but it does, and you don’t have to take my word for it. As you’ll see throughout this book, the approach we pioneered at IMVU has become the basis for a new movement of entrepreneurs around the world. It builds on many previous management and product development ideas, including lean manufacturing, design thinking, customer development, and agile development. It represents a new approach to creating continuous innovation. It’s called the Lean Startup. Despite the volumes written on business strategy, the key attributes of business leaders, and ways to identify the next big thing, innovators still struggle to bring their ideas to life. This was the frustration that led us to try a radical new approach at IMVU, one characterized by an extremely fast cycle time, a focus on what customers want (without asking them), and a scienti c approach to making decisions. ORIGINS OF THE LEAN STARTUP I am one of those people who grew up programming computers, and so my journey to thinking about entrepreneurship and management has taken a circuitous path. I have always worked on the product development side of my industry; my partners and bosses were managers or marketers, and my peers worked in engineering and operations. Throughout my career, I kept having the experience of working incredibly hard on products that ultimately failed in the marketplace. At first, largely because of my background, I viewed these as technical problems that required technical solutions: better architecture, a better engineering process, better discipline, focus, or product vision. These supposed xes led to still more failure. So I read everything I could get my hands on and was blessed to have had some of the top minds in Silicon Valley as my mentors. By the time I became a cofounder of IMVU, I was hungry for new ideas about how to build a company. I was fortunate to have cofounders who were willing to experiment with new approaches. They were fed up—as I was—by the failure of traditional thinking. Also, we were lucky to have Steve Blank as an investor and adviser. Back in 2004, Steve had just begun preaching a new idea: the business and marketing functions of a startup should be considered as important as engineering and product development and therefore deserve an equally rigorous methodology to guide them. He called that methodology Customer Development, and it o ered insight and guidance to my daily work as an entrepreneur. Meanwhile, I was building IMVU’s product development team, using some of the unorthodox methods I mentioned earlier. Measured against the traditional theories of product development I had been trained on in my career, these methods did not make sense, yet I could see rsthand that they were working. I struggled to explain the practices to new employees, investors, and the founders of other companies. We lacked a common language for describing them and concrete principles for understanding them. I began to search outside entrepreneurship for ideas that could help me make sense of my experience. I began to study other industries, especially manufacturing, from which most modern theories of management derive. I studied lean manufacturing, a process that originated in Japan with the Toyota Production System, a completely new way of thinking about the manufacturing of physical goods. I found that by applying ideas from lean manufacturing to my own entrepreneurial challenges—with a few tweaks and changes—I had the beginnings of a framework for making sense of them.


This line of thought evolved into the Lean Startup: the application of lean thinking to the process of innovation. IMVU became a tremendous success. IMVU customers have created more than 60 million avatars. It is a pro table company with annual revenues of more than $50 million in 2011, employing more than a hundred people in our current o ces in Mountain View, California. IMVU’s virtual goods catalog—which seemed so risky years ago—now has more than 6 million items in it; more than 7,000 are added every day, almost all created by customers. As a result of IMVU’s success, I began to be asked for advice by other startups and venture capitalists. When I would describe my experiences at IMVU, I was often met with blank stares or extreme skepticism. The most common reply was “That could never work!” My experience so ew in the face of conventional thinking that most people, even in the innovation hub of Silicon Valley, could not wrap their minds around it. Then I started to write, rst on a blog called Startup Lessons Learned, and speak—at conferences and to companies, startups, and venture capitalists—to anyone who would listen. In the process of being called on to defend and explain my insights and with the collaboration of other writers, thinkers, and entrepreneurs, I had a chance to re ne and develop the theory of the Lean Startup beyond its rudimentary beginnings. My hope all along was to nd ways to eliminate the tremendous waste I saw all around me: startups that built products nobody wanted, new products pulled from the shelves, countless dreams unrealized. Eventually, the Lean Startup idea blossomed into a global movement. Entrepreneurs began forming local in-person groups to discuss and apply Lean Startup ideas. There are now organized communities of practice in more than a hundred cities around the world.1 My travels have taken me across countries and continents. Everywhere I have seen the signs of a new entrepreneurial renaissance. The Lean Startup movement is making entrepreneurship accessible to a whole new generation of founders who are hungry for new ideas about how to build successful companies. Although my background is in high-tech software entrepreneurship, the movement has grown way beyond those entrepreneurship, the movement has grown way beyond those roots. Thousands of entrepreneurs are putting Lean Startup principles to work in every conceivable industry. I’ve had the chance to work with entrepreneurs in companies of all sizes, in di erent industries, and even in government. This journey has taken me to places I never imagined I’d see, from the world’s most elite venture capitalists, to Fortune 500 boardrooms, to the Pentagon. The most nervous I have ever been in a meeting was when I was attempting to explain Lean Startup principles to the chief information o cer of the U.S. Army, who is a three-star general (for the record, he was extremely open to new ideas, even from a civilian like me). Pretty soon I realized that it was time to focus on the Lean Startup movement full time. My mission: to improve the success rate of new innovative products worldwide. The result is the book you are reading. THE LEAN STARTUP METHOD This is a book for entrepreneurs and the people who hold them accountable. The ve principles of the Lean Startup, which inform all three parts of this book, are as follows: 1. Entrepreneurs are everywhere. You don’t have to work in a garage to be in a startup. The concept of entrepreneurship includes anyone who works within my de nition of a startup: a human institution designed to create new products and services under conditions of extreme uncertainty. That means entrepreneurs are everywhere and the Lean Startup approach can work in any size company, even a very large enterprise, in any sector or industry. 2. Entrepreneurship is management. A startup is an institution, not just a product, and so it requires a new kind of management speci cally geared to its context of extreme uncertainty. In fact, as I will argue later, I believe “entrepreneur” should be considered a will argue later, I believe “entrepreneur” should be considered a job title in all modern companies that depend on innovation for their future growth. 3. Validated learning. Startups exist not just to make stu , make money, or even serve customers. They exist to learn how to build a sustainable business. This learning can be validated scienti cally by running frequent experiments that allow entrepreneurs to test each element of their vision. 4. Build-Measure-Learn. The fundamental activity of a startup is to turn ideas into products, measure how customers respond, and then learn whether to pivot or persevere. All successful startup processes should be geared to accelerate that feedback loop. 5. Innovation accounting. To improve entrepreneurial outcomes and hold innovators accountable, we need to focus on the boring stu : how to measure progress, how to set up milestones, and how to prioritize work. This requires a new kind of accounting designed for startups—and the people who hold them accountable. Why Startups Fail Why are startups failing so badly everywhere we look? The rst problem is the allure of a good plan, a solid strategy, and thorough market research. In earlier eras, these things were indicators of likely success. The overwhelming temptation is to apply them to startups too, but this doesn’t work, because startups operate with too much uncertainty. Startups do not yet know who their customer is or what their product should be. As the world becomes more uncertain, it gets harder and harder to predict the future. The old management methods are not up to the task. Planning and forecasting are only accurate when based on a long, stable operating history and a relatively static environment. Startups have neither. The second problem is that after seeing traditional management fail to solve this problem, some entrepreneurs and investors have thrown up their hands and adopted the “Just Do It” school of startups. This school believes that if management is the problem, chaos is the answer. Unfortunately, as I can attest rsthand, this doesn’t work either. It may seem counterintuitive to think that something as disruptive, innovative, and chaotic as a startup can be managed or, to be accurate, must be managed. Most people think of process and management as boring and dull, whereas startups are dynamic and exciting. But what is actually exciting is to see startups succeed and change the world. The passion, energy, and vision that people bring to these new ventures are resources too precious to waste. We can— and must—do better. This book is about how.

HOW THIS BOOK IS ORGANIZED This book is divided into three parts: “Vision,” “Steer,” and “Accelerate.” “Vision” makes the case for a new discipline of entrepreneurial management. I identify who is an entrepreneur, de ne a startup, and articulate a new way for startups to gauge if they are making progress, called validated learning. To achieve that learning, we’ll see that startups—in a garage or inside an enterprise—can use scientific experimentation to discover how to build a sustainable business. “Steer” dives into the Lean Startup method in detail, showing one major turn through the core Build-Measure-Learn feedback loop. Beginning with leap-of-faith assumptions that cry out for rigorous testing, you’ll learn how to build a minimum viable product to test those assumptions, a new accounting system for evaluating whether you’re making progress, and a method for deciding whether to pivot (changing course with one foot anchored to the ground) or persevere. In “Accelerate,” we’ll explore techniques that enable Lean Startups to speed through the Build-Measure-Learn feedback loop as quickly as possible, even as they scale. We’ll explore lean manufacturing concepts that are applicable to startups, too, such as the power of small batches. We’ll also discuss organizational design, how products grow, and how to apply Lean Startup principles beyond the proverbial garage, even inside the world’s largest companies.

MANAGEMENT’S SECOND CENTURY As a society, we have a proven set of techniques for managing big companies and we know the best practices for building physical products. But when it comes to startups and innovation, we are still shooting in the dark. We are relying on vision, chasing the “great

shooting in the dark. We are relying on vision, chasing the “great men” who can make magic happen, or trying to analyze our new products to death. These are new problems, born of the success of management in the twentieth century. This book attempts to put entrepreneurship and innovation on a rigorous footing. We are at the dawn of management’s second century. It is our challenge to do something great with the opportunity we have been given. The Lean Startup movement seeks to ensure that those of us who long to build the next big thing will have the tools we need to change the world.

Part One

Part One VISION

1 START ENTREPRENEURIAL MANAGEMENT uilding a startup is an exercise in institution building; thus, it necessarily involves management. This often comes as a surprise to aspiring entrepreneurs, because their associations with these two words are so diametrically opposed. Entrepreneurs are rightly wary of implementing traditional management practices early on in a startup, afraid that they will invite bureaucracy or stifle creativity. Entrepreneurs have been trying to t the square peg of their unique problems into the round hole of general management for decades. As a result, many entrepreneurs take a “just do it” attitude, avoiding all forms of management, process, and discipline. Unfortunately, this approach leads to chaos more often than it does to success. I should know: my rst startup failures were all of this kind. The tremendous success of general management over the last century has provided unprecedented material abundance, but those management principles are ill suited to handle the chaos and uncertainty that startups must face.

B

I believe that entrepreneurship requires a managerial discipline to harness the entrepreneurial opportunity we have been given. There are more entrepreneurs operating today than at any previous time in history. This has been made possible by dramatic

previous time in history. This has been made possible by dramatic changes in the global economy. To cite but one example, one often hears commentators lament the loss of manufacturing jobs in the United States over the previous two decades, but one rarely hears about a corresponding loss of manufacturing capability. That’s because total manufacturing output in the United States is increasing (by 15 percent in the last decade) even as jobs continue to be lost (see the charts below). In e ect, the huge productivity increases made possible by modern management and technology have created more productive capacity than rms know what to do with.1 We are living through an unprecedented worldwide entrepreneurial renaissance, but this opportunity is laced with peril. Because we lack a coherent management paradigm for new innovative ventures, we’re throwing our excess capacity around with wild abandon. Despite this lack of rigor, we are nding some ways to make money, but for every success there are far too many failures: products pulled from shelves mere weeks after being launched, high-pro le startups lauded in the press and forgotten a few months later, and new products that wind up being used by nobody. What makes these failures particularly painful is not just the economic damage done to individual employees, companies, and investors; they are also a colossal waste of our civilization’s most precious resource: the time, passion, and skill of its people. The Lean Startup movement is dedicated to preventing these failures.

THE ROOTS OF THE LEAN STARTUP The Lean Startup takes its name from the lean manufacturing revolution that Taiichi Ohno and Shigeo Shingo are credited with developing at Toyota. Lean thinking is radically altering the way supply chains and production systems are run. Among its tenets are drawing on the knowledge and creativity of individual workers, the shrinking of batch sizes, just-in-time production and inventory control, and an acceleration of cycle times. It taught the world the di erence between value-creating activities and waste and showed how to build quality into products from the inside out. The Lean Startup adapts these ideas to the context of entrepreneurship, proposing that entrepreneurs judge their progress di erently from the way other kinds of ventures do. Progress in

manufacturing is measured by the production of high-quality physical goods. As we’ll see in Chapter 3, the Lean Startup uses a di erent unit of progress, called validated learning. With scientific learning as our yardstick, we can discover and eliminate the sources of waste that are plaguing entrepreneurship. A comprehensive theory of entrepreneurship should address all the functions of an early-stage venture: vision and concept, product development, marketing and sales, scaling up, partnerships and distribution, and structure and organizational design. It has to provide a method for measuring progress in the context of extreme uncertainty. It can give entrepreneurs clear guidance on how to make the many trade-o decisions they face: whether and when to invest in process; formulating, planning, and creating infrastructure; when to go it alone and when to partner; when to respond to feedback and when to stick with vision; and how and when to invest in scaling the business. Most of all, it must allow entrepreneurs to make testable predictions. For example, consider the recommendation that you build crossfunctional teams and hold them accountable to what we call learning milestones instead of organizing your company into strict functional departments (marketing, sales, information technology, human resources, etc.) that hold people accountable for performing well in their specialized areas (see Chapter 7). Perhaps you agree with this recommendation, or perhaps you are skeptical. Either way, if you decide to implement it, I predict that you pretty quickly will get feedback from your teams that the new process is reducing their productivity. They will ask to go back to the old way of working, in which they had the opportunity to “stay e cient” by working in larger batches and passing work between departments. It’s safe to predict this result, and not just because I have seen it many times in the companies I work with. It is a straightforward prediction of the Lean Startup theory itself. When people are used to evaluating their productivity locally, they feel that a good day is one in which they did their job well all day. When I worked as a programmer, that meant eight straight hours of programming without interruption. That was a good day. In contrast, if I was

without interruption. That was a good day. In contrast, if I was interrupted with questions, process, or—heaven forbid—meetings, I felt bad. What did I really accomplish that day? Code and product features were tangible to me; I could see them, understand them, and show them off. Learning, by contrast, is frustratingly intangible. The Lean Startup asks people to start measuring their productivity di erently. Because startups often accidentally build something nobody wants, it doesn’t matter much if they do it on time and on budget. The goal of a startup is to gure out the right thing to build—the thing customers want and will pay for—as quickly as possible. In other words, the Lean Startup is a new way of looking at the development of innovative new products that emphasizes fast iteration and customer insight, a huge vision, and great ambition, all at the same time. Henry Ford is one of the most successful and celebrated entrepreneurs of all time. Since the idea of management has been bound up with the history of the automobile since its rst days, I believe it is tting to use the automobile as a metaphor for a startup. An internal combustion automobile is powered by two important and very di erent feedback loops. The rst feedback loop is deep inside the engine. Before Henry Ford was a famous CEO, he was an engineer. He spent his days and nights tinkering in his garage with the precise mechanics of getting the engine cylinders to move. Each tiny explosion within the cylinder provides the motive force to turn the wheels but also drives the ignition of the next explosion. Unless the timing of this feedback loop is managed precisely, the engine will sputter and break down. Startups have a similar engine that I call the engine of growth. The markets and customers for startups are diverse: a toy company, a consulting rm, and a manufacturing plant may not seem like they have much in common, but, as we’ll see, they operate with the same engine of growth. Every new version of a product, every new feature, and every

Every new version of a product, every new feature, and every new marketing program is an attempt to improve this engine of growth. Like Henry Ford’s tinkering in his garage, not all of these changes turn out to be improvements. New product development happens in ts and starts. Much of the time in a startup’s life is spent tuning the engine by making improvements in product, marketing, or operations. The second important feedback loop in an automobile is between the driver and the steering wheel. This feedback is so immediate and automatic that we often don’t think about it, but it is steering that di erentiates driving from most other forms of transportation. If you have a daily commute, you probably know the route so well that your hands seem to steer you there on their own accord. We can practically drive the route in our sleep. Yet if I asked you to close your eyes and write down exactly how to get to your o ce—not the street directions but every action you need to take, every push of hand on wheel and foot on pedals—you’d nd it impossible. The choreography of driving is incredibly complex when one slows down to think about it. By contrast, a rocket ship requires just this kind of in-advance calibration. It must be launched with the most precise instructions on what to do: every thrust, every ring of a booster, and every change in direction. The tiniest error at the point of launch could yield catastrophic results thousands of miles later. Unfortunately, too many startup business plans look more like they are planning to launch a rocket ship than drive a car. They prescribe the steps to take and the results to expect in excruciating detail, and as in planning to launch a rocket, they are set up in such a way that even tiny errors in assumptions can lead to catastrophic outcomes. One company I worked with had the misfortune of forecasting signi cant customer adoption—in the millions—for one of its new products. Powered by a splashy launch, the company successfully executed its plan. Unfortunately, customers did not ock to the product in great numbers. Even worse, the company had invested in massive infrastructure, hiring, and support to handle the in ux of customers it expected. When the customers failed to materialize, the

customers it expected. When the customers failed to materialize, the company had committed itself so completely that they could not adapt in time. They had “achieved failure”—successfully, faithfully, and rigorously executing a plan that turned out to have been utterly flawed. The Lean Startup method, in contrast, is designed to teach you how to drive a startup. Instead of making complex plans that are based on a lot of assumptions, you can make constant adjustments with a steering wheel called the Build-Measure-Learn feedback loop. Through this process of steering, we can learn when and if it’s time to make a sharp turn called a pivot or whether we should persevere along our current path. Once we have an engine that’s revved up, the Lean Startup o ers methods to scale and grow the business with maximum acceleration. Throughout the process of driving, you always have a clear idea of where you’re going. If you’re commuting to work, you don’t give up because there’s a detour in the road or you made a wrong turn. You remain thoroughly focused on getting to your destination. Startups also have a true north, a destination in mind: creating a thriving and world-changing business. I call that a startup’s vision. To achieve that vision, startups employ a strategy, which includes a business model, a product road map, a point of view about partners and competitors, and ideas about who the customer will be. The product is the end result of this strategy (see the chart on this page).

Products change constantly through the process of optimization, what I call tuning the engine. Less frequently, the strategy may have to change (called a pivot). However, the overarching vision rarely changes. Entrepreneurs are committed to seeing the startup through to that destination. Every setback is an opportunity for learning how to get where they want to go (see the chart below).

In real life, a startup is a portfolio of activities. A lot is happening simultaneously: the engine is running, acquiring new customers and serving existing ones; we are tuning, trying to improve our product, marketing, and operations; and we are steering, deciding if and when to pivot. The challenge of entrepreneurship is to balance all these activities. Even the smallest startup faces the challenge of supporting existing customers while trying to innovate. Even the most established company faces the imperative to invest in innovation lest it become obsolete. As companies grow, what changes is the mix of these activities in the company’s portfolio of work. Entrepreneurship is management. And yet, imagine a modern manager who is tasked with building a new product in the context of an established company. Imagine that she goes back to her company’s chief nancial o cer (CFO) a year later and says, “We have failed to meet the growth targets we predicted. In fact, we have almost no new customers and no new revenue. However, we have learned an incredible amount and are on the cusp of a breakthrough new line of business. All we need is another year.” Most of the time, this would be the last report this intrapreneur would give her employer. The reason is that in general management, a failure to deliver results is due to either a failure to plan adequately or a failure to execute properly. Both are signi cant lapses, yet new product development in our modern economy routinely requires exactly this kind of failure on the way to greatness. In the Lean Startup movement, we have come to realize that these internal innovators are actually entrepreneurs, too, and that entrepreneurial management can help them succeed; this is the subject of the next chapter.

2 DEFINE WHO, EXACTLY, IS AN ENTREPRENEUR? s I travel the world talking about the Lean Startup, I’m consistently surprised that I meet people in the audience who seem out of place. In addition to the more traditional startup entrepreneurs I meet, these people are general managers, mostly working in very large companies, who are tasked with creating new ventures or product innovations. They are adept at organizational politics: they know how to form autonomous divisions with separate pro t and loss statements (P&Ls) and can shield controversial teams from corporate meddling. The biggest surprise is that they are visionaries. Like the startup founders I have worked with for years, they can see the future of their industries and are prepared to take bold risks to seek out new and innovative solutions to the problems their companies face. Mark, for example, is a manager for an extremely large company who came to one of my lectures. He is the leader of a division that recently had been chartered to bring his company into the twentyrst century by building a new suite of products designed to take advantage of the Internet. When he came to talk to me afterward, I started to give him the standard advice about how to create innovation teams inside big companies, and he stopped me in midstream: “Yeah, I’ve read The Innovator’s Dilemma.1 I’ve got that all taken care of.” He was a long-term employee of the company and a successful manager to boot, so managing internal politics was

A

and a successful manager to boot, so managing internal politics was the least of his problems. I should have known; his success was a testament to his ability to navigate the company’s corporate policies, personnel, and processes to get things done. Next, I tried to give him some advice about the future, about cool new highly leveraged product development technologies. He interrupted me again: “Right. I know all about the Internet, and I have a vision for how our company needs to adapt to it or die.” Mark has all the entrepreneurial prerequisites nailed—proper team structure, good personnel, a strong vision for the future, and an appetite for risk taking—and so it nally occurred to me to ask why he was coming to me for advice. He said, “It’s as if we have all of the raw materials: kindling, wood, paper, int, even some sparks. But where’s the re?” The theories of management that Mark had studied treat innovation like a “black box” by focusing on the structures companies need to put in place to form internal startup teams. But Mark found himself working inside the black box—and in need of guidance. What Mark was missing was a process for converting the raw materials of innovation into real-world breakthrough successes. Once a team is set up, what should it do? What process should it use? How should it be held accountable to performance milestones? These are questions the Lean Startup methodology is designed to answer. My point? Mark is an entrepreneur just like a Silicon Valley hightech founder with a garage startup. He needs the principles of the Lean Startup just as much as the folks I thought of as classic entrepreneurs do. Entrepreneurs who operate inside an established organization sometimes are called “intrapreneurs” because of the special circumstances that attend building a startup within a larger company. As I have applied Lean Startup ideas in an ever-widening variety of companies and industries, I have come to believe that intrapreneurs have much more in common with the rest of the community of entrepreneurs than most people believe. Thus, when I use the term entrepreneur, I am referring to the whole startup ecosystem regardless of company size, sector, or stage of

ecosystem regardless of company size, sector, or stage of development. This book is for entrepreneurs of all stripes: from young visionaries with little backing but great ideas to seasoned visionaries within larger companies such as Mark—and the people who hold them accountable. IF I’M AN ENTREPRENEUR, WHAT’S A STARTUP? The Lean Startup is a set of practices for helping entrepreneurs increase their odds of building a successful startup. To set the record straight, it’s important to define what a startup is: A startup is a human institution designed to create a new product or service under conditions of extreme uncertainty. I’ve come to realize that the most important part of this de nition is what it omits. It says nothing about size of the company, the industry, or the sector of the economy. Anyone who is creating a new product or business under conditions of extreme uncertainty is an entrepreneur whether he or she knows it or not and whether working in a government agency, a venture-backed company, a nonpro t, or a decidedly for-pro t company with financial investors. Let’s take a look at each of the pieces. The word institution connotes bureaucracy, process, even lethargy. How can that be part of a startup? Yet successful startups are full of activities associated with building an institution: hiring creative employees, coordinating their activities, and creating a company culture that delivers results. We often lose sight of the fact that a startup is not just about a product, a technological breakthrough, or even a brilliant idea. A startup is greater than the sum of its parts; it is an acutely human enterprise. The fact that a startup’s product or service is a new innovation is also an essential part of the de nition and a tricky part too. I prefer to use the broadest de nition of product, one that encompasses any

to use the broadest de nition of product, one that encompasses any source of value for the people who become customers. Anything those customers experience from their interaction with a company should be considered part of that company’s product. This is true of a grocery store, an e-commerce website, a consulting service, and a nonpro t social service agency. In every case, the organization is dedicated to uncovering a new source of value for customers and cares about the impact of its product on those customers. It’s also important that the word innovation be understood broadly. Startups use many kinds of innovation: novel scienti c discoveries, repurposing an existing technology for a new use, devising a new business model that unlocks value that was hidden, or simply bringing a product or service to a new location or a previously underserved set of customers. In all these cases, innovation is at the heart of the company’s success. There is one more important part of this definition: the context in which the innovation happens. Most businesses—large and small alike—are excluded from this context. Startups are designed to confront situations of extreme uncertainty. To open up a new business that is an exact clone of an existing business all the way down to the business model, pricing, target customer, and product may be an attractive economic investment, but it is not a startup because its success depends only on execution—so much so that this success can be modeled with high accuracy. (This is why so many small businesses can be nanced with simple bank loans; the level of risk and uncertainty is understood well enough that a loan o cer can assess its prospects.) Most tools from general management are not designed to ourish in the harsh soil of extreme uncertainty in which startups thrive. The future is unpredictable, customers face a growing array of alternatives, and the pace of change is ever increasing. Yet most startups—in garages and enterprises alike—still are managed by using standard forecasts, product milestones, and detailed business plans. THE SNAPTAX STORY

In 2009, a startup decided to try something really audacious. They wanted to liberate taxpayers from expensive tax stores by automating the process of collecting information typically found on W-2 forms (the end-of-year statement that most employees receive from their employer that summarizes their taxable wages for the year). The startup quickly ran into di culties. Even though many consumers had access to a printer/scanner in their home or o ce, few knew how to use those devices. After numerous conversations with potential customers, the team lit upon the idea of having customers take photographs of the forms directly from their cell phone. In the process of testing this concept, customers asked something unexpected: would it be possible to nish the whole tax return right on the phone itself? That was not an easy task. Traditional tax preparation requires consumers to wade through hundreds of questions, many forms, and a lot of paperwork. This startup tried something novel by deciding to ship an early version of its product that could do much less than a complete tax package. The initial version worked only for consumers with a very simple return to le, and it worked only in California. Instead of having consumers ll out a complex form, they allowed the customers to use the phone’s camera to take a picture of their W-2 forms. From that single picture, the company developed the technology to compile and le most of the 1040 EZ tax return. Compared with the drudgery of traditional tax ling, the new product—called SnapTax—provides a magical experience. From its modest beginning, SnapTax grew into a signi cant startup success story. Its nationwide launch in 2011 showed that customers loved it, to the tune of more than 350,000 downloads in the rst three weeks. This is the kind of amazing innovation you’d expect from a new startup. However, the name of this company may surprise you. SnapTax was developed by Intuit, America’s largest producer of nance, tax, and accounting tools for individuals and small businesses. With

more than 7,700 employees and annual revenues in the billions, Intuit is not a typical startup.2 The team that built SnapTax doesn’t look much like the archetypal image of entrepreneurs either. They don’t work in a garage or eat ramen noodles. Their company doesn’t lack for resources. They are paid a full salary and benefits. They come into a regular office every day. Yet they are entrepreneurs. Stories like this one are not nearly as common inside large corporations as they should be. After all, SnapTax competes directly with one of Intuit’s agship products: the fully featured TurboTax desktop software. Usually, companies like Intuit fall into the trap described in Clayton Christensten’s The Innovator’s Dilemma: they are very good at creating incremental improvements to existing products and serving existing customers, which Christensen called sustaining innovation, but struggle to create breakthrough new products—disruptive innovation—that can create new sustainable sources of growth. One remarkable part of the SnapTax story is what the team leaders said when I asked them to account for their unlikely success. Did they hire superstar entrepreneurs from outside the company? No, they assembled a team from within Intuit. Did they face constant meddling from senior management, which is the bane of innovation teams in many companies? No, their executive sponsors created an “island of freedom” where they could experiment as necessary. Did they have a huge team, a large budget, and lots of marketing dollars? Nope, they started with a team of five. What allowed the SnapTax team to innovate was not their genes, destiny, or astrological signs but a process deliberately facilitated by Intuit’s senior management. Innovation is a bottoms-up, decentralized, and unpredictable thing, but that doesn’t mean it cannot be managed. It can, but to do so requires a new management discipline, one that needs to be mastered not just by practicing entrepreneurs seeking to build the next big thing but also by the people who support them, nurture them, and hold them accountable. In other words, cultivating entrepreneurship is the

accountable. In other words, cultivating entrepreneurship is the responsibility of senior management. Today, a cutting-edge company such as Intuit can point to success stories like SnapTax because it has recognized the need for a new management paradigm. This is a realization that was years in the making.3 A SEVEN-THOUSAND-PERSON LEAN STARTUP In 1983, Intuit’s founder, the legendary entrepreneur Scott Cook, had the radical notion (with cofounder Tom Proulx) that personal accounting should happen by computer. Their success was far from inevitable; they faced numerous competitors, an uncertain future, and an initially tiny market. A decade later, the company went public and subsequently fended o well-publicized attacks from larger incumbents, including the software behemoth Microsoft. Partly with the help of famed venture capitalist John Doerr, Intuit became a fully diversi ed enterprise, a member of the Fortune 1000 that now provides dozens of market-leading products across its major divisions. This is the kind of entrepreneurial success we’re used to hearing about: a ragtag team of underdogs who eventually achieve fame, acclaim, and significant riches. Flash-forward to 2002. Cook was frustrated. He had just tabulated ten years of data on all of Intuit’s new product introductions and had concluded that the company was getting a measly return on its massive investments. Simply put, too many of its new products were failing. By traditional standards, Intuit is an extremely wellmanaged company, but as Scott dug into the root causes of those failures, he came to a di cult conclusion: the prevailing management paradigm he and his company had been practicing was inadequate to the problem of continuous innovation in the modern economy. By fall 2009, Cook had been working to change Intuit’s management culture for several years. He came across my early work on the Lean Startup and asked me to give a talk at Intuit. In Silicon Valley this is not the kind of invitation you turn down. I

Silicon Valley this is not the kind of invitation you turn down. I admit I was curious. I was still at the beginning of my Lean Startup journey and didn’t have much appreciation for the challenges faced by a Fortune 1000 company like his. My conversations with Cook and Intuit chief executive o cer (CEO) Brad Smith were my initiation into the thinking of modern general managers, who struggle with entrepreneurship every bit as much as do venture capitalists and founders in a garage. To combat these challenges, Scott and Brad are going back to Intuit’s roots. They are working to build entrepreneurship and risk taking into all their divisions. For example, consider one of Intuit’s agship products. Because TurboTax does most of its sales around tax season in the United States, it used to have an extremely conservative culture. Over the course of the year, the marketing and product teams would conceive one major initiative that would be rolled out just in time for tax season. Now they test over ve hundred di erent changes in a two-and-a-half-month tax season. They’re running up to seventy di erent tests per week. The team can make a change live on its website on Thursday, run it over the weekend, read the results on Monday, and come to conclusions starting Tuesday; then they rebuild new tests on Thursday and launch the next set on Thursday night. As Scott put it, “Boy, the amount of learning they get is just immense now. And what it does is develop entrepreneurs, because when you have only one test, you don’t have entrepreneurs, you have politicians, because you have to sell. Out of a hundred good ideas, you’ve got to sell your idea. So you build up a society of politicians and salespeople. When you have ve hundred tests you’re running, then everybody’s ideas can run. And then you create entrepreneurs who run and learn and can retest and relearn as opposed to a society of politicians. So we’re trying to drive that throughout our organization, using examples which have nothing to do with high tech, like the website example. Every business today has a website. You don’t have to be high tech to use fast-cycle testing.” This kind of change is hard. After all, the company has a

signi cant number of existing customers who continue to demand exceptional service and investors who expect steady, growing returns. Scott says, It goes against the grain of what people have been taught in business and what leaders have been taught. The problem isn’t with the teams or the entrepreneurs. They love the chance to quickly get their baby out into the market. They love the chance to have the customer vote instead of the suits voting. The real issue is with the leaders and the middle managers. There are many business leaders who have been successful because of analysis. They think they’re analysts, and their job is to do great planning and analyzing and have a plan. The amount of time a company can count on holding on to market leadership to exploit its earlier innovations is shrinking, and this creates an imperative for even the most entrenched companies to invest in innovation. In fact, I believe a company’s only sustainable path to long-term economic growth is to build an “innovation factory” that uses Lean Startup techniques to create disruptive innovations on a continuous basis. In other words, established companies need to gure out how to accomplish what Scott Cook did in 1983, but on an industrial scale and with an established cohort of managers steeped in traditional management culture. Ever the maverick, Cook asked me to put these ideas to the test, and so I gave a talk that was simulcast to all seven thousand–plus Intuit employees during which I explained the theory of the Lean Startup, repeating my de nition: an organization designed to create new products and services under conditions of extreme uncertainty. What happened next is etched in my memory. CEO Brad Smith had been sitting next to me as I spoke. When I was done, he got up and said before all of Intuit’s employees, “Folks, listen up. You

heard Eric’s definition of a startup. It has three parts, and we here at Intuit match all three parts of that definition.” Scott and Brad are leaders who realize that something new is needed in management thinking. Intuit is proof that this kind of thinking can work in established companies. Brad explained to me how they hold themselves accountable for their new innovation e orts by measuring two things: the number of customers using products that didn’t exist three years ago and the percentage of revenue coming from offerings that did not exist three years ago. Under the old model, it took an average of 5.5 years for a successful new product to start generating $50 million in revenue. Brad explained to me, “We’ve generated $50 million in o erings that did not exist twelve months ago in the last year. Now it’s not one particular o ering. It’s a combination of a whole bunch of innovation happening, but that’s the kind of stu that’s creating some energy for us, that we think we can truly short-circuit the ramp by killing things that don’t make sense fast and doubling down on the ones that do.” For a company as large as Intuit, these are modest results and early days. They have decades of legacy systems and legacy thinking to overcome. However, their leadership in adopting entrepreneurial management is starting to pay off. Leadership requires creating conditions that enable employees to do the kinds of experimentation that entrepreneurship requires. For example, changes in TurboTax enabled the Intuit team to develop ve hundred experiments per tax season. Before that, marketers with great ideas couldn’t have done those tests even if they’d wanted to, because they didn’t have a system in place through which to change the website rapidly. Intuit invested in systems that increased the speed at which tests could be built, deployed, and analyzed. As Cook says, “Developing these experimentation systems is the responsibility of senior management; they have to be put in by the leadership. It’s moving leaders from playing Caesar with their thumbs up and down on every idea to—instead—putting in the culture and the systems so that teams can move and innovate at the speed of the experimentation system.”

3 LEARN an entrepreneur, nothing plagued me more than the question Asofsuccessful whether my company was making progress toward creating a business. As an engineer and later as a manager, I was

accustomed to measuring progress by making sure our work proceeded according to plan, was high quality, and cost about what we had projected. After many years as an entrepreneur, I started to worry about measuring progress in this way. What if we found ourselves building something that nobody wanted? In that case what did it matter if we did it on time and on budget? When I went home at the end of a day’s work, the only things I knew for sure were that I had kept people busy and spent money that day. I hoped that my team’s e orts took us closer to our goal. If we wound up taking a wrong turn, I’d have to take comfort in the fact that at least we’d learned something important. Unfortunately, “learning” is the oldest excuse in the book for a failure of execution. It’s what managers fall back on when they fail to achieve the results we promised. Entrepreneurs, under pressure to succeed, are wildly creative when it comes to demonstrating what we have learned. We can all tell a good story when our job, career, or reputation depends on it. However, learning is cold comfort to employees who are following an entrepreneur into the unknown. It is cold comfort to the investors who allocate precious money, time, and energy to entrepreneurial teams. It is cold comfort to the organizations—large

entrepreneurial teams. It is cold comfort to the organizations—large and small—that depend on entrepreneurial innovation to survive. You can’t take learning to the bank; you can’t spend it or invest it. You cannot give it to customers and cannot return it to limited partners. Is it any wonder that learning has a bad name in entrepreneurial and managerial circles? Yet if the fundamental goal of entrepreneurship is to engage in organization building under conditions of extreme uncertainty, its most vital function is learning. We must learn the truth about which elements of our strategy are working to realize our vision and which are just crazy. We must learn what customers really want, not what they say they want or what we think they should want. We must discover whether we are on a path that will lead to growing a sustainable business. In the Lean Startup model, we are rehabilitating learning with a concept I call validated learning. Validated learning is not after-thefact rationalization or a good story designed to hide failure. It is a rigorous method for demonstrating progress when one is embedded in the soil of extreme uncertainty in which startups grow. Validated learning is the process of demonstrating empirically that a team has discovered valuable truths about a startup’s present and future business prospects. It is more concrete, more accurate, and faster than market forecasting or classical business planning. It is the principal antidote to the lethal problem of achieving failure: successfully executing a plan that leads nowhere. VALIDATED LEARNING AT IMVU Let me illustrate this with an example from my career. Many audiences have heard me recount the story of IMVU’s founding and the many mistakes we made in developing our rst product. I’ll now elaborate on one of those mistakes to illustrate validated learning clearly. Those of us involved in the founding of IMVU aspired to be serious strategic thinkers. Each of us had participated in previous ventures that had failed, and we were loath to repeat that

ventures that had failed, and we were loath to repeat that experience. Our main concerns in the early days dealt with the following questions: What should we build and for whom? What market could we enter and dominate? How could we build durable value that would not be subject to erosion by competition?1 Brilliant Strategy We decided to enter the instant messaging (IM) market. In 2004, that market had hundreds of millions of consumers actively participating worldwide. However, the majority of the customers who were using IM products were not paying for the privilege. Instead, large media and portal companies such as AOL, Microsoft, and Yahoo! operated their IM networks as a loss leader for other services while making modest amounts of money through advertising. IM is an example of a market that involves strong network effects. Like most communication networks, IM is thought to follow Metcalfe’s law: the value of a network as a whole is proportional to the square of the number of participants. In other words, the more people in the network, the more valuable the network. This makes intuitive sense: the value to each participant is driven primarily by how many other people he or she can communicate with. Imagine a world in which you own the only telephone; it would have no value. Only when other people also have a telephone does it become valuable. In 2004, the IM market was locked up by a handful of incumbents. The top three networks controlled more than 80 percent of the overall usage and were in the process of consolidating their gains in market share at the expense of a number of smaller players.2 The common wisdom was that it was more or less impossible to bring a new IM network to market without spending an extraordinary amount of money on marketing. The reason for that wisdom is simple. Because of the power of network e ects, IM products have high switching costs. To switch from one network to another, customers would have to convince

from one network to another, customers would have to convince their friends and colleagues to switch with them. This extra work for customers creates a barrier to entry in the IM market: with all consumers locked in to an incumbent’s product, there are no customers left with whom to establish a beachhead. At IMVU we settled on a strategy of building a product that would combine the large mass appeal of traditional IM with the high revenue per customer of three-dimensional (3D) video games and virtual worlds. Because of the near impossibility of bringing a new IM network to market, we decided to build an IM add-on product that would interoperate with the existing networks. Thus, customers would be able to adopt the IMVU virtual goods and avatar communication technology without having to switch IM providers, learn a new user interface, and—most important—bring their friends with them. In fact, we thought this last point was essential. For the add-on product to be useful, customers would have to use it with their existing friends. Every communication would come embedded with an invitation to join IMVU. Our product would be inherently viral, spreading throughout the existing IM networks like an epidemic. To achieve that viral growth, it was important that our add-on product support as many of the existing IM networks as possible and work on all kinds of computers. Six Months to Launch With this strategy in place, my cofounders and I began a period of intense work. As the chief technology o cer, it was my responsibility, among other things, to write the software that would support IM interoperability across networks. My cofounders and I worked for months, putting in crazy hours struggling to get our rst product released. We gave ourselves a hard deadline of six months —180 days—to launch the product and attract our rst paying customers. It was a grueling schedule, but we were determined to launch on time. The add-on product was so large and complex and had so many

moving parts that we had to cut a lot of corners to get it done on time. I won’t mince words: the rst version was terrible. We spent endless hours arguing about which bugs to x and which we could live with, which features to cut and which to try to cram in. It was a wonderful and terrifying time: we were full of hope about the possibilities for success and full of fear about the consequences of shipping a bad product. Personally, I was worried that the low quality of the product would tarnish my reputation as an engineer. People would think I didn’t know how to build a quality product. All of us feared tarnishing the IMVU brand; after all, we were charging people money for a product that didn’t work very well. We all envisioned the damning newspaper headlines: “Inept Entrepreneurs Build Dreadful Product.” As launch day approached, our fears escalated. In our situation, many entrepreneurial teams give in to fear and postpone the launch date. Although I understand this impulse, I am glad we persevered, since delay prevents many startups from getting the feedback they need. Our previous failures made us more afraid of another, even worse, outcome than shipping a bad product: building something that nobody wants. And so, teeth clenched and apologies at the ready, we released our product to the public. Launch And then—nothing happened! It turned out that our fears were unfounded, because nobody even tried our product. At rst I was relieved because at least nobody was nding out how bad the product was, but soon that gave way to serious frustration. After all the hours we had spent arguing about which features to include and which bugs to x, our value proposition was so far o that customers weren’t getting far enough into the experience to nd out how bad our design choices were. Customers wouldn’t even download our product. Over the ensuing weeks and months, we labored to make the

product better. We brought in a steady ow of customers through our online registration and download process. We treated each day’s customers as a brand-new report card to let us know how we were doing. We eventually learned how to change the product’s positioning so that customers at least would download it. We were making improvements to the underlying product continuously, shipping bug xes and new changes daily. However, despite our best e orts, we were able to persuade only a pathetically small number of people to buy the product. In retrospect, one good decision we made was to set clear revenue targets for those early days. In the rst month we intended to make $300 in total revenue, and we did—barely. Many friends and family members were asked (okay, begged). Each month our small revenue targets increased, rst to $350 and then to $400. As they rose, our struggles increased. We soon ran out of friends and family; our frustration escalated. We were making the product better every day, yet our customers’ behavior remained unchanged: they still wouldn’t use it. Our failure to move the numbers prodded us to accelerate our e orts to bring customers into our o ce for in-person interviews and usability tests. The quantitative targets created the motivation to engage in qualitative inquiry and guided us in the questions we asked; this is a pattern we’ll see throughout this book. I wish I could say that I was the one to realize our mistake and suggest the solution, but in truth, I was the last to admit the problem. In short, our entire strategic analysis of the market was utterly wrong. We gured this out empirically, through experimentation, rather than through focus groups or market research. Customers could not tell us what they wanted; most, after all, had never heard of 3D avatars. Instead, they revealed the truth through their action or inaction as we struggled to make the product better. Talking to Customers

Out of desperation, we decided to talk to some potential customers. We brought them into our o ce, and said, “Try this new product; it’s IMVU.” If the person was a teenager, a heavy user of IM, or a tech early adopter, he or she would engage with us. In constrast, if it was a mainstream person, the response was, “Right. So exactly what would you like me to do?” We’d get nowhere with the mainstream group; they thought IMVU was too weird. Imagine a seventeen-year-old girl sitting down with us to look at this product. She chooses her avatar and says, “Oh, this is really fun.” She’s customizing the avatar, deciding how she’s going to look. Then we say, “All right, it’s time to download the instant messaging add-on,” and she responds, “What’s that?” “Well, it’s this thing that interoperates with the instant messaging client.” She’s looking at us and thinking, “I’ve never heard of that, my friends have never heard of that, why do you want me to do that?” It required a lot of explanation; an instant messaging add-on was not a product category that existed in her mind. But since she was in the room with us, we were able to talk her into doing it. She downloads the product, and then we say, “Okay, invite one of your friends to chat.” And she says, “No way!” We say, “Why not?” And she says, “Well, I don’t know if this thing is cool yet. You want me to risk inviting one of my friends? What are they going to think of me? If it sucks, they’re going to think I suck, right?” And we say, “No, no, it’s going to be so much fun once you get the person in there; it’s a social product.” She looks at us, her face lled with doubt; you can see that this is a deal breaker. Of course, the rst time I had that experience, I said, “It’s all right, it’s just this one person, send her away and get me a new one.” Then the second customer comes in and says the same thing. Then the third customer comes in, and it’s the same thing. You start to see patterns, and no matter how stubborn you are, there’s obviously something wrong. Customers kept saying, “I want to use it by myself. I want to try it out rst to see if it’s really cool before I invite a friend.” Our team was from the video game industry, so we understood what that meant: single-player mode. So we built a single-player version.

meant: single-player mode. So we built a single-player version. We’d bring new customers into our o ce. They’d customize the avatar and download the product like before. Then they would go into single-player mode, and we’d say, “Play with your avatar and dress it up; check out the cool moves it can make.” Followed by, “Okay, you did that by yourself; now it’s time to invite one of your friends.” You can see what’s coming. They’d say, “No way! This isn’t cool.” And we’d say, “Well, we told you it wasn’t going to be cool! What is the point of a single-player experience for a social product?” See, we thought we should get a gold star just for listening to our customers. Except our customers still didn’t like the product. They would look at us and say, “Listen, old man, you don’t understand. What is the deal with this crazy business of inviting friends before I know if it’s cool?” It was a total deal breaker. Out of further desperation, we introduced a feature called ChatNow that allows you to push a button and be randomly matched with somebody else anywhere in the world. The only thing you have in common is that you both pushed the button at the same time. All of a sudden, in our customer service tests, people were saying, “Oh, this is fun!” So we’d bring them in, they’d use ChatNow, and maybe they would meet somebody they thought was cool. They’d say, “Hey, that guy was neat; I want to add him to my buddy list. Where’s my buddy list?” And we’d say, “Oh, no, you don’t want a new buddy list; you want to use your regular AOL buddy list.” Remember, this was how we planned to harness the interoperability that would lead to network e ects and viral growth. Picture the customer looking at us, asking, “What do you want me to do exactly?” And we’d say, “Well, just give the stranger your AIM screen name so you can put him on your buddy list.” You could see their eyes go wide, and they’d say, “Are you kidding me? A stranger on my AIM buddy list?” To which we’d respond, “Yes; otherwise you’d have to download a whole new IM client with a new buddy list.” And they’d say, “Do you have any idea how many IM clients I already run?” “No. One or two, maybe?” That’s how many clients each of us in the o ce used. To which the teenager would say, “Duh! I run

eight.” We had no idea how many instant messaging clients there were in the world. We had the incorrect preconception that it’s a challenge to learn new software and it’s tricky to move your friends over to a new buddy list. Our customers revealed that this was nonsense. We wanted to draw diagrams on the whiteboard that showed why our strategy was brilliant, but our customers didn’t understand concepts like network e ects and switching costs. If we tried to explain why they should behave the way we predicted, they’d just shake their heads at us, bewildered. We had a mental model for how people used software that was years out of date, and so eventually, painfully, after dozens of meetings like that, it started to dawn on us that the IM add-on concept was fundamentally flawed.3 Our customers did not want an IM add-on; they wanted a standalone IM network. They did not consider having to learn how to use a new IM program a barrier; on the contrary, our early adopters used many di erent IM programs simultaneously. Our customers were not intimidated by the idea of having to take their friends with them to a new IM network; it turned out that they enjoyed that challenge. Even more surprising, our assumption that customers would want to use avatar-based IM primarily with their existing friends was also wrong. They wanted to make new friends, an activity that 3D avatars are particularly well suited to facilitating. Bit by bit, customers tore apart our seemingly brilliant initial strategy. Throwing My Work Away Perhaps you can sympathize with our situation and forgive my obstinacy. After all, it was my work over the prior months that needed to be thrown away. I had slaved over the software that was required to make our IM program interoperate with other networks, which was at the heart of our original strategy. When it came time to pivot and abandon that original strategy, almost all of

my work—thousands of lines of code—was thrown out. I felt betrayed. I was a devotee of the latest in software development methods (known collectively as agile development), which promised to help drive waste out of product development. However, despite that, I had committed the biggest waste of all: building a product that our customers refused to use. That was really depressing. I wondered: in light of the fact that my work turned out to be a waste of time and energy, would the company have been just as well o if I had spent the last six months on a beach sipping umbrella drinks? Had I really been needed? Would it have been better if I had not done any work at all? There is, as I mentioned at the beginning of this chapter, always one last refuge for people aching to justify their own failure. I consoled myself that if we hadn’t built this rst product—mistakes and all—we never would have learned these important insights about customers. We never would have learned that our strategy was awed. There is truth in this excuse: what we learned during those critical early months set IMVU on a path that would lead to our eventual breakout success. For a time, this “learning” consolation made me feel better, but my relief was short-lived. Here’s the question that bothered me most of all: if the goal of those months was to learn these important insights about customers, why did it take so long? How much of our e ort contributed to the essential lessons we needed to learn? Could we have learned those lessons earlier if I hadn’t been so focused on making the product “better” by adding features and fixing bugs? VALUE VS. WASTE In other words, which of our e orts are value-creating and which are wasteful? This question is at the heart of the lean manufacturing revolution; it is the rst question any lean manufacturing adherent is trained to ask. Learning to see waste and then systematically

eliminate it has allowed lean companies such as Toyota to dominate entire industries. In the world of software, the agile development methodologies I had practiced until that time had their origins in lean thinking. They were designed to eliminate waste too. Yet those methods had led me down a road in which the majority of my team’s efforts were wasted. Why? The answer came to me slowly over the subsequent years. Lean thinking de nes value as providing bene t to the customer; anything else is waste. In a manufacturing business, customers don’t care how the product is assembled, only that it works correctly. But in a startup, who the customer is and what the customer might nd valuable are unknown, part of the very uncertainty that is an essential part of the de nition of a startup. I realized that as a startup, we needed a new de nition of value. The real progress we had made at IMVU was what we had learned over those rst months about what creates value for customers. Anything we had done during those months that did not contribute to our learning was a form of waste. Would it have been possible to learn the same things with less e ort? Clearly, the answer is yes. For one thing, think of all the debate and prioritization of e ort that went into features that customers would never discover. If we had shipped sooner, we could have avoided that waste. Also consider all the waste caused by our incorrect strategic assumptions. I had built interoperability for more than a dozen di erent IM clients and networks. Was this really necessary to test our assumptions? Could we have gotten the same feedback from our customers with half as many networks? With only three? With only one? Since the customers of all IM networks found our product equally unattractive, the level of learning would have been the same, but our effort would have been dramatically less. Here’s the thought that kept me up nights: did we have to support any networks at all? Is it possible that we could have discovered how awed our assumptions were without building anything? For example, what if we simply had o ered customers

anything? For example, what if we simply had o ered customers the opportunity to download the product from us solely on the basis of its proposed features before building anything? Remember, almost no customers were willing to use our original product, so we wouldn’t have had to do much apologizing when we failed to deliver. (Note that this is di erent from asking customers what they want. Most of the time customers don’t know what they want in advance.) We could have conducted an experiment, o ering customers the chance to try something and then measuring their behavior. Such thought experiments were extremely disturbing to me because they undermined my job description. As the head of product development, I thought my job was to ensure the timely delivery of high-quality products and features. But if many of those features were a waste of time, what should I be doing instead? How could we avoid this waste? I’ve come to believe that learning is the essential unit of progress for startups. The e ort that is not absolutely necessary for learning what customers want can be eliminated. I call this validated learning because it is always demonstrated by positive improvements in the startup’s core metrics. As we’ve seen, it’s easy to kid yourself about what you think customers want. It’s also easy to learn things that are completely irrelevant. Thus, validated learning is backed up by empirical data collected from real customers. WHERE DO YOU FIND VALIDATION? As I can attest, anybody who fails in a startup can claim that he or she has learned a lot from the experience. They can tell a compelling story. In fact, in the story of IMVU so far, you might have noticed something missing. Despite my claims that we learned a lot in those early months, lessons that led to our eventual success, I haven’t o ered any evidence to back that up. In hindsight, it’s easy to make such claims and sound credible (and you’ll see some evidence later in the book), but imagine us in IMVU’s early months

trying to convince investors, employees, family members, and most of all ourselves that we had not squandered our time and resources. What evidence did we have? Certainly our stories of failure were entertaining, and we had fascinating theories about what we had done wrong and what we needed to do to create a more successful product. However, the proof did not come until we put those theories into practice and built subsequent versions of the product that showed superior results with actual customers. The next few months are where the true story of IMVU begins, not with our brilliant assumptions and strategies and whiteboard gamesmanship but with the hard work of discovering what customers really wanted and adjusting our product and strategy to meet those desires. We adopted the view that our job was to nd a synthesis between our vision and what customers would accept; it wasn’t to capitulate to what customers thought they wanted or to tell customers what they ought to want. As we came to understand our customers better, we were able to improve our products. As we did that, the fundamental metrics of our business changed. In the early days, despite our e orts to improve the product, our metrics were stubbornly at. We treated each day’s customers as a new report card. We’d pay attention to the percentage of new customers who exhibited product behaviors such as downloading and buying our product. Each day, roughly the same number of customers would buy the product, and that number was pretty close to zero despite the many improvements. However, once we pivoted away from the original strategy, things started to change. Aligned with a superior strategy, our product development e orts became magically more productive—not because we were working harder but because we were working smarter, aligned with our customers’ real needs. Positive changes in metrics became the quantitative validation that our learning was real. This was critically important because we could show our stakeholders—employees, investors, and ourselves—that we were making genuine progress, not deluding ourselves. It is also the right way to think about productivity in a startup: not in terms of how

much stu we are building but in terms of how much validated learning we’re getting for our efforts.4 For example, in one early experiment, we changed our entire website, home page, and product registration ow to replace “avatar chat” with “3D instant messaging.” New customers were split automatically between these two versions of the site; half saw one, and half saw the other. We were able to measure the di erence in behavior between the two groups. Not only were the people in the experimental group more likely to sign up for the product, they were more likely to become long-term paying customers. We had plenty of failed experiments too. During one period in which we believed that customers weren’t using the product because they didn’t understand its many bene ts, we went so far as to pay customer service agents to act as virtual tour guides for new customers. Unfortunately, customers who got that VIP treatment were no more likely to become active or paying customers. Even after ditching the IM add-on strategy, it still took months to understand why it hadn’t worked. After our pivot and many failed experiments, we nally gured out this insight: customers wanted to use IMVU to make new friends online. Our customers intuitively grasped something that we were slow to realize. All the existing social products online were centered on customers’ real-life identity. IMVU’s avatar technology, however, was uniquely well suited to help people get to know each other online without compromising safety or opening themselves up to identity theft. Once we formed this hypothesis, our experiments became much more likely to produce positive results. Whenever we would change the product to make it easier for people to nd and keep new friends, we discovered that customers were more likely to engage. This is true startup productivity: systematically guring out the right things to build. These were just a few experiments among hundreds that we ran week in and week out as we started to learn which customers would use the product and why. Each bit of knowledge we

gathered suggested new experiments to run, which moved our metrics closer and closer to our goal. THE AUDACITY OF ZERO Despite IMVU’s early success, our gross numbers were still pretty small. Unfortunately, because of the traditional way businesses are evaluated, this is a dangerous situation. The irony is that it is often easier to raise money or acquire other resources when you have zero revenue, zero customers, and zero traction than when you have a small amount. Zero invites imagination, but small numbers invite questions about whether large numbers will ever materialize. Everyone knows (or thinks he or she knows) stories of products that achieved breakthrough success overnight. As long as nothing has been released and no data have been collected, it is still possible to imagine overnight success in the future. Small numbers pour cold water on that hope. This phenomenon creates a brutal incentive: postpone getting any data until you are certain of success. Of course, as we’ll see, such delays have the unfortunate e ect of increasing the amount of wasted work, decreasing essential feedback, and dramatically increasing the risk that a startup will build something nobody wants. However, releasing a product and hoping for the best is not a good plan either, because this incentive is real. When we launched IMVU, we were ignorant of this problem. Our earliest investors and advisers thought it was quaint that we had a $300-per-month revenue plan at rst. But after several months with our revenue hovering around $500 per month, some began to lose faith, as did some of our advisers, employees, and even spouses. In fact, at one point, some investors were seriously recommending that we pull the product out of the market and return to stealth mode. Fortunately, as we pivoted and experimented, incorporating what we learned into our product development and marketing e orts, our numbers started to improve.

But not by much! On the one hand, we were lucky to see a growth pattern that started to look like the famous hockey stick graph. On the other hand, the graph went up only to a few thousand dollars per month. These early graphs, although promising, were not by themselves su cient to combat the loss of faith caused by our early failure, and we lacked the language of validated learning to provide an alternative concept to rally around. We were quite fortunate that some of our early investors understood its importance and were willing to look beyond our small gross numbers to see the real progress we were making. (You’ll see the exact same graphs they did in Chapter 7.) Thus, we can mitigate the waste that happens because of the audacity of zero with validated learning. What we needed to demonstrate was that our product development e orts were leading us toward massive success without giving in to the temptation to fall back on vanity metrics and “success theater”—the work we do to make ourselves look successful. We could have tried marketing gimmicks, bought a Super Bowl ad, or tried amboyant public relations (PR) as a way of juicing our gross numbers. That would have given investors the illusion of traction, but only for a short time. Eventually, the fundamentals of the business would win out and the PR bump would pass. Because we would have squandered precious resources on theatrics instead of progress, we would have been in real trouble. Sixty million avatars later, IMVU is still going strong. Its legacy is not just a great product, an amazing team, and promising nancial results but a whole new way of measuring the progress of startups. LESSONS BEYOND IMVU I have had many opportunities to teach the IMVU story as a business case ever since Stanford’s Graduate School of Business wrote an o cial study about IMVU’s early years.5 The case is now part of the entrepreneurship curriculum at several business schools, including Harvard Business School, where I serve as an

entrepreneur in residence. I’ve also told these stories at countless workshops, lectures, and conferences. Every time I teach the IMVU story, students have an overwhelming temptation to focus on the tactics it illustrates: launching a low-quality early prototype, charging customers from day one, and using low-volume revenue targets as a way to drive accountability. These are useful techniques, but they are not the moral of the story. There are too many exceptions. Not every kind of customer will accept a low-quality prototype, for example. If the students are more skeptical, they may argue that the techniques do not apply to their industry or situation, but work only because IMVU is a software company, a consumer Internet business, or a non-mission-critical application. None of these takeaways is especially useful. The Lean Startup is not a collection of individual tactics. It is a principled approach to new product development. The only way to make sense of its recommendations is to understand the underlying principles that make them work. As we’ll see in later chapters, the Lean Startup model has been applied to a wide variety of businesses and industries: manufacturing, clean tech, restaurants, and even laundry. The tactics from the IMVU story may or may not make sense in your particular business. Instead, the way forward is to learn to see every startup in any industry as a grand experiment. The question is not “Can this product be built?” In the modern economy, almost any product that can be imagined can be built. The more pertinent questions are “Should this product be built?” and “Can we build a sustainable business around this set of products and services?” To answer those questions, we need a method for systematically breaking down a business plan into its component parts and testing each part empirically. In other words, we need the scienti c method. In the Lean Startup model, every product, every feature, every marketing campaign—everything a startup does—is understood to be an experiment designed to achieve validated learning. This experimental approach works across industries and sectors, as we’ll

see in Chapter 4.

4 EXPERIMENT across many startups that are struggling to answer the Icome following questions: Which customer opinions should we listen to, if any? How should we prioritize across the many features we could build? Which features are essential to the product’s success and which are ancillary? What can be changed safely, and what might anger customers? What might please today’s customers at the expense of tomorrow’s? What should we work on next? These are some of the questions teams struggle to answer if they have followed the “let’s just ship a product and see what happens” plan. I call this the “just do it” school of entrepreneurship after Nike’s famous slogan.1 Unfortunately, if the plan is to see what happens, a team is guaranteed to succeed—at seeing what happens —but won’t necessarily gain validated learning. This is one of the most important lessons of the scienti c method: if you cannot fail, you cannot learn. FROM ALCHEMY TO SCIENCE The Lean Startup methodology reconceives a startup’s e orts as experiments that test its strategy to see which parts are brilliant and which are crazy. A true experiment follows the scienti c method. It begins with a clear hypothesis that makes predictions about what is supposed to happen. It then tests those predictions empirically. Just as scienti c experimentation is informed by theory, startup experimentation is guided by the startup’s vision. The goal of every

startup experiment is to discover how to build a sustainable business around that vision. Think Big, Start Small Zappos is the world’s largest online shoe store, with annual gross sales in excess of $1 billion. It is known as one of the most successful, customer-friendly e-commerce businesses in the world, but it did not start that way. Founder Nick Swinmurn was frustrated because there was no central online site with a great selection of shoes. He envisioned a new and superior retail experience. Swinmurn could have waited a long time, insisting on testing his complete vision complete with warehouses, distribution partners, and the promise of signi cant sales. Many early e-commerce pioneers did just that, including infamous dot-com failures such as Webvan and Pets.com. Instead, he started by running an experiment. His hypothesis was that customers were ready and willing to buy shoes online. To test it, he began by asking local shoe stores if he could take pictures of their inventory. In exchange for permission to take the pictures, he would post the pictures online and come back to buy the shoes at full price if a customer bought them online. Zappos began with a tiny, simple product. It was designed to answer one question above all: is there already su cient demand for a superior online shopping experience for shoes? However, a well-designed startup experiment like the one Zappos began with does more than test a single aspect of a business plan. In the course of testing this rst assumption, many other assumptions were tested as well. To sell the shoes, Zappos had to interact with customers: taking payment, handling returns, and dealing with customer support. This is decidedly di erent from market research. If Zappos had relied on existing market research or conducted a survey, it could have asked what customers thought they wanted. By building a product instead, albeit a simple one, the company learned much more:

1. It had more accurate data about customer demand because it was observing real customer behavior, not asking hypothetical questions. 2. It put itself in a position to interact with real customers and learn about their needs. For example, the business plan might call for discounted pricing, but how are customer perceptions of the product affected by the discounting strategy? 3. It allowed itself to be surprised when customers behaved in unexpected ways, revealing information Zappos might not have known to ask about. For example, what if customers returned the shoes? Zappos’ initial experiment provided a clear, quanti able outcome: either a su cient number of customers would buy the shoes or they would not. It also put the company in a position to observe, interact with, and learn from real customers and partners. This qualitative learning is a necessary companion to quantitative testing. Although the early e orts were decidedly small-scale, that did not prevent the huge Zappos vision from being realized. In fact, in 2009 Zappos was acquired by the e-commerce giant Amazon.com for a reported $1.2 billion.2 For Long-Term Change, Experiment Immediately Caroline Barlerin is a director in the global social innovation division at Hewlett-Packard (HP), a multinational company with more than three hundred thousand employees and more than $100 billion in annual sales. Caroline, who leads global community involvement, is a social entrepreneur working to get more of HP’s employees to take advantage of the company’s policy on volunteering. Corporate guidelines encourage every employee to spend up to four hours a month of company time volunteering in his or her community; that volunteer work could take the form of any philanthropic e ort: painting fences, building houses, or even using

philanthropic e ort: painting fences, building houses, or even using pro bono or work-based skills outside the company. Encouraging the latter type of volunteering was Caroline’s priority. Because of its talent and values, HP’s combined workforce has the potential to have a monumental positive impact. A designer could help a nonpro t with a new website design. A team of engineers could wire a school for Internet access. Caroline’s project is just beginning, and most employees do not know that this volunteering policy exists, and only a tiny fraction take advantage of it. Most of the volunteering has been of the lowimpact variety, involving manual labor, even when the volunteers were highly trained experts. Barlerin’s vision is to take the hundreds of thousands of employees in the company and transform them into a force for social good. This is the kind of corporate initiative undertaken every day at companies around the world. It doesn’t look like a startup by the conventional de nition or what we see in the movies. On the surface it seems to be suited to traditional management and planning. However, I hope the discussion in Chapter 2 has prompted you to be a little suspicious. Here’s how we might analyze this project using the Lean Startup framework. Caroline’s project faces extreme uncertainty: there had never been a volunteer campaign of this magnitude at HP before. How con dent should she be that she knows the real reasons people aren’t volunteering? Most important, how much does she really know about how to change the behavior of hundreds of thousand people in more than 170 countries? Barlerin’s goal is to inspire her colleagues to make the world a better place. Looked at that way, her plan seems full of untested assumptions—and a lot of vision. In accordance with traditional management practices, Barlerin is spending time planning, getting buy-in from various departments and other managers, and preparing a road map of initiatives for the rst eighteen months of her project. She also has a strong accountability framework with metrics for the impact her project should have on the company over the next four years. Like many entrepreneurs, she has a business plan that lays out her intentions nicely. Yet despite all that work, she is—so far—creating one-o

nicely. Yet despite all that work, she is—so far—creating one-o wins and no closer to knowing if her vision will be able to scale. One assumption, for example, might be that the company’s longstanding values included a commitment to improving the community but that recent economic trouble had resulted in an increased companywide strategic focus on short-term pro tability. Perhaps longtime employees would feel a desire to rea rm their values of giving back to the community by volunteering. A second assumption could be that they would nd it more satisfying and therefore more sustainable to use their actual workplace skills in a volunteer capacity, which would have a greater impact on behalf of the organizations to which they donated their time. Also lurking within Caroline’s plans are many practical assumptions about employees’ willingness to take the time to volunteer, their level of commitment and desire, and the way to best reach them with her message. The Lean Startup model o ers a way to test these hypotheses rigorously, immediately, and thoroughly. Strategic planning takes months to complete; these experiments could begin immediately. By starting small, Caroline could prevent a tremendous amount of waste down the road without compromising her overall vision. Here’s what it might look like if Caroline were to treat her project as an experiment. Break It Down The rst step would be to break down the grand vision into its component parts. The two most important assumptions entrepreneurs make are what I call the value hypothesis and the growth hypothesis. The value hypothesis tests whether a product or service really delivers value to customers once they are using it. What’s a good indicator that employees nd donating their time valuable? We could survey them to get their opinion, but that would not be very accurate because most people have a hard time assessing their feelings objectively.

Experiments provide a more accurate gauge. What could we see in real time that would serve as a proxy for the value participants were gaining from volunteering? We could nd opportunities for a small number of employees to volunteer and then look at the retention rate of those employees. How many of them sign up to volunteer again? When an employee voluntarily invests their time and attention in this program, that is a strong indicator that they find it valuable. For the growth hypothesis, which tests how new customers will discover a product or service, we can do a similar analysis. Once the program is up and running, how will it spread among the employees, from initial early adopters to mass adoption throughout the company? A likely way this program could expand is through viral growth. If that is true, the most important thing to measure is behavior: would the early participants actively spread the word to other employees? In this case, a simple experiment would involve taking a very small number—a dozen, perhaps—of existing long-term employees and providing an exceptional volunteer opportunity for them. Because Caroline’s hypothesis was that employees would be motivated by their desire to live up to HP’s historical commitment to community service, the experiment would target employees who felt the greatest sense of disconnect between their daily routine and the company’s expressed values. The point is not to nd the average customer but to nd early adopters: the customers who feel the need for the product most acutely. Those customers tend to be more forgiving of mistakes and are especially eager to give feedback. Next, using a technique I call the concierge minimum viable product (described in detail in Chapter 6), Caroline could make sure the first few participants had an experience that was as good as she could make it, completely aligned with her vision. Unlike in a focus group, her goal would be to measure what the customers actually did. For example, how many of the rst volunteers actually complete their volunteer assignments? How many volunteer a second time? How many are willing to recruit a colleague to

second time? How many are willing to recruit a colleague to participate in a subsequent volunteer activity? Additional experiments can expand on this early feedback and learning. For example, if the growth model requires that a certain percentage of participants share their experiences with colleagues and encourage their participation, the degree to which that takes place can be tested even with a very small sample of people. If ten people complete the rst experiment, how many do we expect to volunteer again? If they are asked to recruit a colleague, how many do we expect will do so? Remember that these are supposed to be the kinds of early adopters with the most to gain from the program. Put another way, what if all ten early adopters decline to volunteer again? That would be a highly signi cant—and very negative—result. If the numbers from such early experiments don’t look promising, there is clearly a problem with the strategy. That doesn’t mean it’s time to give up; on the contrary, it means it’s time to get some immediate qualitative feedback about how to improve the program. Here’s where this kind of experimentation has an advantage over traditional market research. We don’t have to commission a survey or nd new people to interview. We already have a cohort of people to talk to as well as knowledge about their actual behavior: the participants in the initial experiment. This entire experiment could be conducted in a matter of weeks, less than one-tenth the time of the traditional strategic planning process. Also, it can happen in parallel with strategic planning while the plan is still being formulated. Even when experiments produce a negative result, those failures prove instructive and can in uence the strategy. For example, what if no volunteers can be found who are experiencing the con ict of values within the organization that was such an important assumption in the business plan? If so, congratulations: it’s time to pivot (a concept that is explored in more detail in Chapter 8).3 AN EXPERIMENT IS A PRODUCT In the Lean Startup model, an experiment is more than just a

In the Lean Startup model, an experiment is more than just a theoretical inquiry; it is also a rst product. If this or any other experiment is successful, it allows the manager to get started with his or her campaign: enlisting early adopters, adding employees to each further experiment or iteration, and eventually starting to build a product. By the time that product is ready to be distributed widely, it will already have established customers. It will have solved real problems and o er detailed speci cations for what needs to be built. Unlike a traditional strategic planning or market research process, this speci cation will be rooted in feedback on what is working today rather than in anticipation of what might work tomorrow. To see this in action, consider an example from Kodak. Kodak’s history is bound up with cameras and lm, but today it also operates a substantial online business called Kodak Gallery. Mark Cook is Kodak Gallery’s vice president of products, and he is working to change Kodak Gallery’s culture of development to embrace experimentation. Mark explained, “Traditionally, the product manager says, ‘I just want this.’ In response, the engineer says, ‘I’m going to build it.’ Instead, I try to push my team to first answer four questions: 1. Do consumers recognize that they have the problem you are trying to solve? 2. If there was a solution, would they buy it? 3. Would they buy it from us? 4. Can we build a solution for that problem?” The common tendency of product development is to skip straight to the fourth question and build a solution before con rming that customers have the problem. For example, Kodak Gallery o ered wedding cards with gilded text and graphics on its site. Those designs were popular with customers who were getting married, and so the team redesigned the cards to be used at other special occasions, such as for holidays. The market research and design process indicated that customers would like the new cards, and that

finding justified the significant effort that went into creating them. Days before the launch, the team realized the cards were too di cult to understand from their depiction on the website; people couldn’t see how beautiful they were. They were also hard to produce. Cook realized that they had done the work backward. He explained, “Until we could gure out how to sell and make the product, it wasn’t worth spending any engineering time on.” Learning from that experience, Cook took a di erent approach when he led his team through the development of a new set of features for a product that makes it easier to share photos taken at an event. They believed that an online “event album” would provide a way for people who attended a wedding, a conference, or another gathering to share photos with other attendees. Unlike other online photo sharing services, Kodak Gallery’s event album would have strong privacy controls, assuring that the photos would be shared only with people who attended the same event. In a break with the past, Cook led the group through a process of identifying risks and assumptions before building anything and then testing those assumptions experimentally. There were two main hypotheses underlying the proposed event album: 1. The team assumed that customers would want to create the albums in the first place. 2. It assumed that event participants would upload photos to event albums created by friends or colleagues. The Kodak Gallery team built a simple prototype of the event album. It lacked many features—so many, in fact, that the team was reluctant to show it to customers. However, even at that early stage, allowing customers to use the prototype helped the team refute their hypotheses. First, creating an album was not as easy as the team had predicted; none of the early customers were able to create one. Further, customers complained that the early product version lacked essential features. Those negative results demoralized the team. The usability

problems frustrated them, as did customer complains about missing features, many of which matched the original road map. Cook explained that even though the product was missing features, the project was not a failure. The initial product— aws and all— con rmed that users did have the desire to create event albums, which was extremely valuable information. Where customers complained about missing features, this suggested that the team was on the right track. The team now had early evidence that those features were in fact important. What about features that were on the road map but that customers didn’t complain about? Maybe those features weren’t as important as they initially seemed. Through a beta launch the team continued to learn and iterate. While the early users were enthusiastic and the numbers were promising, the team made a major discovery. Through the use of online surveying tool KISSinsights, the team learned that many customers wanted to be able to arrange the order of pictures before they would invite others to contribute. Knowing they weren’t ready to launch, Cook held o his division’s general manager by explaining how iterating and experimenting before beginning the marketing campaign would yield far better results. In a world where marketing launch dates were often set months in advance, waiting until the team had really solved the problem was a break from the past. This process represented a dramatic change for Kodak Gallery; employees were used to being measured on their progress at completing tasks. As Cook says, “Success is not delivering a feature; success is learning how to solve the customer’s problem.”4 THE VILLAGE LAUNDRY SERVICE In India, due to the cost of a washing machine, less than seven percent of the population have one in their homes. Most people either hand wash their clothing at home or pay a Dhobi to do it for them. Dhobis take the clothes to the nearest river, wash them in the river water, bang them against rocks to get them clean, and hang

them to dry, which takes two to seven days. The result? Clothes are returned in about ten days and are probably not that clean. Akshay Mehra had been working at Procter & Gamble Singapore for eight years when he sensed an opportunity. As the brand manager of the Tide and Pantene brands for India and ASEAN countries, he thought he could make laundry services available to people who previously could not a ord them. Returning to India, Akshay joined the Village Laundry Services (VLS), created by Innosight Ventures. VLS began a series of experiments to test its business assumptions. For their rst experiment, VLS mounted a consumer-grade laundry machine on the back of a pickup truck parked on a street corner in Bangalore. The experiment cost less than $8,000 and had the simple goal of proving that people would hand over their laundry and pay to have it cleaned. The entrepreneurs did not clean the laundry on the truck, which was more for marketing and show, but took it o -site to be cleaned and brought it back to their customers by the end of the day. The VLS team continued the experiment for a week, parking the truck on di erent street corners, digging deeper to discover all they could about their potential customers. They wanted to know how they could encourage people to come to the truck. Did cleaning speed matter? Was cleanliness a concern? What were people asking for when they left their laundry with them? They discovered that customers were happy to give them their laundry to clean. However, those customers were suspicious of the washing machine mounted on the back of the truck, concerned that VLS would take their laundry and run. To address that concern, VLS created a slightly more substantial mobile cart that looked more like a kiosk. VLS also experimented with parking the carts in front of a local minimarket chain. Further iterations helped VLS gure out which services people were most interested in and what price they were willing to pay. They discovered that customers often wanted their clothes ironed and were willing to pay double the price to get their laundry back in four hours rather than twenty-four hours. As a result of those early experiments, VLS created an end

As a result of those early experiments, VLS created an end product that was a three-foot by four-foot mobile kiosk that included an energy-e cient, consumer-grade washing machine, a dryer, and an extra-long extension cord. The kiosk used Western detergents and was supplied daily with fresh clean water delivered by VLS. Since then, the Village Laundry Service has grown substantially, with fourteen locations operational in Bangalore, Mysore, and Mumbai. As CEO Akshay Mehra shared with me, “We have serviced 116,000 kgs. in 2010 (vs. 30,600 kg. in 2009). And almost 60 percent of the business is coming from repeat customers. We have serviced more than 10,000 customers in the past year alone across all the outlets.”5 A LEAN STARTUP IN GOVERNMENT? On July 21, 2010, President Obama signed the Dodd–Frank Wall Street Reform and Consumer Protection Act into law. One of its landmark provisions created a new federal agency, the Consumer Federal Protection Bureau (CFPB). This agency is tasked with protecting American citizens from predatory lending by nancial services companies such as credit card companies, student lenders, and payday loan o ces. The plan calls for it to accomplish this by setting up a call center where trained case workers will eld calls directly from the public. Left to its own devices, a new government agency would probably hire a large sta with a large budget to develop a plan that is expensive and time-consuming. However, the CFPB is considering doing things differently. Despite its $500 million budget and high-profile origins, the CPFB is really a startup. President Obama tasked his chief technology o cer, Aneesh Chopra, with collecting ideas for how to set up the new startup agency, and that is how I came to be involved. On one of Chopra’s visits to Silicon Valley, he invited a number of entrepreneurs to make suggestions for ways to cultivate a startup mentality in the new agency. In particular, his focus was on leveraging technology

and innovation to make the agency more e cient, cost-e ective, and thorough. My suggestion was drawn straight from the principles of this chapter: treat the CFPB as an experiment, identify the elements of the plan that are assumptions rather than facts, and gure out ways to test them. Using these insights, we could build a minimum viable product and have the agency up and running—on a micro scale— long before the official plan was set in motion. The number one assumption underlying the current plan is that once Americans know they can call the CFPB for help with nancial fraud and abuse, there will be a signi cant volume of citizens who do that. This sounds reasonable, as it is based on market research about the amount of fraud that a ects Americans each year. However, despite all that research, it is still an assumption. If the actual call volume di ers markedly from that in the plan, it will require signi cant revision. What if Americans who are subjected to nancial abuse don’t view themselves as victims and therefore don’t seek help? What if they have very di erent notions of what problems are important? What if they call the agency seeking help for problems that are outside its purview? Once the agency is up and running with a $500 million budget and a correspondingly large sta , altering the plan will be expensive and time-consuming, but why wait to get feedback? To start experimenting immediately, the agency could start with the creation of a simple hotline number, using one of the new breed of low-cost and fast setup platforms such as Twilio. With a few hours’ work, they could add simple voice prompts, o ering callers a menu of nancial problems to choose from. In the rst version, the prompts could be drawn straight from the existing research. Instead of a caseworker on the line, each prompt could o er the caller useful information about how to solve her or his problem. Instead of marketing this hotline to the whole country, the agency could run the experiment in a much more limited way: start with a small geographic area, perhaps as small as a few city blocks, and instead of paying for expensive television or radio advertising to let people know about the service, use highly targeted advertising.

Flyers on billboards, newspaper advertisements to those blocks, or specially targeted online ads would be a good start. Since the target area is so small, they could a ord to pay a premium to create a high level of awareness in the target zone. The total cost would remain quite small. As a comprehensive solution to the problem of nancial abuse, this minimum viable product is not very good compared with what a $500 million agency could accomplish. But it is also not very expensive. This product could be built in a matter of days or weeks, and the whole experiment probably would cost only a few thousand dollars. What we would learn from this experiment would be invaluable. On the basis of the selections of those rst callers, the agency could immediately start to get a sense of what kinds of problems Americans believe they have, not just what they “should” have. The agency could begin to test marketing messages: What motivates people to call? It could start to extrapolate real-world trends: What percentage of people in the target area actually call? The extrapolation would not be perfect, but it would establish a baseline behavior that would be far more accurate than market research. Most important, this product would serve as a seed that could germinate into a much more elaborate service. With this beginning, the agency could engage in a continuous process of improvement, slowly but surely adding more and better solutions. Eventually, it would sta the hotline with caseworkers, perhaps at rst addressing only one category of problems, to give the caseworkers the best chance of success. By the time the o cial plan was ready for implementation, this early service could serve as a real-world template. The CFPB is just getting started, but already they are showing signs of following an experimental approach. For example, instead of doing a geographically limited rollout, they are segmenting their rst products by use case. They have established a preliminary order of nancial products to provide consumer services for, with credit cards coming rst. As their rst experiment unfolds, they will

have the opportunity to closely monitor all of the other complaints and consumer feedback they receive. This data will in uence the depth, breadth, and sequence of future offerings. As David Forrest, the CFPB’s chief technology o cer, told me, “Our goal is to give American citizens an easy way to tell us about the problems they see out there in the consumer nancial marketplace. We have an opportunity to closely monitor what the public is telling us and react to new information. Markets change all the time and our job is to change with them.”6 The entrepreneurs and managers pro led in this book are smart, capable, and extremely results-oriented. In many cases, they are in the midst of building an organization in a way consistent with the best practices of current management thinking. They face the same challenges in both the public and private sectors, regardless of industry. As we’ve seen, even the seasoned managers and executives at the world’s best-run companies struggle to consistently develop and launch innovative new products. Their challenge is to overcome the prevailing management thinking that puts its faith in well-researched plans. Remember, planning is a tool that only works in the presence of a long and stable operating history. And yet, do any of us feel that the world around us is getting more and more stable every day? Changing such a mind-set is hard but critical to startup success. My hope is that this book will help managers and entrepreneurs make this change.

Part Two

Part Two STEER

How Vision Leads to Steering At its heart, a startup is a catalyst that transforms ideas into products. As customers interact with those products, they generate feedback and data. The feedback is both qualitative (such as what they like and don’t like) and quantitative (such as how many people use it and nd it valuable). As we saw in Part One, the products a startup builds are really experiments; the learning about how to build a sustainable business is the outcome of those experiments. For startups, that information is much more important than dollars, awards, or mentions in the press, because it can influence and reshape the next set of ideas. We can visualize this three-step process with this simple diagram:

This Build-Measure-Learn feedback loop is at the core of the Lean Startup model. In Part Two, we will examine it in great detail. Many people have professional training that emphasizes one element of this feedback loop. For engineers, it’s learning to build things as e ciently as possible. Some managers are experts at strategizing and learning at the whiteboard. Plenty of entrepreneurs focus their energies on the individual nouns: having the best product idea or the best-designed initial product or obsessing over data and metrics. The truth is that none of these activities by itself is of paramount importance. Instead, we need to focus our energies on minimizing the total time through this feedback loop. This is the essence of steering a startup and is the subject of Part Two. We will walk through a complete turn of the Build-Measure-Learn feedback loop, discussing each of the components in detail. The purpose of Part One was to explore the importance of learning as the measure of progress for a startup. As I hope is

learning as the measure of progress for a startup. As I hope is evident by now, by focusing our energies on validated learning, we can avoid much of the waste that plagues startups today. As in lean manufacturing, learning where and when to invest energy results in saving time and money. To apply the scienti c method to a startup, we need to identify which hypotheses to test. I call the riskiest elements of a startup’s plan, the parts on which everything depends, leap-of-faith assumptions. The two most important assumptions are the value hypothesis and the growth hypothesis. These give rise to tuning variables that control a startup’s engine of growth. Each iteration of a startup is an attempt to rev this engine to see if it will turn. Once it is running, the process repeats, shifting into higher and higher gears. Once clear on these leap-of-faith assumptions, the rst step is to enter the Build phase as quickly as possible with a minimum viable product (MVP). The MVP is that version of the product that enables a full turn of the Build-Measure-Learn loop with a minimum amount of e ort and the least amount of development time. The minimum viable product lacks many features that may prove essential later on. However, in some ways, creating a MVP requires extra work: we must be able to measure its impact. For example, it is inadequate to build a prototype that is evaluated solely for internal quality by engineers and designers. We also need to get it in front of potential customers to gauge their reactions. We may even need to try selling them the prototype, as we’ll soon see. When we enter the Measure phase, the biggest challenge will be determining whether the product development e orts are leading to real progress. Remember, if we’re building something that nobody wants, it doesn’t much matter if we’re doing it on time and on budget. The method I recommend is called innovation accounting, a quantitative approach that allows us to see whether our engine-tuning e orts are bearing fruit. It also allows us to create learning milestones, which are an alternative to traditional business and product milestones. Learning milestones are useful for entrepreneurs as a way of assessing their progress accurately and objectively; they are also invaluable to managers and investors who

objectively; they are also invaluable to managers and investors who must hold entrepreneurs accountable. However, not all metrics are created equal, and in Chapter 7 I’ll clarify the danger of vanity metrics in contrast to the nuts-and-bolts usefulness of actionable metrics, which help to analyze customer behavior in ways that support innovation accounting. Finally, and most important, there’s the pivot. Upon completing the Build-Measure-Learn loop, we confront the most di cult question any entrepreneur faces: whether to pivot the original strategy or persevere. If we’ve discovered that one of our hypotheses is false, it is time to make a major change to a new strategic hypothesis. The Lean Startup method builds capital-e cient companies because it allows startups to recognize that it’s time to pivot sooner, creating less waste of time and money. Although we write the feedback loop as Build-Measure-Learn because the activities happen in that order, our planning really works in the reverse order: we gure out what we need to learn, use innovation accounting to gure out what we need to measure to know if we are gaining validated learning, and then gure out what product we need to build to run that experiment and get that measurement. All of the techniques in Part Two are designed to minimize the total time through the Build-Measure-Learn feedback loop.

5 LEAP 2004, three college sophomores arrived in Silicon Valley with Intheir edgling college social network. It was live on a handful of college campuses. It was not the market-leading social network or

even the rst college social network; other companies had launched sooner and with more features. With 150,000 registered users, it made very little revenue, yet that summer they raised their rst $500,000 in venture capital. Less than a year later, they raised an additional $12.7 million. Of course, by now you’ve guessed that these three college sophomores were Mark Zuckerberg, Dustin Moskovitz, and Chris Hughes of Facebook. Their story is now world famous. Many things about it are remarkable, but I’d like to focus on only one: how Facebook was able to raise so much money when its actual usage was so small.1 By all accounts, what impressed investors the most were two facts about Facebook’s early growth. The first fact was the raw amount of time Facebook’s active users spent on the site. More than half of the users came back to the site every single day.2 This is an example of how a company can validate its value hypothesis—that customers nd the product valuable. The second impressive thing about Facebook’s early traction was the rate at which it had taken over its rst few college campuses. The rate of growth was staggering: Facebook launched on February 4, 2004, and by the end of that month almost three-quarters of Harvard’s undergraduates were using it, without a dollar of marketing or advertising having been

spent. In other words, Facebook also had validated its growth hypothesis. These two hypotheses represent two of the most important leap-of-faith questions any new startup faces.3 At the time, I heard many people criticize Facebook’s early investors, claiming that Facebook had “no business model” and only modest revenues relative to the valuation o ered by its investors. They saw in Facebook a return to the excesses of the dot-com era, when companies with little revenue raised massive amounts of cash to pursue a strategy of “attracting eyeballs” and “getting big fast.” Many dot-com-era startups planned to make money later by selling the eyeballs they had bought to other advertisers. In truth, those dot-com failures were little more than middlemen, e ectively paying money to acquire customers’ attention and then planning to resell it to others. Facebook was di erent, because it employed a di erent engine of growth. It paid nothing for customer acquisition, and its high engagement meant that it was accumulating massive amounts of customer attention every day. There was never any question that attention would be valuable to advertisers; the only question was how much they would pay. Many entrepreneurs are attempting to build the next Facebook, yet when they try to apply the lessons of Facebook and other famous startup success stories, they quickly get confused. Is the lesson of Facebook that startups should not charge customers money in the early days? Or is it that startups should never spend money on marketing? These questions cannot be answered in the abstract; there are an almost infinite number of counterexamples for any technique. Instead, as we saw in Part One, startups need to conduct experiments that help determine what techniques will work in their unique circumstances. For startups, the role of strategy is to help figure out the right questions to ask. STRATEGY IS BASED ON ASSUMPTIONS Every business plan begins with a set of assumptions. It lays out a strategy that takes those assumptions as a given and proceeds to

strategy that takes those assumptions as a given and proceeds to show how to achieve the company’s vision. Because the assumptions haven’t been proved to be true (they are assumptions, after all) and in fact are often erroneous, the goal of a startup’s early efforts should be to test them as quickly as possible. What traditional business strategy excels at is helping managers identify clearly what assumptions are being made in a particular business. The rst challenge for an entrepreneur is to build an organization that can test these assumptions systematically. The second challenge, as in all entrepreneurial situations, is to perform that rigorous testing without losing sight of the company’s overall vision. Many assumptions in a typical business plan are unexceptional. These are well-established facts drawn from past industry experience or straightforward deductions. In Facebook’s case, it was clear that advertisers would pay for customers’ attention. Hidden among these mundane details are a handful of assumptions that require more courage to state—in the present tense—with a straight face: we assume that customers have a signi cant desire to use a product like ours, or we assume that supermarkets will carry our product. Acting as if these assumptions are true is a classic entrepreneur superpower. They are called leaps of faith precisely because the success of the entire venture rests on them. If they are true, tremendous opportunity awaits. If they are false, the startup risks total failure. Most leaps of faith take the form of an argument by analogy. For example, one business plan I remember argued as follows: “Just as the development of progressive image loading allowed the widespread use of the World Wide Web over dial-up, so too our progressive rendering technology will allow our product to run on low-end personal computers.” You probably have no idea what progressive image loading or rendering is, and it doesn’t much matter. But you know the argument (perhaps you’ve even used it): Previous technology X was used to win market Y because of attribute Z. We have a new technology X2 that will enable us to win market Y2 because we too have attribute Z.

The problem with analogies like this is that they obscure the true leap of faith. That is their goal: to make the business seem less risky. They are used to persuade investors, employees, or partners to sign on. Most entrepreneurs would cringe to see their leap of faith written this way: Large numbers of people already wanted access to the World Wide Web. They knew what it was, they could a ord it, but they could not get access to it because the time it took to load images was too long. When progressive image loading was introduced, it allowed people to get onto the World Wide Web and tell their friends about it. Thus, company X won market Y. Similarly, there is already a large number of potential customers who want access to our product right now. They know they want it, they can a ord it, but they cannot access it because the rendering is too slow. When we debut our product with progressive rendering technology, they will ock to our software and tell their friends, and we will win market Y2. There are several things to notice in this revised statement. First, it’s important to identify the facts clearly. Is it really true that progressive image loading caused the adoption of the World Wide Web, or was this just one factor among many? More important, is it really true that there are large numbers of potential customers out there who want our solution right now? The earlier analogy was designed to convince stakeholders that a reasonable rst step is to build the new startup’s technology and see if customers will use it. The restated approach should make clear that what is needed is to do some empirical testing rst: let’s make sure that there really are hungry customers out there eager to embrace our new technology. Analogs and Antilogs

There is nothing intrinsically wrong with basing strategy on comparisons to other companies and industries. In fact, that approach can help you discover assumptions that are not really leaps of faith. For example, the venture capitalist Randy Komisar, whose book Getting to Plan B discussed the concept of leaps of faith in great detail, uses a framework of “analogs” and “antilogs” to plot strategy. He explains the analog-antilog concept by using the iPod as an example. “If you were looking for analogs, you would have to look at the Walkman,” he says. “It solved a critical question that Steve Jobs never had to ask himself: Will people listen to music in a public place using earphones? We think of that as a nonsense question today, but it is fundamental. When Sony asked the question, they did not have the answer. Steve Jobs had [the answer] in the analog [version]” Sony’s Walkman was the analog. Jobs then had to face the fact that although people were willing to download music, they were not willing to pay for it. “Napster was an antilog. That antilog had to lead him to address his business in a particular way,” Komisar says. “Out of these analogs and antilogs come a series of unique, unanswered questions. Those are leaps of faith that I, as an entrepreneur, am taking if I go through with this business venture. They are going to make or break my business. In the iPod business, one of those leaps of faith was that people would pay for music.” Of course that leap of faith turned out to be correct.4 Beyond “The Right Place at the Right Time” There are any number of famous entrepreneurs who made millions because they seemed to be in the right place at the right time. However, for every successful entrepreneur who was in the right place in the right time, there are many more who were there, too, in that right place at the right time but still managed to fail. Henry Ford was joined by nearly ve hundred other entrepreneurs in the early twentieth century. Imagine being an automobile entrepreneur, trained in state-of-the-art engineering, on the ground oor of one of

the biggest market opportunities in history. Yet the vast majority managed to make no money at all.5 We saw the same phenomenon with Facebook, which faced early competition from other collegebased social networks whose head start proved irrelevant. What di erentiates the success stories from the failures is that the successful entrepreneurs had the foresight, the ability, and the tools to discover which parts of their plans were working brilliantly and which were misguided, and adapt their strategies accordingly. Value and Growth As we saw in the Facebook story, two leaps of faith stand above all others: the value creation hypothesis and the growth hypothesis. The rst step in understanding a new product or service is to gure out if it is fundamentally value-creating or value-destroying. I use the language of economics in referring to value rather than pro t, because entrepreneurs include people who start not-for-pro t social ventures, those in public sector startups, and internal change agents who do not judge their success by pro t alone. Even more confusing, there are many organizations that are wildly profitable in the short term but ultimately value-destroying, such as the organizers of Ponzi schemes, and fraudulent or misguided companies (e.g., Enron and Lehman Brothers). A similar thing is true for growth. As with value, it’s essential that entrepreneurs understand the reasons behind a startup’s growth. There are many value-destroying kinds of growth that should be avoided. An example would be a business that grows through continuous fund-raising from investors and lots of paid advertising but does not develop a value-creating product. Such businesses are engaged in what I call success theater, using the appearance of growth to make it seem that they are successful. One of the goals of innovation accounting, which is discussed in depth in Chapter 7, is to help di erentiate these false startups from true innovators. Traditional accounting judges new ventures by the same standards it uses for established companies, but these

same standards it uses for established companies, but these indications are not reliable predictors of a startup’s future prospects. Consider companies such as Amazon.com that racked up huge losses on their way to breakthrough success. Like its traditional counterpart, innovation accounting requires that a startup have and maintain a quantitative nancial model that can be used to evaluate progress rigorously. However, in a startup’s earliest days, there is not enough data to make an informed guess about what this model might look like. A startup’s earliest strategic plans are likely to be hunch- or intuition-guided, and that is a good thing. To translate those instincts into data, entrepreneurs must, in Steve Blank’s famous phrase, “get out of the building” and start learning. GENCHI GEMBUTSU The importance of basing strategic decisions on rsthand understanding of customers is one of the core principles that underlies the Toyota Production System. At Toyota, this goes by the Japanese term genchi gembutsu, which is one of the most important phrases in the lean manufacturing vocabulary. In English, it is usually translated as a directive to “go and see for yourself” so that business decisions can be based on deep rsthand knowledge. Je rey Liker, who has extensively documented the “Toyota Way,” explains it this way: In my Toyota interviews, when I asked what distinguishes the Toyota Way from other management approaches, the most common rst response was genchi gembutsu —whether I was in manufacturing, product development, sales, distribution, or public a airs. You cannot be sure you really understand any part of any business problem unless you go and see for yourself rsthand. It is unacceptable to take anything for granted or to rely on the reports of others.6

To demonstrate, take a look at the development of Toyota’s Sienna minivan for the 2004 model year. At Toyota, the manager responsible for the design and development of a new model is called the chief engineer, a cross-functional leader who oversees the entire process from concept to production. The 2004 Sienna was assigned to Yuji Yokoya, who had very little experience in North America, which was the Sienna’s primary market. To gure out how to improve the minivan, he proposed an audacious entrepreneurial undertaking: a road trip spanning all fty U.S. states, all thirteen provinces and territories of Canada, and all parts of Mexico. In all, he logged more than 53,000 miles of driving. In small towns and large cities, Yokoya would rent a current-model Sienna, driving it in addition to talking to and observing real customers. From those rsthand observations, Yokoya was able to start testing his critical assumptions about what North American consumers wanted in a minivan. It is common to think of selling to consumers as easier than selling to enterprises, because customers lack the complexity of multiple departments and di erent people playing di erent roles in the purchasing process. Yokoya discovered this was untrue for his customers: “The parents and grandparents may own the minivan. But it’s the kids who rule it. It’s the kids who occupy the rear twothirds of the vehicle. And it’s the kids who are the most critical— and the most appreciative of their environment. If I learned anything in my travels, it was the new Sienna would need kid appeal.”7 Identifying these assumptions helped guide the car’s development. For example, Yokoya spent an unusual amount of the Sienna’s development budget on internal comfort features, which are critical to a long-distance family road trip (such trips are much more common in America than in Japan). The results were impressive, boosting the Sienna’s market share dramatically. The 2004 model’s sales were 60 percent higher than those in 2003. Of course, a product like the Sienna is a classic sustaining innovation, the kind that the world’s best-managed established companies, such as Toyota, excel at. Entrepreneurs face

a di erent set of challenges because they operate with much higher uncertainty. While a company working on a sustaining innovation knows enough about who and where their customers are to use genchi gembutsu to discover what customers want, startups’ early contact with potential customers merely reveals what assumptions require the most urgent testing. GET OUT OF THE BUILDING Numbers tell a compelling story, but I always remind entrepreneurs that metrics are people, too. No matter how many intermediaries lie between a company and its customers, at the end of the day, customers are breathing, thinking, buying individuals. Their behavior is measurable and changeable. Even when one is selling to large institutions, as in a business-to-business model, it helps to remember that those businesses are made up of individuals. All successful sales models depend on breaking down the monolithic view of organizations into the disparate people that make them up. As Steve Blank has been teaching entrepreneurs for years, the facts that we need to gather about customers, markets, suppliers, and channels exist only “outside the building.” Startups need extensive contact with potential customers to understand them, so get out of your chair and get to know them. The rst step in this process is to con rm that your leap-of-faith questions are based in reality, that the customer has a signi cant problem worth solving.8 When Scott Cook conceived Intuit in 1982, he had a vision—at that time quite radical—that someday consumers would use personal computers to pay bills and keep track of expenses. When Cook left his consulting job to take the entrepreneurial plunge, he didn’t start with stacks of market research or in-depth analysis at the whiteboard. Instead, he picked up two phone books: one for Palo Alto, California, where he was living at the time, and the other for Winnetka, Illinois. Calling people at random, he inquired if he could ask them a few questions about the way they managed their nances. Those early

conversations were designed to answer this leap-of-faith question: do people nd it frustrating to pay bills by hand? It turned out that they did, and this early validation gave Cook the con rmation he needed to get started on a solution.9 Those early conversations did not delve into the product features of a proposed solution; that attempt would have been foolish. The average consumers at that time were not conversant enough with personal computers to have an opinion about whether they’d want to use them in a new way. Those early conversations were with mainstream customers, not early adopters. Still, the conversations yielded a fundamental insight: if Intuit could nd a way to solve this problem, there could be a large mainstream audience on which it could build a significant business. Design and the Customer Archetype The goal of such early contact with customers is not to gain definitive answers. Instead, it is to clarify at a basic, coarse level that we understand our potential customer and what problems they have. With that understanding, we can craft a customer archetype, a brief document that seeks to humanize the proposed target customer. This archetype is an essential guide for product development and ensures that the daily prioritization decisions that every product team must make are aligned with the customer to whom the company aims to appeal. There are many techniques for building an accurate customer archetype that have been developed over long years of practice in the design community. Traditional approaches such as interaction design or design thinking are enormously helpful. To me, it has always seemed ironic that many of these approaches are highly experimental and iterative, using techniques such as rapid prototyping and in-person customer observations to guide designers’ work. Yet because of the way design agencies traditionally have been compensated, all this work culminates in a monolithic deliverable to the client. All of a sudden, the rapid

learning and experimentation stops; the assumption is that the designers have learned all there is to know. For startups, this is an unworkable model. No amount of design can anticipate the many complexities of bringing a product to life in the real world. In fact, a new breed of designers is developing brand-new techniques under the banner of Lean User Experience (Lean UX). They recognize that the customer archetype is a hypothesis, not a fact. The customer pro le should be considered provisional until the strategy has shown via validated learning that we can serve this type of customer in a sustainable way.10 ANALYSIS PARALYSIS There are two ever-present dangers when entrepreneurs conduct market research and talk to customers. Followers of the just-do-it school of entrepreneurship are impatient to get started and don’t want to spend time analyzing their strategy. They’d rather start building immediately, often after just a few cursory customer conversations. Unfortunately, because customers don’t really know what they want, it’s easy for these entrepreneurs to delude themselves that they are on the right path. Other entrepreneurs can fall victim to analysis paralysis, endlessly re ning their plans. In this case, talking to customers, reading research reports, and whiteboard strategizing are all equally unhelpful. The problem with most entrepreneurs’ plans is generally not that they don’t follow sound strategic principles but that the facts upon which they are based are wrong. Unfortunately, most of these errors cannot be detected at the whiteboard because they depend on the subtle interactions between products and customers. If too much analysis is dangerous but none can lead to failure, how do entrepreneurs know when to stop analyzing and start building? The answer is a concept called the minimum viable product, the subject of Chapter 6.

6 TEST is one of the fastest-growing companies of all time. Its Groupon name comes from “group coupons,” an ingenious idea that has spawned an entire industry of social commerce imitators. However, it didn’t start out successful. When customers took Groupon up on its rst deal, a whopping twenty people bought two-for-one pizza in a restaurant on the rst oor of the company’s Chicago offices—hardly a world-changing event. In fact, Groupon wasn’t originally meant to be about commerce at all. The founder, Andrew Mason, intended his company to become a “collective activism platform” called The Point. Its goal was to bring people together to solve problems they couldn’t solve on their own, such as fund-raising for a cause or boycotting a certain retailer. The Point’s early results were disappointing, however, and at the end of 2008 the founders decided to try something new. Although they still had grand ambitions, they were determined to keep the new product simple. They built a minimum viable product. Does this sound like a billion-dollar company to you? Mason tells the story: We took a WordPress Blog and we skinned it to say Groupon and then every day we would do a new post. It was totally ghetto. We would sell T-shirts on the rst version of Groupon. We’d say in the write-up, “This T-shirt will come in the color red, size large. If you want a different color or size, e-mail that to us.” We didn’t have a form to

color or size, e-mail that to us.” We didn’t have a form to add that stuff. It was just so cobbled together. It was enough to prove the concept and show that it was something that people really liked. The actual coupon generation that we were doing was all FileMaker. We would run a script that would e-mail the coupon PDF to people. It got to the point where we’d sell 500 sushi coupons in a day, and we’d send 500 PDFs to people with Apple Mail at the same time. Really until July of the rst year it was just a scrambling to grab the tiger by the tail. It was trying to catch up and reasonably piece together a product.1 Handmade PDFs, a pizza coupon, and a simple blog were enough to launch Groupon into record-breaking success; it is on pace to become the fastest company in history to achieve $1 billion in sales. It is revolutionizing the way local businesses nd new customers, o ering special deals to consumers in more than 375 cities worldwide.2 A minimum viable product (MVP) helps entrepreneurs start the process of learning as quickly as possible.3 It is not necessarily the smallest product imaginable, though; it is simply the fastest way to get through the Build-Measure-Learn feedback loop with the minimum amount of effort. Contrary to traditional product development, which usually involves a long, thoughtful incubation period and strives for product perfection, the goal of the MVP is to begin the process of learning, not end it. Unlike a prototype or concept test, an MVP is designed not just to answer product design or technical questions. Its goal is to test fundamental business hypotheses. WHY FIRST PRODUCTS AREN’T MEANT TO BE PERFECT At IMVU, when we were raising money from venture investors, we

At IMVU, when we were raising money from venture investors, we were embarrassed. First of all, our product was still buggy and lowquality. Second, although we were proud of our business results, they weren’t exactly earth-shattering. The good news was that we were on a hockey-stick-shaped growth curve. The bad news was that the hockey stick went up to only about $8,000 per month of revenue. These numbers were so low that we’d often have investors ask us, “What are the units on these charts? Are those numbers in thousands?” We’d have to reply, “No, sir, those are in ones.” However, those early results were extremely signi cant in predicting IMVU’s future path. As you’ll see in Chapter 7, we were able to validate two of our leap-of-faith assumptions: IMVU was providing value for customers, and we had a working engine of growth. The gross numbers were small because we were selling the product to visionary early customers called early adopters. Before new products can be sold successfully to the mass market, they have to be sold to early adopters. These people are a special breed of customer. They accept—in fact prefer—an 80 percent solution; you don’t need a perfect solution to capture their interest.4 Early technology adopters lined up around the block for Apple’s original iPhone even though it lacked basic features such as copy and paste, 3G Internet speed, and support for corporate e-mail. Google’s original search engine could answer queries about specialized topics such as Stanford University and the Linux operating system, but it would be years before it could “organize the world’s information.” However, this did not stop early adopters from singing its praises. Early adopters use their imagination to ll in what a product is missing. They prefer that state of a airs, because what they care about above all is being the rst to use or adopt a new product or technology. In consumer products, it’s often the thrill of being the rst one on the block to show o a new basketball shoe, music player, or cool phone. In enterprise products, it’s often about gaining a competitive advantage by taking a risk with something new that competitors don’t have yet. Early adopters are suspicious of something that is too polished: if it’s ready for everyone to adopt,

of something that is too polished: if it’s ready for everyone to adopt, how much advantage can one get by being early? As a result, additional features or polish beyond what early adopters demand is a form of wasted resources and time. This is a hard truth for many entrepreneurs to accept. After all, the vision entrepreneurs keep in their heads is of a high-quality mainstream product that will change the world, not one used by a small niche of people who are willing to give it a shot before it’s ready. That world-changing product is polished, slick, and ready for prime time. It wins awards at trade shows and, most of all, is something you can proudly show Mom and Dad. An early, buggy, incomplete product feels like an unacceptable compromise. How many of us were raised with the expectation that we would put our best work forward? As one manager put it to me recently, “I know for me, the MVP feels a little dangerous—in a good way—since I have always been such a perfectionist.” Minimum viable products range in complexity from extremely simple smoke tests (little more than an advertisement) to actual early prototypes complete with problems and missing features. Deciding exactly how complex an MVP needs to be cannot be done formulaically. It requires judgment. Luckily, this judgment is not di cult to develop: most entrepreneurs and product development people dramatically overestimate how many features are needed in an MVP. When in doubt, simplify. For example, consider a service sold with a one-month free trial. Before a customer can use the service, he or she has to sign up for the trial. One obvious assumption, then, of the business model is that customers will sign up for a free trial once they have a certain amount of information about the service. A critical question to consider is whether customers will in fact sign up for the free trial given a certain number of promised features (the value hypothesis). Somewhere in the business model, probably buried in a single cell in a spreadsheet, it speci es the “percentage of customers who see the free trial o er who then sign up.” Maybe in our projections we say that this number should be 10 percent. If you think about it, this is a leap-of-faith question. It really should be represented in giant letters in a bold red font: WE ASSUME 10 PERCENT OF CUSTOMERS WILL SIGN UP.

Most entrepreneurs approach a question like this by building the product and then checking to see how customers react to it. I consider this to be exactly backward because it can lead to a lot of waste. First, if it turns out that we’re building something nobody wants, the whole exercise will be an avoidable expense of time and money. If customers won’t sign up for the free trial, they’ll never get to experience the amazing features that await them. Even if they do sign up, there are many other opportunities for waste. For example, how many features do we really need to include to appeal to early adopters? Every extra feature is a form of waste, and if we delay the test for these extra features, it comes with a tremendous potential cost in terms of learning and cycle time. The lesson of the MVP is that any additional work beyond what was required to start learning is waste, no matter how important it might have seemed at the time. To demonstrate, I’ll share several MVP techniques from actual Lean Startups. In each case, you’ll witness entrepreneurs avoiding the temptation to overbuild and overpromise. THE VIDEO MINIMUM VIABLE PRODUCT Drew Houston is the CEO of Dropbox, a Silicon Valley company that makes an extremely easy-to-use le-sharing tool. Install its application, and a Dropbox folder appears on your computer desktop. Anything you drag into that folder is uploaded automatically to the Dropbox service and then instantly replicated across all your computers and devices. The founding team was made up of engineers, as the product demanded signi cant technical expertise to build. It required, for example, integration with a variety of computer platforms and operating systems: Windows, Macintosh, iPhone, Android, and so on. Each of these implementations happens at a deep level of the system and requires specialized know-how to make the user experience exceptional. In fact, one of Dropbox’s biggest competitive advantages is that the product works in such a seamless

way that the competition struggles to emulate it. These are not the kind of people one would think of as marketing geniuses. In fact, none of them had ever worked in a marketing job. They had prominent venture capital backers and could have been expected to apply the standard engineering thinking to building the business: build it and they will come. But Dropbox did something different. In parallel with their product development e orts, the founders wanted feedback from customers about what really mattered to them. In particular, Dropbox needed to test its leap-of-faith question: if we can provide a superior customer experience, will people give our product a try? They believed—rightly, as it turned out—that le synchronization was a problem that most people didn’t know they had. Once you experience the solution, you can’t imagine how you ever lived without it. This is not the kind of entrepreneurial question you can ask or expect an answer to in a focus group. Customers often don’t know what they want, and they often had a hard time understanding Dropbox when the concept was explained. Houston learned this the hard way when he tried to raise venture capital. In meeting after meeting, investors would explain that this “market space” was crowded with existing products, none of them had made very much money, and the problem wasn’t a very important one. Drew would ask: “Have you personally tried those other products?” When they would say yes, he’d ask: “Did they work seamlessly for you?” The answer was almost always no. Yet in meeting after meeting, the venture capitalists could not imagine a world in line with Drew’s vision. Drew, in contrast, believed that if the software “just worked like magic,” customers would flock to it. The challenge was that it was impossible to demonstrate the working software in a prototype form. The product required that they overcome signi cant technical hurdles; it also had an online service component that required high reliability and availability. To avoid the risk of waking up after years of development with a product nobody wanted, Drew did something unexpectedly easy: he made a video.

made a video. The video is banal, a simple three-minute demonstration of the technology as it is meant to work, but it was targeted at a community of technology early adopters. Drew narrates the video personally, and as he’s narrating, the viewer is watching his screen. As he describes the kinds of les he’d like to synchronize, the viewer can watch his mouse manipulate his computer. Of course, if you’re paying attention, you start to notice that the files he’s moving around are full of in-jokes and humorous references that were appreciated by this community of early adopters. Drew recounted, “It drove hundreds of thousands of people to the website. Our beta waiting list went from 5,000 people to 75,000 people literally overnight. It totally blew us away.” Today, Dropbox is one of Silicon Valley’s hottest companies, rumored to be worth more than $1 billion.5 In this case, the video was the minimum viable product. The MVP validated Drew’s leap-of-faith assumption that customers wanted the product he was developing not because they said so in a focus group or because of a hopeful analogy to another business, but because they actually signed up. THE CONCIERGE MINIMUM VIABLE PRODUCT Consider another kind of MVP technique: the concierge MVP. To understand how this technique works, meet Manuel Rosso, the CEO of an Austin, Texas–based startup called Food on the Table. Food on the Table creates weekly meal plans and grocery lists that are based on food you and your family enjoy, then hooks into your local grocery stores to find the best deals on the ingredients. After you sign up for the site, you walk through a little setup in which you identify your main grocery store and check o the foods your family likes. Later, you can pick another nearby store if you want to compare prices. Next, you’re presented with a list of items that are based on your preferences and asked: “What are you in the mood for this week?” Make your choices, select the number of meals you’re ready to plan, and choose what you care about most

meals you’re ready to plan, and choose what you care about most in terms of time, money, health, or variety. At this point, the site searches through recipes that match your needs, prices out the cost of the meal for you, and lets you print out your shopping list.6 Clearly, this is an elaborate service. Behind the scenes, a team of professional chefs devise recipes that take advantage of items that are on sale at local grocery stores around the country. Those recipes are matched via computer algorithm to each family’s unique needs and preferences. Try to visualize the work involved: databases of almost every grocery store in the country must be maintained, including what’s on sale at each one this week. Those groceries have to be matched to appropriate recipes and then appropriately customized, tagged, and sorted. If a recipe calls for broccoli rabe, is that the same ingredient as the broccoli on sale at the local market? After reading that description, you might be surprised to learn that Food on the Table (FotT) began life with a single customer. Instead of supporting thousands of grocery stores around the country as it does today, FotT supported just one. How did the company choose which store to support? The founders didn’t—until they had their rst customer. Similarly, they began life with no recipes whatsoever—until their rst customer was ready to begin her meal planning. In fact, the company served its rst customer without building any software, without signing any business development partnerships, and without hiring any chefs. Manuel, along with VP of product Steve Sanderson, went to local supermarkets and moms’ groups in his hometown of Austin. Part of their mission was the typical observation of customers that is a part of design thinking and other ideation techniques. However, Manuel and his team were also on the hunt for something else: their rst customer. As they met potential customers in those settings, they would interview them the way any good market researcher would, but at the end of each interview they would attempt to make a sale. They’d describe the bene ts of FotT, name a weekly subscription fee, and invite the customer to sign up. Most times they were rejected. After all, most people are not early adopters and will not

sign up for a new service sight unseen. But eventually someone did. That one early adopter got the concierge treatment. Instead of interacting with the FotT product via impersonal software, she got a personal visit each week from the CEO of the company. He and the VP of product would review what was on sale at her preferred grocery store and carefully select recipes on the basis of her preferences, going so far as to learn her favorite recipes for items she regularly cooked for her family. Each week they would hand her—in person—a prepared packet containing a shopping list and relevant recipes, solicit her feedback, and make changes as necessary. Most important, each week they would collect a check for $9.95. Talk about ine cient! Measured according to traditional criteria, this is a terrible system, entirely nonscalable and a complete waste of time. The CEO and VP of product, instead of building their business, are engaged in the drudgery of solving just one customer’s problem. Instead of marketing themselves to millions, they sold themselves to one. Worst of all, their e orts didn’t appear to be leading to anything tangible. They had no product, no meaningful revenue, no databases of recipes, not even a lasting organization. However, viewed through the lens of the Lean Startup, they were making monumental progress. Each week they were learning more and more about what was required to make their product a success. After a few weeks they were ready for another customer. Each customer they brought on made it easier to get the next one, because FotT could focus on the same grocery store, getting to know its products and the kinds of people who shopped there well. Each new customer got the concierge treatment: personal in-home visits, the works. But after a few more customers, the overhead of serving them one-on-one started to increase. Only at the point where the founders were too busy to bring on additional customers did Manuel and his team start to invest in automation in the form of product development. Each iteration of their minimum viable product allowed them to save a little more time and serve a few more customers: delivering the recipes and shopping list via e-mail instead of via an in-home visit, starting to

parse lists of what was on sale automatically via software instead of by hand, even eventually taking credit card payments online instead of a handwritten check. Before long, they had built a substantial service o ering, rst in the Austin area and eventually nationwide. But along the way, their product development team was always focused on scaling something that was working rather than trying to invent something that might work in the future. As a result, their development e orts involved far less waste than is typical for a venture of this kind. It is important to contrast this with the case of a small business, in which it is routine to see the CEO, founder, president, and owner serving customers directly, one at a time. In a concierge MVP, this personalized service is not the product but a learning activity designed to test the leap-of-faith assumptions in the company’s growth model. In fact, a common outcome of a concierge MVP is to invalidate the company’s proposed growth model, making it clear that a di erent approach is needed. This can happen even if the initial MVP is pro table for the company. Without a formal growth model, many companies get caught in the trap of being satis ed with a small pro table business when a pivot (change in course or strategy) might lead to more signi cant growth. The only way to know is to have tested the growth model systematically with real customers. PAY NO ATTENTION TO THE EIGHT PEOPLE BEHIND THE CURTAIN Meet Max Ventilla and Damon Horowitz, technologists with a vision to build a new type of search software designed to answer the kinds of questions that befuddle state-of-the-art companies such as Google. Google befuddled? Think about it. Google and its peers excel at answering factual questions: What is the tallest mountain in the world? Who was the twenty-third president of the United States? But for more subjective questions, Google struggles. Ask, “What’s a good place to go out for a drink after the ball game in my

city?” and the technology flails. What’s interesting about this class of queries is that they are relatively easy for a person to answer. Imagine being at a cocktail party surrounded by friends. How likely would you be to get a high-quality answer to your subjective question? You almost certainly would get one. Unlike factual queries, because these subjective questions have no single right answer, today’s technology struggles to answer them. Such questions depend on the person answering them, his or her personal experience, taste, and assessment of what you’re looking for. To solve this problem, Max and Damon created a product called Aardvark. With their deep technical knowledge and industry experience, it would have been reasonable to expect them to dive in and start programming. Instead, they took six months to gure out what they should be building. But they didn’t spend that year at the whiteboard strategizing or engage in a lengthy market research project. Instead, they built a series of functioning products, each designed to test a way of solving this problem for their customers. Each product was then o ered to beta testers, whose behavior was used to validate or refute each speci c hypothesis (see examples in sidebar).

The following list of projects are examples from Aardvark’s ideation period.7 Rekkit. A service to collect your ratings from across the web and give better recommendations to you. Ninjapa. A way that you could open accounts in various applications through a single website and manage your data across multiple sites.

The Webb. A central number that you could call and talk to a person who could do anything for you that you could do online. Web Macros. A way to record sequences of steps on websites so that you could repeat common actions, even across sites, and share “recipes” for how you accomplished online tasks. Internet Button Company. A way to package steps taken on a website and smart form- ll functionality. People could encode buttons and share buttons à la social bookmarking. Max and Damon had a vision that computers could be used to create a virtual personal assistant to which their customers could ask questions. Because the assistant was designed for subjective questions, the answers required human judgment. Thus, the early Aardvark experiments tried many variations on this theme, building a series of prototypes for ways customers could interact with the virtual assistant and get their questions answered. All the early prototypes failed to engage the customers. As Max describes it, “We self-funded the company and released very cheap prototypes to test. What became Aardvark was the sixth prototype. Each prototype was a two- to four-week e ort. We used humans to replicate the back end as much as possible. We invited one hundred to two hundred friends to try the prototypes and measured how many of them came back. The results were unambiguously negative until Aardvark.” Because of the short time line, none of the prototypes involved advanced technology. Instead, they were MVPs designed to test a more important question: what would be required to get customers to engage with the product and tell their friends about it? “Once we chose Aardvark,” Ventilla says, “we continued to run

with humans replicating pieces of the backend for nine months. We hired eight people to manage queries, classify conversations, etc. We actually raised our seed and series A rounds before the system was automated—the assumption was that the lines between humans and arti cial intelligence would cross, and we at least proved that we were building stuff people would respond to. “As we re ned the product, we would bring in six to twelve people weekly to react to mockups, prototypes, or simulations that we were working on. It was a mix of existing users and people who never saw the product before. We had our engineers join for many of these sessions, both so that they could make modi cations in real time, but also so we could all experience the pain of a user not knowing what to do.”8 The Aardvark product they settled on worked via instant messaging (IM). Customers could send Aardvark a question via IM, and Aardvark would get them an answer that was drawn from the customer’s social network: the system would seek out the customer’s friends and friends of friends and pose the question to them. Once it got a suitable answer, it would report back to the initial customer. Of course, a product like that requires a very important algorithm: given a question about a certain topic, who is the best person in the customer’s social network to answer that question? For example, a question about restaurants in San Francisco shouldn’t be routed to someone in Seattle. More challenging still, a question about computer programming probably shouldn’t be routed to an art student. Throughout their testing process, Max and Damon encountered many di cult technological problems like these. Each time, they emphatically refused to solve them at that early stage. Instead, they used Wizard of Oz testing to fake it. In a Wizard of Oz test, customers believe they are interacting with the actual product, but behind the scenes human beings are doing the work. Like the concierge MVP, this approach is incredibly ine cient. Imagine a service that allowed customers to ask questions of human

researchers—for free—and expect a real-time response. Such a service (at scale) would lose money, but it is easy to build on a micro scale. At that scale, it allowed Max and Damon to answer these all-important questions: If we can solve the tough technical problems behind this arti cial intelligence product, will people use it? Will their use lead to the creation of a product that has real value? It was this system that allowed Max and Damon to pivot over and over again, rejecting concepts that seemed promising but that would not have been viable. When they were ready to start scaling, they had a ready-made road map of what to build. The result: Aardvark was acquired for a reported $50 million—by Google.9 THE ROLE OF QUALITY AND DESIGN IN AN MVP One of the most vexing aspects of the minimum viable product is the challenge it poses to traditional notions of quality. The best professionals and craftspersons alike aspire to build quality products; it is a point of pride. Modern production processes rely on high quality as a way to boost e ciency. They operate using W. Edwards Deming’s famous dictum that the customer is the most important part of the production process. This means that we must focus our energies exclusively on producing outcomes that the customer perceives as valuable. Allowing sloppy work into our process inevitably leads to excessive variation. Variation in process yields products of varying quality in the eyes of the customer that at best require rework and at worst lead to a lost customer. Most modern business and engineering philosophies focus on producing high-quality experiences for customers as a primary principle; it is the foundation of Six Sigma, lean manufacturing, design thinking, extreme programming, and the software craftsmanship movement. These discussions of quality presuppose that the company already knows what attributes of the product the customer will perceive as worthwhile. In a startup, this is a risky assumption to make. Often

we are not even sure who the customer is. Thus, for startups, I believe in the following quality principle: If we do not know who the customer is, we do not know what quality is. Even a “low-quality” MVP can act in service of building a great high-quality product. Yes, MVPs sometimes are perceived as lowquality by customers. If so, we should use this as an opportunity to learn what attributes customers care about. This is in nitely better than mere speculation or whiteboard strategizing, because it provides a solid empirical foundation on which to build future products. Sometimes, however, customers react quite di erently. Many famous products were released in a “low-quality” state, and customers loved them. Imagine if Craig Newmark, in the early days of Craigslist, had refused to publish his humble e-mail newsletter because it lacked su cient high design. What if the founders of Groupon had felt “two pizzas for the price of one” was beneath them? I have had many similar experiences. In the early days of IMVU, our avatars were locked in one place, unable to move around the screen. The reason? We were building an MVP and had not yet tackled the di cult task of creating the technology that would allow avatars to walk around the virtual environments they inhabit. In the video game industry, the standard is that 3D avatars should move uidly as they walk, avoid obstacles in their path, and take an intelligent route toward their destination. Famous best-selling games such as Electronic Arts’ The Sims work on this principle. We didn’t want to ship a low-quality version of this feature, so we opted instead to ship with stationary avatars. Feedback from the customers was very consistent: they wanted the ability to move their avatars around the environment. We took this as bad news because it meant we would have to spend considerable amounts of time and money on a high-quality solution similar to The Sims. But before we committed ourselves to that

path, we decided to try another MVP. We used a simple hack, which felt almost like cheating. We changed the product so that customers could click where they wanted their avatar to go, and the avatar would teleport there instantly. No walking, no obstacle avoidance. The avatar disappeared and then reappeared an instant later in the new place. We couldn’t even a ord fancy teleportation graphics or sound e ects. We felt lame shipping this feature, but it was all we could afford. You can imagine our surprise when we started to get positive customer feedback. We never asked about the movement feature directly (we were too embarrassed). But when asked to name the top things about IMVU they liked best, customers consistently listed avatar “teleportation” among the top three (unbelievably, they often speci cally described it as “more advanced than The Sims”). This inexpensive compromise outperformed many features of the product we were most proud of, features that had taken much more time and money to produce. Customers don’t care how much time something takes to build. They care only if it serves their needs. Our customers preferred the quick teleportation feature because it allowed them to get where they wanted to go as fast as possible. In retrospect, this makes sense. Wouldn’t we all like to get wherever we’re going in an instant? No lines, no hours on a plane or sitting on the tarmac, no connections, no cabs or subways. Beam me up, Scotty. Our expensive “real-world” approach was beaten handily by a cool fantasy-world feature that cost much less but that our customers preferred. So which version of the product is low-quality, again? MVPs require the courage to put one’s assumptions to the test. If customers react the way we expect, we can take that as con rmation that our assumptions are correct. If we release a poorly designed product and customers (even early adopters) cannot gure out how to use it, that will con rm our need to invest in superior design. But we must always ask: what if they don’t care about design in the same way we do? Thus, the Lean Startup method is not opposed to building high-

quality products, but only in service of the goal of winning over customers. We must be willing to set aside our traditional professional standards to start the process of validated learning as soon as possible. But once again, this does not mean operating in a sloppy or undisciplined way. (This is an important caveat. There is a category of quality problems that have the net e ect of slowing down the Build-Measure-Learn feedback loop. Defects make it more di cult to evolve the product. They actually interfere with our ability to learn and so are dangerous to tolerate in any production process. We will consider methods for guring out when to make investments in preventing these kinds of problems in Part Three.) As you consider building your own minimum viable product, let this simple rule su ce: remove any feature, process, or e ort that does not contribute directly to the learning you seek. SPEED BUMPS IN BUILDING AN MVP Building an MVP is not without risks, both real and imagined. Both can derail a startup e ort unless they are understood ahead of time. The most common speed bumps are legal issues, fears about competitors, branding risks, and the impact on morale. For startups that rely on patent protection, there are special challenges with releasing an early product. In some jurisdictions, the window for ling a patent begins when the product is released to the general public, and depending on the way the MVP is structured, releasing it may start this clock. Even if your startup is not in one of those jurisdictions, you may want international patent protection and may wind up having to abide by these more stringent requirements. (In my opinion, issues like this are one of the many ways in which current patent law inhibits innovation and should be remedied as a matter of public policy.) In many industries, patents are used primarily for defensive purposes, as a deterrent to hold competitors at bay. In such cases, the patent risks of an MVP are minor compared with the learning bene ts. However, in industries in which a new scienti c

breakthrough is at the heart of a company’s competitive advantage, these risks need to be balanced more carefully. In all cases, entrepreneurs should seek legal counsel to ensure that they understand the risks fully. Legal risks may be daunting, but you may be surprised to learn that the most common objection I have heard over the years to building an MVP is fear of competitors—especially large established companies—stealing a startup’s ideas. If only it were so easy to have a good idea stolen! Part of the special challenge of being a startup is the near impossibility of having your idea, company, or product be noticed by anyone, let alone a competitor. In fact, I have often given entrepreneurs fearful of this issue the following assignment: take one of your ideas (one of your lesser insights, perhaps), nd the name of the relevant product manager at an established company who has responsibility for that area, and try to get that company to steal your idea. Call them up, write them a memo, send them a press release—go ahead, try it. The truth is that most managers in most companies are already overwhelmed with good ideas. Their challenge lies in prioritization and execution, and it is those challenges that give a startup hope of surviving.10 If a competitor can outexecute a startup once the idea is known, the startup is doomed anyway. The reason to build a new team to pursue an idea is that you believe you can accelerate through the Build-Measure-Learn feedback loop faster than anyone else can. If that’s true, it makes no di erence what the competition knows. If it’s not true, a startup has much bigger problems, and secrecy won’t x them. Sooner or later, a successful startup will face competition from fast followers. A head start is rarely large enough to matter, and time spent in stealth mode—away from customers—is unlikely to provide a head start. The only way to win is to learn faster than anyone else. Many startups plan to invest in building a great brand, and an MVP can seem like a dangerous branding risk. Similarly, entrepreneurs in existing organizations often are constrained by the fear of damaging the parent company’s established brand. In either

of these cases, there is an easy solution: launch the MVP under a di erent brand name. In addition, a long-term reputation is only at risk when companies engage in vocal launch activities such as PR and building hype. When a product fails to live up to those pronouncements, real long-term damage can happen to a corporate brand. But startups have the advantage of being obscure, having a pathetically small number of customers, and not having much exposure. Rather than lamenting them, use these advantages to experiment under the radar and then do a public marketing launch once the product has proved itself with real customers.11 Finally, it helps to prepare for the fact that MVPs often result in bad news. Unlike traditional concept tests or prototypes, they are designed to speak to the full range of business questions, not just design or technical ones, and they often provide a needed dose of reality. In fact, piercing the reality distortion eld is quite uncomfortable. Visionaries are especially afraid of a false negative: that customers will reject a awed MVP that is too small or too limited. It is precisely this attitude that one sees when companies launch fully formed products without prior testing. They simply couldn’t bear to test them in anything less than their full splendor. Yet there is wisdom in the visionary’s fear. Teams steeped in traditional product development methods are trained to make go/kill decisions on a regular basis. That is the essence of the waterfall or stage-gate development model. If an MVP fails, teams are liable to give up hope and abandon the project altogether. But this is a solvable problem. FROM THE MVP TO INNOVATION ACCOUNTING The solution to this dilemma is a commitment to iteration. You have to commit to a locked-in agreement—ahead of time—that no matter what comes of testing the MVP, you will not give up hope. Successful entrepreneurs do not give up at the rst sign of trouble, nor do they persevere the plane right into the ground. Instead, they possess a unique combination of perseverance and exibility. The

possess a unique combination of perseverance and exibility. The MVP is just the rst step on a journey of learning. Down that road —after many iterations—you may learn that some element of your product or strategy is awed and decide it is time to make a change, which I call a pivot, to a di erent method for achieving your vision. Startups are especially at risk when outside stakeholders and investors (especially corporate CFOs for internal projects) have a crisis of con dence. When the project was authorized or the investment made, the entrepreneur promised that the new product would be world-changing. Customers were supposed to ock to it in record numbers. Why are so few actually doing so? In traditional management, a manager who promises to deliver something and fails to do so is in trouble. There are only two possible explanations: a failure of execution or a failure to plan appropriately. Both are equally inexcusable. Entrepreneurial managers face a di cult problem: because the plans and projections we make are full of uncertainty, how can we claim success when we inevitably fail to deliver what we promised? Put another way, how does the CFO or VC know that we’re failing because we learned something critical and not because we were goofing off or misguided? The solution to this problem resides at the heart of the Lean Startup model. We all need a disciplined, systematic approach to guring out if we’re making progress and discovering if we’re actually achieving validated learning. I call this system innovation accounting, an alternative to traditional accounting designed specifically for startups. It is the subject of Chapter 7.

7 MEASURE beginning, a startup is little more than a model on a piece Atofofthepaper. The nancials in the business plan include projections how many customers the company expects to attract, how

much it will spend, and how much revenue and pro t that will lead to. It’s an ideal that’s usually far from where the startup is in its early days. A startup’s job is to (1) rigorously measure where it is right now, confronting the hard truths that assessment reveals, and then (2) devise experiments to learn how to move the real numbers closer to the ideal reflected in the business plan. Most products—even the ones that fail—do not have zero traction. Most products have some customers, some growth, and some positive results. One of the most dangerous outcomes for a startup is to bumble along in the land of the living dead. Employees and entrepreneurs tend to be optimistic by nature. We want to keep believing in our ideas even when the writing is on the wall. This is why the myth of perseverance is so dangerous. We all know stories of epic entrepreneurs who managed to pull out a victory when things seemed incredibly bleak. Unfortunately, we don’t hear stories about the countless nameless others who persevered too long, leading their companies to failure. WHY SOMETHING AS SEEMINGLY DULL AS ACCOUNTING WILL CHANGE YOUR LIFE

People are accustomed to thinking of accounting as dry and boring, a necessary evil used primarily to prepare nancial reports and survive audits, but that is because accounting is something that has become taken for granted. Historically, under the leadership of people such as Alfred Sloan at General Motors, accounting became an essential part of the method of exerting centralized control over far- ung divisions. Accounting allowed GM to set clear milestones for each of its divisions and then hold each manager accountable for his or her division’s success in reaching those goals. All modern corporations use some variation of that approach. Accounting is the key to their success. Unfortunately, standard accounting is not helpful in evaluating entrepreneurs. Startups are too unpredictable for forecasts and milestones to be accurate. I recently met with a phenomenal startup team. They are well nanced, have signi cant customer traction, and are growing rapidly. Their product is a leader in an emerging category of enterprise software that uses consumer marketing techniques to sell into large companies. For example, they rely on employee-toemployee viral adoption rather than a traditional sales process, which might target the chief information o cer or the head of information technology (IT). As a result, they have the opportunity to use cutting-edge experimental techniques as they constantly revise their product. During the meeting, I asked the team a simple question that I make a habit of asking startups whenever we meet: are you making your product better? They always say yes. Then I ask: how do you know? I invariably get this answer: well, we are in engineering and we made a number of changes last month, and our customers seem to like them, and our overall numbers are higher this month. We must be on the right track. This is the kind of storytelling that takes place at most startup board meetings. Most milestones are built the same way: hit a certain product milestone, maybe talk to a few customers, and see if the numbers go up. Unfortunately, this is not a good indicator of whether a startup is making progress. How do we know that the changes we’ve made are related to the results we’re seeing? More

changes we’ve made are related to the results we’re seeing? More important, how do we know that we are drawing the right lessons from those changes? To answer these kinds of questions, startups have a strong need for a new kind of accounting geared speci cally to disruptive innovation. That’s what innovation accounting is. An Accountability Framework That Works Across Industries Innovation accounting enables startups to prove objectively that they are learning how to grow a sustainable business. Innovation accounting begins by turning the leap-of-faith assumptions discussed i n Chapter 5 into a quantitative nancial model. Every business plan has some kind of model associated with it, even if it’s written on the back of a napkin. That model provides assumptions about what the business will look like at a successful point in the future. For example, the business plan for an established manufacturing company would show it growing in proportion to its sales volume. As the pro ts from the sales of goods are reinvested in marketing and promotions, the company gains new customers. The rate of growth depends primarily on three things: the pro tability of each customer, the cost of acquiring new customers, and the repeat purchase rate of existing customers. The higher these values are, the faster the company will grow and the more pro table it will be. These are the drivers of the company’s growth model. By contrast, a marketplace company that matches buyers and sellers such as eBay will have a di erent growth model. Its success depends primarily on the network e ects that make it the premier destination for both buyers and sellers to transact business. Sellers want the marketplace with the highest number of potential customers. Buyers want the marketplace with the most competition among sellers, which leads to the greatest availability of products and the lowest prices. (In economics, this sometimes is called supply-side increasing returns and demand-side increasing returns.) For this kind of startup, the important thing to measure is that the network e ects are working, as evidenced by the high retention rate

network e ects are working, as evidenced by the high retention rate of new buyers and sellers. If people stick with the product with very little attrition, the marketplace will grow no matter how the company acquires new customers. The growth curve will look like a compounding interest table, with the rate of growth depending on the “interest rate” of new customers coming to the product. Though these two businesses have very di erent drivers of growth, we can still use a common framework to hold their leaders accountable. This framework supports accountability even when the model changes. HOW INNOVATION ACCOUNTING WORKS—THREE LEARNING MILESTONES Innovation accounting works in three steps: rst, use a minimum viable product to establish real data on where the company is right now. Without a clear-eyed picture of your current status—no matter how far from the goal you may be—you cannot begin to track your progress. Second, startups must attempt to tune the engine from the baseline toward the ideal. This may take many attempts. After the startup has made all the micro changes and product optimizations it can to move its baseline toward the ideal, the company reaches a decision point. That is the third step: pivot or persevere. If the company is making good progress toward the ideal, that means it’s learning appropriately and using that learning effectively, in which case it makes sense to continue. If not, the management team eventually must conclude that its current product strategy is awed and needs a serious change. When a company pivots, it starts the process all over again, reestablishing a new baseline and then tuning the engine from there. The sign of a successful pivot is that these engine-tuning activities are more productive after the pivot than before. Establish the Baseline

For example, a startup might create a complete prototype of its product and o er to sell it to real customers through its main marketing channel. This single MVP would test most of the startup’s assumptions and establish baseline metrics for each assumption simultaneously. Alternatively, a startup might prefer to build separate MVPs that are aimed at getting feedback on one assumption at a time. Before building the prototype, the company might perform a smoke test with its marketing materials. This is an old direct marketing technique in which customers are given the opportunity to preorder a product that has not yet been built. A smoke test measures only one thing: whether customers are interested in trying a product. By itself, this is insu cient to validate an entire growth model. Nonetheless, it can be very useful to get feedback on this assumption before committing more money and other resources to the product. These MVPs provide the first example of a learning milestone. An MVP allows a startup to ll in real baseline data in its growth model—conversion rates, sign-up and trial rates, customer lifetime value, and so on—and this is valuable as the foundation for learning about customers and their reactions to a product even if that foundation begins with extremely bad news. When one is choosing among the many assumptions in a business plan, it makes sense to test the riskiest assumptions first. If you can’t nd a way to mitigate these risks toward the ideal that is required for a sustainable business, there is no point in testing the others. For example, a media business that is selling advertising has two basic assumptions that take the form of questions: Can it capture the attention of a de ned customer segment on an ongoing basis? and can it sell that attention to advertisers? In a business in which the advertising rates for a particular customer segment are well known, the far riskier assumption is the ability to capture attention. Therefore, the rst experiments should involve content production rather than advertising sales. Perhaps the company will produce a pilot episode or issue to see how customers engage.

Tuning the Engine Once the baseline has been established, the startup can work toward the second learning milestone: tuning the engine. Every product development, marketing, or other initiative that a startup undertakes should be targeted at improving one of the drivers of its growth model. For example, a company might spend time improving the design of its product to make it easier for new customers to use. This presupposes that the activation rate of new customers is a driver of growth and that its baseline is lower than the company would like. To demonstrate validated learning, the design changes must improve the activation rate of new customers. If they do not, the new design should be judged a failure. This is an important rule: a good design is one that changes customer behavior for the better. Compare two startups. The rst company sets out with a clear baseline metric, a hypothesis about what will improve that metric, and a set of experiments designed to test that hypothesis. The second team sits around debating what would improve the product, implements several of those changes at once, and celebrates if there is any positive increase in any of the numbers. Which startup is more likely to be doing e ective work and achieving lasting results? Pivot or Persevere Over time, a team that is learning its way toward a sustainable business will see the numbers in its model rise from the horrible baseline established by the MVP and converge to something like the ideal one established in the business plan. A startup that fails to do so will see that ideal recede ever farther into the distance. When this is done right, even the most powerful reality distortion eld won’t be able to cover up this simple fact: if we’re not moving the drivers of our business model, we’re not making progress. That becomes a sure sign that it’s time to pivot.

INNOVATION ACCOUNTING AT IMVU Here’s what innovation accounting looked like for us in the early days of IMVU. Our minimum viable product had many defects and, when we rst released it, extremely low sales. We naturally assumed that the lack of sales was related to the low quality of the product, so week after week we worked on improving the quality of the product, trusting that our e orts were worthwhile. At the end of each month, we would have a board meeting at which we would present the results. The night before the board meeting, we’d run our standard analytics, measuring conversion rates, customer counts, and revenue to show what a good job we had done. For several meetings in a row, this caused a last-minute panic because the quality improvements were not yielding any change in customer behavior. This led to some frustrating board meetings at which we could show great product “progress” but not much in the way of business results. After a while, rather than leave it to the last minute, we began to track our metrics more frequently, tightening the feedback loop with product development. This was even more depressing. Week in, week out, our product changes were having no effect. Improving a Product on Five Dollars a Day We tracked the “funnel metrics” behaviors that were critical to our engine of growth: customer registration, the download of our application, trial, repeat usage, and purchase. To have enough data to learn, we needed just enough customers using our product to get real numbers for each behavior. We allocated a budget of ve dollars per day: enough to buy clicks on the then-new Google AdWords system. In those days, the minimum you could bid for a click was 5 cents, but there was no overall minimum to your spending. Thus, we could a ord to open an account and get started even though we had very little money.1

even though we had very little money. Five dollars bought us a hundred clicks—every day. From a marketing point of view this was not very signi cant, but for learning it was priceless. Every single day we were able to measure our product’s performance with a brand new set of customers. Also, each time we revised the product, we got a brand new report card on how we were doing the very next day. For example, one day we would debut a new marketing message aimed at rst-time customers. The next day we might change the way new customers were initiated into the product. Other days, we would add new features, x bugs, roll out a new visual design, or try a new layout for our website. Every time, we told ourselves we were making the product better, but that subjective con dence was put to the acid test of real numbers. Day in and day out we were performing random trials. Each day was a new experiment. Each day’s customers were independent of those of the day before. Most important, even though our gross numbers were growing, it became clear that our funnel metrics were not changing. Here is a graph from one of IMVU’s early board meetings:

This graph represents approximately seven months of work. Over that period, we were making constant improvements to the IMVU product, releasing new features on a daily basis. We were conducting a lot of in-person customer interviews, and our product development team was working extremely hard. Cohort Analysis To read the graph, you need to understand something called cohort analysis. This is one of the most important tools of startup analytics. Although it sounds complex, it is based on a simple premise.

Although it sounds complex, it is based on a simple premise. Instead of looking at cumulative totals or gross numbers such as total revenue and total number of customers, one looks at the performance of each group of customers that comes into contact with the product independently. Each group is called a cohort. The graph shows the conversion rates to IMVU of new customers who joined in each indicated month. Each conversion rate shows the percentage of customer who registered in that month who subsequently went on to take the indicated action. Thus, among all the customers who joined IMVU in February 2005, about 60 percent of them logged in to our product at least one time. Managers with an enterprise sales background will recognize this funnel analysis as the traditional sales funnel that is used to manage prospects on their way to becoming customers. Lean Startups use it in product development, too. This technique is useful in many types of business, because every company depends for its survival on sequences of customer behavior called ows. Customer ows govern the interaction of customers with a company’s products. They allow us to understand a business quantitatively and have much more predictive power than do traditional gross metrics. If you look closely, you’ll see that the graph shows some clear trends. Some product improvements are helping—a little. The percentage of new customers who go on to use the product at least ve times has grown from less than 5 percent to almost 20 percent. Yet despite this fourfold increase, the percentage of new customers who pay money for IMVU is stuck at around 1 percent. Think about that for a moment. After months and months of work, thousands of individual improvements, focus groups, design sessions, and usability tests, the percentage of new customers who subsequently pay money is exactly the same as it was at the onset even though many more customers are getting a chance to try the product. Thanks to the power of cohort analysis, we could not blame this failure on the legacy of previous customers who were resistant to change, external market conditions, or any other excuse. Each cohort represented an independent report card, and try as we might, we were getting straight C’s. This helped us realize we had a

might, we were getting straight C’s. This helped us realize we had a problem. I was in charge of the product development team, small though it was in those days, and shared with my cofounders the sense that the problem had to be with my team’s e orts. I worked harder, tried to focus on higher- and higher-quality features, and lost a lot of sleep. Our frustration grew. When I could think of nothing else to do, I was nally ready to turn to the last resort: talking to customers. Armed with our failure to make progress tuning our engine of growth, I was ready to ask the right questions. Before this failure, in the company’s earliest days, it was easy to talk to potential customers and come away convinced we were on the right track. In fact, when we would invite customers into the o ce for in-person interviews and usability tests, it was easy to dismiss negative feedback. If they didn’t want to use the product, I assumed they were not in our target market. “Fire that customer,” I’d say to the person responsible for recruiting for our tests. “Find me someone in our target demographic.” If the next customer was more positive, I would take it as confirmation that I was right in my targeting. If not, I’d fire another customer and try again. By contrast, once I had data in hand, my interactions with customers changed. Suddenly I had urgent questions that needed answering: Why aren’t customers responding to our product “improvements”? Why isn’t our hard work paying o ? For example, we kept making it easier and easier for customers to use IMVU with their existing friends. Unfortunately, customers didn’t want to engage in that behavior. Making it easier to use was totally beside the point. Once we knew what to look for, genuine understanding came much faster. As was described in Chapter 3, this eventually led to a critically important pivot: away from an IM add-on used with existing friends and toward a stand-alone network one can use to make new friends. Suddenly, our worries about productivity vanished. Once our e orts were aligned with what customers really wanted, our experiments were much more likely to change their behavior for the better. This pattern would repeat time and again, from the days when we were making less than a thousand dollars in revenue per month

all the way up to the time we were making millions. In fact, this is the sign of a successful pivot: the new experiments you run are overall more productive than the experiments you were running before. This is the pattern: poor quantitative results force us to declare failure and create the motivation, context, and space for more qualitative research. These investigations produce new ideas—new hypotheses—to be tested, leading to a possible pivot. Each pivot unlocks new opportunities for further experimentation, and the cycle repeats. Each time we repeat this simple rhythm: establish the baseline, tune the engine, and make a decision to pivot or persevere. OPTIMIZATION VERSUS LEARNING Engineers, designers, and marketers are all skilled at optimization. For example, direct marketers are experienced at split testing value propositions by sending a di erent o er to two similar groups of customers so that they can measure di erences in the response rates of the two groups. Engineers, of course, are skilled at improving a product’s performance, just as designers are talented at making products easier to use. All these activities in a well-run traditional organization o er incremental bene t for incremental e ort. As long as we are executing the plan well, hard work yields results. However, these tools for product improvement do not work the same way for startups. If you are building the wrong thing, optimizing the product or its marketing will not yield signi cant results. A startup has to measure progress against a high bar: evidence that a sustainable business can be built around its products or services. That’s a standard that can be assessed only if a startup has made clear, tangible predictions ahead of time. In the absence of those predictions, product and strategy decisions are far more di cult and time-consuming. I often see this in my consulting practice. I’ve been called in many times to help a startup that feels that its engineering team “isn’t working hard enough.”

When I meet with those teams, there are always improvements to be made and I recommend them, but invariably the real problem is not a lack of development talent, energy, or e ort. Cycle after cycle, the team is working hard, but the business is not seeing results. Managers trained in a traditional model draw the logical conclusion: our team is not working hard, not working e ectively, or not working efficiently. Thus the downward cycle begins: the product development team valiantly tries to build a product according to the speci cations it is receiving from the creative or business leadership. When good results are not forthcoming, business leaders assume that any discrepancy between what was planned and what was built is the cause and try to specify the next iteration in greater detail. As the speci cations get more detailed, the planning process slows down, batch size increases, and feedback is delayed. If a board of directors or CFO is involved as a stakeholder, it doesn’t take long for personnel changes to follow. A few years ago, a team that sells products to large media companies invited me to help them as a consultant because they were concerned that their engineers were not working hard enough. However, the fault was not in the engineers; it was in the process the whole company was using to make decisions. They had customers but did not know them very well. They were deluged with feature requests from customers, the internal sales team, and the business leadership. Every new insight became an emergency that had to be addressed immediately. As a result, long-term projects were hampered by constant interruptions. Even worse, the team had no clear sense of whether any of the changes they were making mattered to customers. Despite the constant tuning and tweaking, the business results were consistently mediocre. Learning milestones prevent this negative spiral by emphasizing a more likely possibility: the company is executing—with discipline! —a plan that does not make sense. The innovation accounting framework makes it clear when the company is stuck and needs to change direction. In the example above, early in the company’s life, the product

development team was incredibly productive because the company’s founders had identi ed a large unmet need in the target market. The initial product, while awed, was popular with early adopters. Adding the major features that customers asked for seemed to work wonders, as the early adopters spread the word about the innovation far and wide. But unasked and unanswered were other lurking questions: Did the company have a working engine of growth? Was this early success related to the daily work of the product development team? In most cases, the answer was no; success was driven by decisions the team had made in the past. None of its current initiatives were having any impact. But this was obscured because the company’s gross metrics were all “up and to the right.” As we’ll see in a moment, this is a common danger. Companies of any size that have a working engine of growth can come to rely on the wrong kind of metrics to guide their actions. This is what tempts managers to resort to the usual bag of success theater tricks: last-minute ad buys, channel stu ng, and whiz-bang demos, in a desperate attempt to make the gross numbers look better. Energy invested in success theater is energy that could have been used to help build a sustainable business. I call the traditional numbers used to judge startups “vanity metrics,” and innovation accounting requires us to avoid the temptation to use them. VANITY METRICS: A WORD OF CAUTION To see the danger of vanity metrics clearly, let’s return once more to the early days of IMVU. Take a look at the following graph, which is from the same era in IMVU’s history as that shown earlier in this chapter. It covers the same time period as the cohort-style graph on this page; in fact, it is from the same board presentation. This graph shows the traditional gross metrics for IMVU so far: total registered users and total paying customers (the gross revenue graph looks almost the same). From this viewpoint, things look much more exciting. That’s why I call these vanity metrics: they give

much more exciting. That’s why I call these vanity metrics: they give the rosiest possible picture. You’ll see a traditional hockey stick graph (the ideal in a rapid-growth company). As long as you focus on the top-line numbers (signing up more customers, an increase in overall revenue), you’ll be forgiven for thinking this product development team is making great progress. The company’s growth engine is working. Each month it is able to acquire customers and has a positive return on investment. The excess revenue from those customers is reinvested the next month in acquiring more. That’s where the growth is coming from.

But think back to the same data presented in a cohort style. IMVU is adding new customers, but it is not improving the yield on each new group. The engine is turning, but the e orts to tune the

engine are not bearing much fruit. From the traditional graph alone, you cannot tell whether IMVU is on pace to build a sustainable business; you certainly can’t tell anything about the efficacy of the entrepreneurial team behind it. Innovation accounting will not work if a startup is being misled by these kinds of vanity metrics: gross number of customers and so on. The alternative is the kind of metrics we use to judge our business and our learning milestones, what I call actionable metrics. ACTIONABLE METRICS VERSUS VANITY METRICS To get a better sense of the importance of good metrics, let’s look at a company called Grockit. Its founder, Farbood Nivi, spent a decade working as a teacher at two large for-pro t education companies, Princeton Review and Kaplan, helping students prepare for standardized tests such as the GMAT, LSAT, and SAT. His engaging classroom style won accolades from his students and promotions from his superiors; he was honored with Princeton Review’s National Teacher of the Year award. But Farb was frustrated with the traditional teaching methods used by those companies. Teaching six to nine hours per day to thousands of students, he had many opportunities to experiment with new approaches.2 Over time, Farb concluded that the traditional lecture model of education, with its one-to-many instructional approach, was inadequate for his students. He set out to develop a superior approach, using a combination of teacher-led lectures, individual homework, and group study. In particular, Farb was fascinated by how e ective the student-to-student peer-driven learning method was for his students. When students could help each other, they bene ted in two ways. First, they could get customized instruction from a peer who was much less intimidating than a teacher. Second, they could reinforce their learning by teaching it to others. Over time, Farb’s classes became increasingly social—and successful. As this unfolded, Farb felt more and more that his physical presence in the classroom was less important. He made an

important connection: “I have this social learning model in my classroom. There’s all this social stu going on on the web.” His idea was to bring social peer-to-peer learning to people who could not a ord an expensive class from Kaplan or Princeton Review or an even more expensive private tutor. From this insight Grockit was born. Farb explains, “Whether you’re studying for the SAT or you’re studying for algebra, you study in one of three ways. You spend some time with experts, you spend some time on your own, and you spend some time with your peers. Grockit o ers these three same formats of studying. What we do is we apply technology and algorithms to optimize those three forms.” Farb is the classic entrepreneurial visionary. He recounts his original insight this way: “Let’s forget educational design up until now, let’s forget what’s possible and just redesign learning with today’s students and today’s technology in mind. There were plenty of multi-billion-dollar organizations in the education space, and I don’t think they were innovating in the way that we needed them to and I didn’t think we needed them anymore. To me, it’s really all about the students and I didn’t feel like the students were being served as well as they could.” Today Grockit o ers many di erent educational products, but in the beginning Farb followed a lean approach. Grockit built a minimum viable product, which was simply Farb teaching test prep via the popular online web conferencing tool WebEx. He built no custom software, no new technology. He simply attempted to bring his new teaching approach to students via the Internet. News about a new kind of private tutoring spread quickly, and within a few months Farb was making a decent living teaching online, with monthly revenues of $10,000 to $15,000. But like many entrepreneurs with ambition, Farb didn’t build his MVP just to make a living. He had a vision of a more collaborative, more e ective kind of teaching for students everywhere. With his initial traction, he was able to raise money from some of the most prestigious investors in Silicon Valley. When I rst met Farb, his company was already on the fast track

to success. They had raised venture capital from well-regarded investors, had built an awesome team, and were fresh o an impressive debut at one of Silicon Valley’s famous startup competitions. They were extremely process-oriented and disciplined. Their product development followed a rigorous version of the agile development methodology known as Extreme Programming (described below), thanks to their partnership with a San Francisco–based company called Pivotal Labs. Their early product was hailed by the press as a breakthrough. There was only one problem: they were not seeing su cient growth in the use of the product by customers. Grockit is an excellent case study because its problems were not a matter of failure of execution or discipline. Following standard agile practice, Grockit’s work proceeded in a series of sprints, or one-month iteration cycles. For each sprint, Farb would prioritize the work to be done that month by writing a series of user stories, a technique taken from agile development. Instead of writing a speci cation for a new feature that described it in technical terms, Farb would write a story that described the feature from the point of view of the customer. That story helped keep the engineers focused on the customer’s perspective throughout the development process. Each feature was expressed in plain language in terms everyone could understand whether they had a technical background or not. Again following standard agile practice, Farb was free to reprioritize these stories at any time. As he learned more about what customers wanted, he could move things around in the product backlog, the queue of stories yet to be built. The only limit on this ability to change directions was that he could not interrupt any task that was in progress. Fortunately, the stories were written in such a way that the batch size of work (which I’ll discuss in more detail in Chapter 9) was only a day or two. This system is called agile development for a good reason: teams that employ it are able to change direction quickly, stay light on their feet, and be highly responsive to changes in the business

requirements of the product owner (the manager of the process—in this case Farb—who is responsible for prioritizing the stories). How did the team feel at the end of each sprint? They consistently delivered new product features. They would collect feedback from customers in the form of anecdotes and interviews that indicated that at least some customers liked the new features. There was always a certain amount of data that showed improvement: perhaps the total number of customers was increasing, the total number of questions answered by students was going up, or the number of returning customers was increasing. However, I sensed that Farb and his team were left with lingering doubts about the company’s overall progress. Was the increase in their numbers actually caused by their development e orts? Or could it be due to other factors, such as mentions of Grockit in the press? When I met the team, I asked them this simple question: How do you know that the prioritization decisions that Farb is making actually make sense? Their answer: “That’s not our department. Farb makes the decisions; we execute them.” At that time Grockit was focused on just one customer segment: prospective business school students who were studying for the GMAT. The product allowed students to engage in online study sessions with fellow students who were studying for the same exam. The product was working: the students who completed their studying via Grockit achieved signi cantly higher scores than they had before. But the Grockit team was struggling with the age-old startup problems: How do we know which features to prioritize? How can we get more customers to sign up and pay? How can we get out the word about our product? I put this question to Farb: “How con dent are you that you are making the right decisions in terms of establishing priorities?” Like most startup founders, he was looking at the available data and making the best educated guesses he could. But this left a lot of room for ambiguity and doubt. Farb believed in his vision thoroughly and completely, yet he was starting to question whether his company was on pace to realize

starting to question whether his company was on pace to realize that vision. The product improved every day, but Farb wanted to make sure those improvements mattered to customers. I believe he deserves a lot of credit for realizing this. Unlike many visionaries, who cling to their original vision no matter what, Farb was willing to put his vision to the test. Farb worked hard to sustain his team’s belief that Grockit was destined for success. He was worried that morale would su er if anyone thought that the person steering the ship was uncertain about which direction to go. Farb himself wasn’t sure if his team would embrace a true learning culture. After all, this was part of the grand bargain of agile development: engineers agree to adapt the product to the business’s constantly changing requirements but are not responsible for the quality of those business decisions. Agile is an e cient system of development from the point of view of the developers. It allows them to stay focused on creating features and technical designs. An attempt to introduce the need to learn into that process could undermine productivity. (Lean manufacturing faced similar problems when it was introduced in factories. Managers were used to focusing on the utilization rate of each machine. Factories were designed to keep machines running at full capacity as much of the time as possible. Viewed from the perspective of the machine, that is e cient, but from the point of view of the productivity of the entire factory, it is wildly ine cient at times. As they say in systems theory, that which optimizes one part of the system necessarily undermines the system as a whole.) What Farb and his team didn’t realize was that Grockit’s progress was being measured by vanity metrics: the total number of customers and the total number of questions answered. That was what was causing his team to spin its wheels; those metrics gave the team the sensation of forward motion even though the company was making little progress. What’s interesting is how closely Farb’s method followed super cial aspects of the Lean Startup learning milestones: they shipped an early product and established some baseline metrics. They had relatively short iterations, each of which was judged by its ability to improve customer metrics.

was judged by its ability to improve customer metrics. However, because Grockit was using the wrong kinds of metrics, the startup was not genuinely improving. Farb was frustrated in his e orts to learn from customer feedback. In every cycle, the type of metrics his team was focused on would change: one month they would look at gross usage numbers, another month registration numbers, and so on. Those metrics would go up and down seemingly on their own. He couldn’t draw clear cause-and-e ect inferences. Prioritizing work correctly in such an environment is extremely challenging. Farb could have asked his data analyst to investigate a particular question. For example, when we shipped feature X, did it a ect customer behavior? But that would have required tremendous time and e ort. When, exactly, did feature X ship? Which customers were exposed to it? Was anything else launched around that same time? Were there seasonal factors that might be skewing the data? Finding these answers would have required parsing reams and reams of data. The answer often would come weeks after the question had been asked. In the meantime, the team would have moved on to new priorities and new questions that needed urgent attention. Compared to a lot of startups, the Grockit team had a huge advantage: they were tremendously disciplined. A disciplined team may apply the wrong methodology but can shift gears quickly once it discovers its error. Most important, a disciplined team can experiment with its own working style and draw meaningful conclusions. Cohorts and Split-tests Grockit changed the metrics they used to evaluate success in two ways. Instead of looking at gross metrics, Grockit switched to cohort-based metrics, and instead of looking for cause-and-e ect relationships after the fact, Grockit would launch each new feature as a true split-test experiment. A split-test experiment is one in which di erent versions of a

A split-test experiment is one in which di erent versions of a product are o ered to customers at the same time. By observing the changes in behavior between the two groups, one can make inferences about the impact of the di erent variations. This technique was pioneered by direct mail advertisers. For example, consider a company that sends customers a catalog of products to buy, such as Lands’ End or Crate & Barrel. If you wanted to test a catalog design, you could send a new version of it to 50 percent of the customers and send the old standard catalog to the other 50 percent. To assure a scienti c result, both catalogs would contain identical products; the only di erence would be the changes to the design. To gure out if the new design was e ective, all you would have to do was keep track of the sales gures for both groups of customers. (This technique is sometimes called A/B testing after the practice of assigning letter names to each variation.) Although split testing often is thought of as a marketing-speci c (or even a direct marketing–speci c) practice, Lean Startups incorporate it directly into product development. These changes led to an immediate change in Farb’s understanding of the business. Split testing often uncovers surprising things. For example, many features that make the product better in the eyes of engineers and designers have no impact on customer behavior. This was the case at Grockit, as it has been in every company I have seen adopt this technique. Although working with split tests seems to be more di cult because it requires extra accounting and metrics to keep track of each variation, it almost always saves tremendous amounts of time in the long run by eliminating work that doesn’t matter to customers. Split testing also helps teams re ne their understanding of what customers want and don’t want. Grockit’s team constantly added new ways for their customers to interact with each other in the hope that those social communication tools would increase the product’s value. Inherent in those e orts was the belief that customers desired more communication during their studying. When split testing revealed that the extra features did not change customer behavior, it called that belief into question. The questioning inspired the team to seek a deeper

The questioning inspired the team to seek a deeper understanding of what customers really wanted. They brainstormed new ideas for product experiments that might have more impact. In fact, many of these ideas were not new. They had simply been overlooked because the company was focused on building social tools. As a result, Grockit tested an intensive solo-studying mode, complete with quests and gamelike levels, so that students could have the choice of studying by themselves or with others. Just as in Farb’s original classroom, this proved extremely e ective. Without the discipline of split testing, the company might not have had this realization. In fact, over time, through dozens of tests, it became clear that the key to student engagement was to o er them a combination of social and solo features. Students preferred having a choice of how to study. Kanban Following the lean manufacturing principle of kanban, or capacity constraint, Grockit changed the product prioritization process. Under the new system, user stories were not considered complete until they led to validated learning. Thus, stories could be cataloged as being in one of four states of development: in the product backlog, actively being built, done (feature complete from a technical point of view), or in the process of being validated. Validated was de ned as “knowing whether the story was a good idea to have been done in the rst place.” This validation usually would come in the form of a split test showing a change in customer behavior but also might include customer interviews or surveys. The kanban rule permitted only so many stories in each of the four states. As stories ow from one state to the other, the buckets ll up. Once a bucket becomes full, it cannot accept more stories. Only when a story has been validated can it be removed from the kanban board. If the validation fails and it turns out the story is a bad idea, the relevant feature is removed from the product (see the chart on this page).

KANBAN DIAGRAM OF WORK AS IT PROGRESSES FROM STAGE TO STAGE (No bucket can contain more than three projects at a time.)

Work on A begins. D and E are in development. F awaits validation.

F is validated. D and E await validation. G, H, I are new tasks to be undertaken. B and C are being built. A completes development.

B and C have been built, but under kanban, cannot be moved to the next bucket for validation until A, D, E have been validated. Work cannot begin on H and I until space opens up in the buckets ahead.

I have implemented this system with several teams, and the

I have implemented this system with several teams, and the initial result is always frustrating: each bucket lls up, starting with the “validated” bucket and moving on to the “done” bucket, until it’s not possible to start any more work. Teams that are used to measuring their productivity narrowly, by the number of stories they are delivering, feel stuck. The only way to start work on new features is to investigate some of the stories that are done but haven’t been validated. That often requires nonengineering e orts: talking to customers, looking at split-test data, and the like. Pretty soon everyone gets the hang of it. This progress occurs in ts and starts at rst. Engineering may nish a big batch of work, followed by extensive testing and validation. As engineers look for ways to increase their productivity, they start to realize that if they include the validation exercise from the beginning, the whole team can be more productive. For example, why build a new feature that is not part of a splittest experiment? It may save you time in the short run, but it will take more time later to test, during the validation phase. The same logic applies to a story that an engineer doesn’t understand. Under the old system, he or she would just build it and nd out later what it was for. In the new system, that behavior is clearly counterproductive: without a clear hypothesis, how can a story ever be validated? We saw this behavior at IMVU, too. I once saw a junior engineer face down a senior executive over a relatively minor change. The engineer insisted that the new feature be splittested, just like any other. His peers backed him up; it was considered absolutely obvious that all features should be routinely tested, no matter who was commissioning them. (Embarrassingly, all too often I was the executive in question.) A solid process lays the foundation for a healthy culture, one where ideas are evaluated by merit and not by job title. Most important, teams working in this system begin to measure their productivity according to validated learning, not in terms of the production of new features. Hypothesis Testing at Grockit

When Grockit made this transition, the results were dramatic. In one case, they decided to test one of their major features, called lazy registration, to see if it was worth the heavy investment they were making in ongoing support. They were con dent in this feature because lazy registration is considered one of the design best practices for online services. In this system, customers do not have to register for the service up front. Instead, they immediately begin using the service and are asked to register only after they have had a chance to experience the service’s benefit. For a student, lazy registration works like this: when you come to the Grockit website, you’re immediately placed in a study session with other students working on the same test. You don’t have to give your name, e-mail address, or credit card number. There is nothing to prevent you from jumping in and getting started immediately. For Grockit, this was essential to testing one of its core assumptions: that customers would be willing to adopt this new way of learning only if they could see proof that it was working early on. As a result of this hypothesis, Grockit’s design required that it manage three classes of users: unregistered guests, registered (trial) guests, and customers who had paid for the premium version of the product. This design required signi cant extra work to build and maintain: the more classes of users there are, the more work is required to keep track of them, and the more marketing e ort is required to create the right incentives to entice customers to upgrade to the next class. Grockit had undertaken this extra e ort because lazy registration was considered an industry best practice. I encouraged the team to try a simple split-test. They took one cohort of customers and required that they register immediately, based on nothing more than Grockit’s marketing materials. To their surprise, this cohort’s behavior was exactly the same as that of the lazy registration group: they had the same rate of registration, activation, and subsequent retention. In other words, the extra e ort of lazy registration was a complete waste even though it was considered an industry best practice.

Even more important than reducing waste was the insight that this test suggested: customers were basing their decision about Grockit on something other than their use of the product. Think about this. Think about the cohort of customers who were required to register for the product before entering a study session with other students. They had very little information about the product, nothing more than was presented on Grockit’s home page and registration page. By contrast, the lazy registration group had a tremendous amount of information about the product because they had used it. Yet despite this information disparity, customer behavior was exactly the same. This suggested that improving Grockit’s positioning and marketing might have a more signi cant impact on attracting new customers than would adding new features. This was just the rst of many important experiments Grockit was able to run. Since those early days, they have expanded their customer base dramatically: they now o er test prep for numerous standardized tests, including the GMAT, SAT, ACT, and GRE, as well as online math and English courses for students in grades 7 through 12. Grockit continues to evolve its process, seeking continuous improvement at every turn. With more than twenty employees in its San Francisco o ce, Grockit continues to operate with the same deliberate, disciplined approach that has been their hallmark all along. They have helped close to a million students and are sure to help millions more. THE VALUE OF THE THREE A’S These examples from Grockit demonstrate each of the three A’s of metrics: actionable, accessible, and auditable. Actionable For a report to be considered actionable, it must demonstrate clear

cause and e ect. Otherwise, it is a vanity metric. The reports that Grockit’s team began to use to judge their learning milestones made it extremely clear what actions would be necessary to replicate the results. By contrast, vanity metrics fail this criterion. Take the number of hits to a company website. Let’s say we have 40,000 hits this month —a new record. What do we need to do to get more hits? Well, that depends. Where are the new hits coming from? Is it from 40,000 new customers or from one guy with an extremely active web browser? Are the hits the result of a new marketing campaign or PR push? What is a hit, anyway? Does each page in the browser count as one hit, or do all the embedded images and multimedia content count as well? Those who have sat in a meeting debating the units of measurement in a report will recognize this problem. Vanity metrics wreak havoc because they prey on a weakness of the human mind. In my experience, when the numbers go up, people think the improvement was caused by their actions, by whatever they were working on at the time. That is why it’s so common to have a meeting in which marketing thinks the numbers went up because of a new PR or marketing e ort and engineering thinks the better numbers are the result of the new features it added. Finding out what is actually going on is extremely costly, and so most managers simply move on, doing the best they can to form their own judgment on the basis of their experience and the collective intelligence in the room. Unfortunately, when the numbers go down, it results in a very di erent reaction: now it’s somebody else’s fault. Thus, most team members or departments live in a world where their department is constantly making things better, only to have their hard work sabotaged by other departments that just don’t get it. Is it any wonder these departments develop their own distinct language, jargon, culture, and defense mechanisms against the bozos working down the hall? Actionable metrics are the antidote to this problem. When cause and e ect is clearly understood, people are better able to learn from their actions. Human beings are innately talented learners

from their actions. Human beings are innately talented learners when given a clear and objective assessment. Accessible All too many reports are not understood by the employees and managers who are supposed to use them to guide their decision making. Unfortunately, most managers do not respond to this complexity by working hand in hand with the data warehousing team to simplify the reports so that they can understand them better. Departments too often spend their energy learning how to use data to get what they want rather than as genuine feedback to guide their future actions. There is an antidote to this misuse of data. First, make the reports as simple as possible so that everyone understands them. Remember the saying “Metrics are people, too.” The easiest way to make reports comprehensible is to use tangible, concrete units. What is a website hit? Nobody is really sure, but everyone knows what a person visiting the website is: one can practically picture those people sitting at their computers. This is why cohort-based reports are the gold standard of learning metrics: they turn complex actions into people-based reports. Each cohort analysis says: among the people who used our product in this period, here’s how many of them exhibited each of the behaviors we care about. In the IMVU example, we saw four behaviors: downloading the product, logging into the product from one’s computer, engaging in a chat with other customers, and upgrading to the paid version of the product. In other words, the report deals with people and their actions, which are far more useful than piles of data points. For example, think about how hard it would have been to tell if IMVU was being successful if we had reported only on the total number of person-to-person conversations. Let’s say we have 10,000 conversations in a period. Is that good? Is that one person being very, very social, or is it 10,000 people each trying the product one time and then giving up? There’s no way to know without creating a more detailed report.

There’s no way to know without creating a more detailed report. As the gross numbers get larger, accessibility becomes more and more important. It is hard to visualize what it means if the number of website hits goes down from 250,000 in one month to 200,000 the next month, but most people understand immediately what it means to lose 50,000 customers. That’s practically a whole stadium full of people who are abandoning the product. Accessibility also refers to widespread access to the reports. Grockit did this especially well. Every day their system automatically generated a document containing the latest data for every single one of their split-test experiments and other leap-offaith metrics. This document was mailed to every employee of the company: they all always had a fresh copy in their e-mail in-boxes. The reports were well laid out and easy to read, with each experiment and its results explained in plain English. Another way to make reports accessible is to use a technique we developed at IMVU. Instead of housing the analytics or data in a separate system, our reporting data and its infrastructure were considered part of the product itself and were owned by the product development team. The reports were available on our website, accessible to anyone with an employee account. Each employee could log in to the system at any time, choose from a list of all current and past experiments, and see a simple one-page summary of the results. Over time, those one-page summaries became the de facto standard for settling product arguments throughout the organization. When people needed evidence to support something they had learned, they would bring a printout with them to the relevant meeting, con dent that everyone they showed it to would understand its meaning. Auditable When informed that their pet project is a failure, most of us are tempted to blame the messenger, the data, the manager, the gods, or anything else we can think of. That’s why the third A of good metrics, “auditable,” is so essential. We must ensure that the data is

metrics, “auditable,” is so essential. We must ensure that the data is credible to employees. The employees at IMVU would brandish one-page reports to demonstrate what they had learned to settle arguments, but the process often wasn’t so smooth. Most of the time, when a manager, developer, or team was confronted with results that would kill a pet project, the loser of the argument would challenge the veracity of the data. Such challenges are more common than most managers would admit, and unfortunately, most data reporting systems are not designed to answer them successfully. Sometimes this is the result of a well-intentioned but misplaced desire to protect the privacy of customers. More often, the lack of such supporting documentation is simply a matter of neglect. Most data reporting systems are not built by product development teams, whose job is to prioritize and build product features. They are built by business managers and analysts. Managers who must use these systems can only check to see if the reports are mutually consistent. They all too often lack a way to test if the data is consistent with reality. The solution? First, remember that “Metrics are people, too.” We need to be able to test the data by hand, in the messy real world, by talking to customers. This is the only way to be able to check if the reports contain true facts. Managers need the ability to spot check the data with real customers. It also has a second bene t: systems that provide this level of auditability give managers and entrepreneurs the opportunity to gain insights into why customers are behaving the way the data indicate. Second, those building reports must make sure the mechanisms that generate the reports are not too complex. Whenever possible, reports should be drawn directly from the master data, rather than from an intermediate system, which reduces opportunities for error. I have noticed that every time a team has one of its judgments or assumptions overturned as a result of a technical problem with the data, its confidence, morale, and discipline are undermined.

When we watch entrepreneurs succeed in the mythmaking world of Hollywood, books, and magazines, the story is always structured the same way. First, we see the plucky protagonist having an epiphany, hatching a great new idea. We learn about his or her character and personality, how he or she came to be in the right place at the right time, and how he or she took the dramatic leap to start a business. Then the photo montage begins. It’s usually short, just a few minutes of time-lapse photography or narrative. We see the protagonist building a team, maybe working in a lab, writing on whiteboards, closing sales, pounding on a few keyboards. At the end of the montage, the founders are successful, and the story can move on to more interesting fare: how to split the spoils of their success, who will appear on magazine covers, who sues whom, and implications for the future. Unfortunately, the real work that determines the success of startups happens during the photo montage. It doesn’t make the cut in terms of the big story because it is too boring. Only 5 percent of entrepreneurship is the big idea, the business model, the whiteboard strategizing, and the splitting up of the spoils. The other 95 percent is the gritty work that is measured by innovation accounting: product prioritization decisions, deciding which customers to target or listen to, and having the courage to subject a grand vision to constant testing and feedback. One decision stands out above all others as the most di cult, the most time-consuming, and the biggest source of waste for most startups. We all must face this fundamental test: deciding when to pivot and when to persevere. To understand what happens during the photo montage, we have to understand how to pivot, and that is the subject of Chapter 8.

8 PIVOT (OR PERSEVERE) entrepreneur eventually faces an overriding challenge in Every developing a successful product: deciding when to pivot and when to persevere. Everything that has been discussed so far is a

prelude to a seemingly simple question: are we making su cient progress to believe that our original strategic hypothesis is correct, or do we need to make a major change? That change is called a pivot: a structured course correction designed to test a new fundamental hypothesis about the product, strategy, and engine of growth. Because of the scienti c methodology that underlies the Lean Startup, there is often a misconception that it o ers a rigid clinical formula for making pivot or persevere decisions. This is not true. There is no way to remove the human element—vision, intuition, judgment—from the practice of entrepreneurship, nor would that be desirable. My goal in advocating a scienti c approach to the creation of startups is to channel human creativity into its most productive form, and there is no bigger destroyer of creative potential than the misguided decision to persevere. Companies that cannot bring themselves to pivot to a new direction on the basis of feedback from the marketplace can get stuck in the land of the living dead, neither growing enough nor dying, consuming resources and commitment from employees and other stakeholders but not moving ahead. There is good news about our reliance on judgment, though. We

There is good news about our reliance on judgment, though. We are able to learn, we are innately creative, and we have a remarkable ability to see the signal in the noise. In fact, we are so good at this that sometimes we see signals that aren’t there. The heart of the scienti c method is the realization that although human judgment may be faulty, we can improve our judgment by subjecting our theories to repeated testing. Startup productivity is not about cranking out more widgets or features. It is about aligning our e orts with a business and product that are working to create value and drive growth. In other words, successful pivots put us on a path toward growing a sustainable business. INNOVATION ACCOUNTING LEADS TO FASTER PIVOTS To see this process in action, meet David Binetti, the CEO of Votizen. David has had a long career helping to bring the American political process into the twenty- rst century. In the early 1990s, he helped build USA.gov, the rst portal for the federal government. He’s also experienced some classic startup failures. When it came time to build Votizen, David was determined to avoid betting the farm on his vision. David wanted to tackle the problem of civic participation in the political process. His rst product concept was a social network of veri ed voters, a place where people passionate about civic causes could get together, share ideas, and recruit their friends. David built his first minimum viable product for just over $1,200 in about three months and launched it. David wasn’t building something that nobody wanted. In fact, from its earliest days, Votizen was able to attract early adopters who loved the core concept. Like all entrepreneurs, David had to re ne his product and business model. What made David’s challenge especially hard was that he had to make those pivots in the face of moderate success. David’s initial concept involved four big leaps of faith:

1. Customers would be interested enough in the social network to sign up. (Registration) 2. Votizen would be able to verify them as registered voters. (Activation) 3. Customers who were veri ed voters would engage with the site’s activism tools over time. (Retention) 4. Engaged customers would tell their friends about the service and recruit them into civic causes. (Referral) Three months and $1,200 later, David’s rst MVP was in customers’ hands. In the initial cohorts, 5 percent signed up for the service and 17 percent veri ed their registered voter status (see the chart below). The numbers were so low that there wasn’t enough data to tell what sort of engagement or referral would occur. It was time to start iterating. Registration Activation Retention Referral

INITIAL MVP 5% 17% Too low Too low

David spent the next two months and another $5,000 split testing new product features, messaging, and improving the product’s design to make it easier to use. Those tests showed dramatic improvements, going from a 5 percent registration rate to 17 percent and from a 17 percent activation rate to over 90 percent. Such is the power of split testing. This optimization gave David a critical mass of customers with which to measure the next two leaps of faith. However, as shown in the chart below, those numbers proved to be even more discouraging: David achieved a referral rate of only 4 percent and a retention rate of 5 percent.

Registration Activation Retention Referral

INITIAL MVP 5% 17% Too low Too low

AFTER OPTIMIZATION 17% 90% 5% 4%

David knew he had to do more development and testing. For the next three months he continued to optimize, split test, and re ne his pitch. He talked to customers, held focus groups, and did countless A/B experiments. As was explained in Chapter 7, in a split test, di erent versions of a product are o ered to di erent customers at the same time. By observing the changes in behavior between the two groups, one can make inferences about the impact of the di erent variations. As shown in the chart below, the referral rate nudged up slightly to 6 percent and the retention rate went up to 8 percent. A disappointed David had spent eight months and $20,000 to build a product that wasn’t living up to the growth model he’d hoped for. Registration Activation Retention Referral

BEFORE OPTIMIZATION AFTER OPTIMIZATION 17% 17% 90% 90% 5% 8% 4% 6%

David faced the di cult challenge of deciding whether to pivot or persevere. This is one of the hardest decisions entrepreneurs face. The goal of creating learning milestones is not to make the decision

easy; it is to make sure that there is relevant data in the room when it comes time to decide. Remember, at this point David has had many customer conversations. He has plenty of learning that he can use to rationalize the failure he has experienced with the current product. That’s exactly what many entrepreneurs do. In Silicon Valley, we call this experience getting stuck in the land of the living dead. It happens when a company has achieved a modicum of success—just enough to stay alive—but is not living up to the expectations of its founders and investors. Such companies are a terrible drain of human energy. Out of loyalty, the employees and founders don’t want to give in; they feel that success might be just around the corner. David had two advantages that helped him avoid this fate: 1. Despite being committed to a signi cant vision, he had done his best to launch early and iterate. Thus, he was facing a pivot or persevere moment just eight months into the life of his company. The more money, time, and creative energy that has been sunk into an idea, the harder it is to pivot. David had done well to avoid that trap. 2. David had identi ed his leap-of-faith questions explicitly at the outset and, more important, had made quantitative predictions about each of them. It would not have been di cult for him to declare success retroactively from that initial venture. After all, some of his metrics, such as activation, were doing quite well. In terms of gross metrics such as total usage, the company had positive growth. It is only because David focused on actionable metrics for each of his leap-of-faith questions that he was able t o accept that his company was failing. In addition, because David had not wasted energy on premature PR, he was able to make this determination without public embarrassment or distraction. Failure is a prerequisite to learning. The problem with the notion of shipping a product and then seeing what happens is that you are

guaranteed to succeed—at seeing what happens. But then what? As soon as you have a handful of customers, you’re likely to have ve opinions about what to do next. Which should you listen to? Votizen’s results were okay, but they were not good enough. David felt that although his optimization was improving the metrics, they were not trending toward a model that would sustain the business overall. But like all good entrepreneurs, he did not give up prematurely. David decided to pivot and test a new hypothesis. A pivot requires that we keep one foot rooted in what we’ve learned so far, while making a fundamental change in strategy in order to seek even greater validated learning. In this case, David’s direct contact with customers proved essential. He had heard three recurring bits of feedback in his testing: 1. “I always wanted to get more involved; this makes it so much easier.” 2. “The fact that you prove I’m a voter matters.” 3. “There’s no one here. What’s the point of coming back?”1 David decided to undertake what I call a zoom-in pivot, refocusing the product on what previously had been considered just one feature of a larger whole. Think of the customer comments above: customers like the concept, they like the voter registration technology, but they aren’t getting value out of the social networking part of the product. David decided to change Votizen into a product called @2gov, a “social lobbying platform.” Rather than get customers integrated in a civic social network, @2gov allows them to contact their elected representatives quickly and easily via existing social networks such as Twitter. The customer engages digitally, but @2gov translates that digital contact into paper form. Members of Congress receive old-fashioned printed letters and petitions as a result. In other words, @2gov translates the high-tech world of its customers into the low-tech world of politics. @2gov had a slightly di erent set of leap-of-faith questions to

@2gov had a slightly di erent set of leap-of-faith questions to answer. It still depended on customers signing up, verifying their voter status, and referring their friends, but the growth model changed. Instead of relying on an engagement-driven business (“sticky” growth), @2gov was more transactional. David’s hypothesis was that passionate activists would be willing to pay money to have @2gov facilitate contacts on behalf of voters who cared about their issues. David’s new MVP took four months and another $30,000. He’d now spent a grand total of $50,000 and worked for twelve months. But the results from his next round of testing were dramatic: registration rate 42 percent, activation 83 percent, retention 21 percent, and referral a whopping 54 percent. However, the number of activists willing to pay was less than 1 percent. The value of each transaction was far too low to sustain a pro table business even after David had done his best to optimize it. Before we get to David’s next pivot, notice how convincingly he was able to demonstrate validated learning. He hoped that with this new product, he would be able to improve his leap-of-faith metrics dramatically, and he did (see the chart below). Engine of growth Registration rate Activation Retention Referral Revenue Lifetime value (LTV)

BEFORE PIVOT Sticky 17% 90% 8% 6% n/a n/a

AFTER PIVOT Paid 42% 83% 21% 54% 1% Minimal

He did this not by working harder but by working smarter, taking

He did this not by working harder but by working smarter, taking his product development resources and applying them to a new and di erent product. Compared with the previous four months of optimization, the new four months of pivoting had resulted in a dramatically higher return on investment, but David was still stuck in an age-old entrepreneurial trap. His metrics and product were improving, but not fast enough. David pivoted again. This time, rather than rely on activists to pay money to drive contacts, he went to large organizations, professional fund-raisers, and big companies, which all have a professional or business interest in political campaigning. The companies seemed extremely eager to use and pay for David’s service, and David quickly signed letters of intent to build the functionality they needed. In this pivot, David did what I call a customer segment pivot, keeping the functionality of the product the same but changing the audience focus. He focused on who pays: from consumers to businesses and nonpro t organizations. In other words, David went from being a business-to-consumer (B2C) company to being a business-to-business (B2B) company. In the process he changed his planned growth model, as well to one where he would be able to fund growth out of the pro ts generated from each B2B sale. Three months later, David had built the functionality he had promised, based on those early letters of intent. But when he went back to companies to collect his checks, he discovered more problems. Company after company procrastinated, delayed, and ultimately passed up the opportunity. Although they had been excited enough to sign a letter of intent, closing a real sale was much more di cult. It turned out that those companies were not early adopters. On the basis of the letters of intent, David had increased his head count, taking on additional sales sta and engineers in anticipation of having to service higher-margin business-to-business accounts. When the sales didn’t materialize, the whole team had to work harder to try to nd revenue elsewhere. Yet no matter how many sales calls they went on and no matter how much optimization they did to the product, the model wasn’t working. Returning to his

leap-of-faith questions, David concluded that the results refuted his business-to-business hypothesis, and so he decided to pivot once again. All this time, David was learning and gaining feedback from his potential customers, but he was in an unsustainable situation. You can’t pay sta with what you’ve learned, and raising money at that juncture would have escalated the problem. Raising money without early traction is not a certain thing. If he had been able to raise money, he could have kept the company going but would have been pouring money into a value-destroying engine of growth. He would be in a high-pressure situation: use investor’s cash to make the engine of growth work or risk having to shut down the company (or be replaced). David decided to reduce sta and pivot again, this time attempting what I call a platform pivot. Instead of selling an application to one customer at a time, David envisioned a new growth model inspired by Google’s AdWords platform. He built a self-serve sales platform where anyone could become a customer with just a credit card. Thus, no matter what cause you were passionate about, you could go to @2gov’s website and @2gov would help you nd new people to get involved. As always, the new people were veri ed registered voters, and so their opinions carried weight with elected officials. The new product took only one additional month to build and immediately showed results: 51 percent sign-up rate, 92 percent activation rate, 28 percent retention rate, 64 percent referral rate (see the chart below). Most important, 11 percent of these customers were willing to pay 20 cents per message. Most important, this was the beginning of an actual growth model that could work. Receiving 20 cents per message might not sound like much, but the high referral rate meant that @2gov could grow its tra c without spending signi cant marketing money (this is the viral engine of growth). BEFORE PIVOT

AFTER PIVOT

Engine of growth Registration rate Activation Retention Referral Revenue Lifetime value (LTV)

Paid 42% 83% 21% 54% 1% Minimal

Viral 51% 92% 28% 64% 11% $0.20 per message

Votizen’s story exhibits some common patterns. One of the most important to note is the acceleration of MVPs. The rst MVP took eight months, the next four months, then three, then one. Each time David was able to validate or refute his next hypothesis faster than before. How can one explain this acceleration? It is tempting to credit it to the product development work that had been going on. Many features had been created, and with them a fair amount of infrastructure. Therefore, each time the company pivoted, it didn’t have to start from scratch. But this is not the whole story. For one thing, much of the product had to be discarded between pivots. Worse, the product that remained was classified as a legacy product, one that was no longer suited to the goals of the company. As is usually the case, the e ort required to reform a legacy product took extra work. Counteracting these forces were the hard-won lessons David had learned through each milestone. Votizen accelerated its MVP process because it was learning critical things about its customers, market, and strategy. Today, two years after its inception, Votizen is doing well. They recently raised $1.5 million from Facebook’s initial investor Peter Thiel, one of the very few consumer Internet investments he has made in recent years. Votizen’s system now can process voter identity in real time for forty-seven states representing 94 percent of

the U.S. population and has delivered tens of thousands of messages to Congress. The Startup Visa campaign used Votizen’s tools to introduce the Startup Visa Act (S.565), which is the rst legislation introduced into the Senate solely as a result of social lobbying. These activities have attracted the attention of established Washington consultants who are seeking to employ Votizen’s tools in future political campaigns. David Binetti sums up his experience building a Lean Startup: In 2003 I started a company in roughly the same space as I’m in today. I had roughly the same domain expertise and industry credibility, fresh o the USA.gov success. But back then my company was a total failure (despite consuming signi cantly greater investment), while now I have a business making money and closing deals. Back then I did the traditional linear product development model, releasing an amazing product (it really was) after 12 months of development, only to nd that no one would buy it. This time I produced four versions in twelve weeks and generated my rst sale relatively soon after that. And it isn’t just market timing—two other companies that launched in a similar space in 2003 subsequently sold for tens of millions of dollars, and others in 2010 followed a linear model straight to the dead pool. A STARTUP’S RUNWAY IS THE NUMBER OF PIVOTS IT CAN STILL MAKE Seasoned entrepreneurs often speak of the runway that their startup has left: the amount of time remaining in which a startup must either achieve lift-o or fail. This usually is de ned as the remaining cash in the bank divided by the monthly burn rate, or net drain on that account balance. For example, a startup with $1 million in the bank that is spending $100,000 per month has a projected runway of ten months.

projected runway of ten months. When startups start to run low on cash, they can extend the runway two ways: by cutting costs or by raising additional funds. But when entrepreneurs cut costs indiscriminately, they are as liable to cut the costs that are allowing the company to get through its Build-Measure-Learn feedback loop as they are to cut waste. If the cuts result in a slowdown to this feedback loop, all they have accomplished is to help the startup go out of business more slowly. The true measure of runway is how many pivots a startup has left: the number of opportunities it has to make a fundamental change to its business strategy. Measuring runway through the lens of pivots rather than that of time suggests another way to extend that runway: get to each pivot faster. In other words, the startup has to nd ways to achieve the same amount of validated learning at lower cost or in a shorter time. All the techniques in the Lean Startup model that have been discussed so far have this as their overarching goal. PIVOTS REQUIRE COURAGE Ask most entrepreneurs who have decided to pivot and they will tell you that they wish they had made the decision sooner. I believe there are three reasons why this happens. First, vanity metrics can allow entrepreneurs to form false conclusions and live in their own private reality. This is particularly damaging to the decision to pivot because it robs teams of the belief that it is necessary to change. When people are forced to change against their better judgment, the process is harder, takes longer, and leads to a less decisive outcome. Second, when an entrepreneur has an unclear hypothesis, it’s almost impossible to experience complete failure, and without failure there is usually no impetus to embark on the radical change a pivot requires. As I mentioned earlier, the failure of the “launch it and see what happens” approach should now be evident: you will always succeed—in seeing what happens. Except in rare cases, the early results will be ambiguous, and you won’t know whether to

pivot or persevere, whether to change direction or stay the course. Third, many entrepreneurs are afraid. Acknowledging failure can lead to dangerously low morale. Most entrepreneurs’ biggest fear is not that their vision will prove to be wrong. More terrifying is the thought that the vision might be deemed wrong without having been given a real chance to prove itself. This fear drives much of the resistance to the minimum viable product, split testing, and other techniques to test hypotheses. Ironically, this fear drives up the risk because testing doesn’t occur until the vision is fully represented. However, by that time it is often too late to pivot because funding is running out. To avoid this fate, entrepreneurs need to face their fears and be willing to fail, often in a public way. In fact, entrepreneurs who have a high pro le, either because of personal fame or because they are operating as part of a famous brand, face an extreme version of this problem. A new startup in Silicon Valley called Path was started by experienced entrepreneurs: Dave Morin, who previously had overseen Facebook’s platform initiative; Dustin Mierau, product designer and cocreator of Macster; and Shawn Fanning of Napster fame. They decided to release a minimum viable product in 2010. Because of the high-pro le nature of its founders, the MVP attracted signi cant press attention, especially from technology and startup blogs. Unfortunately, their product was not targeted at technology early adopters, and as a result, the early blogger reaction was quite negative. (Many entrepreneurs fail to launch because they are afraid of this kind of reaction, worrying that it will harm the morale of the entire company. The allure of positive press, especially in our “home” industry, is quite strong.) Luckily, the Path team had the courage to ignore this fear and focus on what their customers said. As a result, they were able to get essential early feedback from actual customers. Path’s goal is to create a more personal social network that maintains its quality over time. Many people have had the experience of being overconnected on existing social networks, sharing with past coworkers, high school friends, relatives, and colleagues. Such broad groups make it hard to share intimate moments. Path took an

unusual approach. For example, it limited the number of connections to fty, based on brain research by the anthropologist Robin Dunbar at Oxford. His research suggests that fty is roughly the number of personal relationships in any person’s life at any given time. For members of the tech press (and many tech early adopters) this “arti cial” constraint on the number of connections was anathema. They routinely use new social networking products with thousands of connections. Fifty seemed way too small. As a result, Path endured a lot of public criticism, which was hard to ignore. But customers ocked to the platform, and their feedback was decidedly di erent from the negativity in the press. Customers liked the intimate moments and consistently wanted features that were not on the original product road map, such as the ability to share how friends’ pictures made them feel and the ability to share “video moments.” Dave Morin summed up his experience this way: The reality of our team and our backgrounds built up a massive wall of expectations. I don’t think it would have mattered what we would have released; we would have been met with expectations that are hard to live up to. But to us it just meant we needed to get our product and our vision out into the market broadly in order to get feedback and to begin iteration. We humbly test our theories and our approach to see what the market thinks. Listen to feedback honestly. And continue to innovate in the directions we think will create meaning in the world. Path’s story is just beginning, but already their courage in facing down critics is paying o . If and when they need to pivot, they won’t be hampered by fear. They recently raised $8.5 million in venture capital in a round led by Kleiner Perkins Cau eld & Byers. In doing so, Path reportedly turned down an acquisition o er for $100 million from Google.2

THE PIVOT OR PERSEVERE MEETING The decision to pivot requires a clear-eyed and objective mind-set. We’ve discussed the telltale signs of the need to pivot: the decreasing e ectiveness of product experiments and the general feeling that product development should be more productive. Whenever you see those symptoms, consider a pivot. The decision to pivot is emotionally charged for any startup and has to be addressed in a structured way. One way to mitigate this challenge is to schedule the meeting in advance. I recommend that every startup have a regular “pivot or persevere” meeting. In my experience, less than a few weeks between meetings is too often and more than a few months is too infrequent. However, each startup needs to find its own pace. Each pivot or persevere meeting requires the participation of both the product development and business leadership teams. At IMVU, we also added the perspectives of outside advisers who could help us see past our preconceptions and interpret data in new ways. The product development team must bring a complete report of the results of its product optimization e orts over time (not just the past period) as well as a comparison of how those results stack up against expectations (again, over time). The business leadership should bring detailed accounts of their conversations with current and potential customers. Let’s take a look at this process in action in a dramatic pivot done by a company called Wealthfront. That company was founded in 2007 by Dan Carroll and added Andy Rachle as CEO shortly thereafter. Andy is a well-known gure in Silicon Valley: he is a cofounder and former general partner of the venture capital rm Benchmark Capital and is on the faculty of the Stanford Graduate School of Business, where he teaches a variety of courses on technology entrepreneurship. I rst met Andy when he commissioned a case study on IMVU to teach his students about the process we had used to build the company. Wealthfront’s mission is to disrupt the mutual fund industry by bringing greater transparency, access, and value to retail investors.

What makes Wealthfront’s story unusual, however, is not where it is today but how it began: as an online game. In Wealthfront’s original incarnation it was called kaChing and was conceived as a kind of fantasy league for amateur investors. It allowed anyone to open a virtual trading account and build a portfolio that was based on real market data without having to invest real money. The idea was to identify diamonds in the rough: amateur traders who lacked the resources to become fund managers but who possessed market insight. Wealthfront’s founders did not want to be in the online gaming business per se; kaChing was part of a sophisticated strategy in the service of their larger vision. Any student of disruptive innovation would have looked on approvingly: they were following that system perfectly by initially serving customers who were unable to participate in the mainstream market. Over time, they believed, the product would become more and more sophisticated, eventually allowing users to serve (and disrupt) existing professional fund managers. To identify the best amateur trading savants, Wealthfront built sophisticated technology to rate the skill of each fund manager, using techniques employed by the most sophisticated evaluators of money managers, the premier U.S. university endowments. Those methods allowed them to evaluate not just the returns the managers generated but also the amount of risk they had taken along with how consistent they performed relative to their declared investment strategy. Thus, fund managers who achieved great returns through reckless gambles (i.e., investments outside their area of expertise) would be ranked lower than those who had figured out how to beat the market through skill. With its kaChing game, Wealthfront hoped to test two leap-offaith assumptions: 1. A signi cant percentage of the game players would demonstrate enough talent as virtual fund managers to prove themselves suitable to become managers of real assets (the value hypothesis).

2. The game would grow using the viral engine of growth and generate value using a freemium business model. The game was free to play, but the team hoped that a percentage of the players would realize that they were lousy traders and therefore want to convert to paying customers once Wealthfront started o ering real asset management services (the growth hypothesis). kaChing was a huge early success, attracting more than 450,000 gamers in its initial launch. By now, you should be suspicious of this kind of vanity metric. Many less disciplined companies would have celebrated that success and felt their future was secure, but Wealthfront had identi ed its assumptions clearly and was able to think more rigorously. By the time Wealthfront was ready to launch its paid nancial product, only seven amateur managers had quali ed as worthy of managing other people’s money, far less than the ideal model had anticipated. After the paid product launched, they were able to measure the conversion rate of gamers into paying customers. Here too the numbers were discouraging: the conversion rate was close to zero. Their model had predicted that hundreds of customers would sign up, but only fourteen did. The team worked valiantly to nd ways to improve the product, but none showed any particular promise. It was time for a pivot or persevere meeting. If the data we have discussed so far was all that was available at that critical meeting, Wealthfront would have been in trouble. They would have known that their current strategy wasn’t working but not what to do to x it. That is why it was critical that they followed the recommendation earlier in this chapter to investigate alternative possibilities. In this case, Wealthfront had pursued two important lines of inquiry. The rst was a series of conversations with professional money managers, beginning with John Powers, the head of Stanford University’s endowment, who reacted surprisingly positively. Wealthfront’s strategy was premised on the assumption that professional money managers would be reluctant to join the system

because the increased transparency would threaten their sense of authority. Powers had no such concerns. CEO Andy Rachle then began a series of conversations with other professional investment managers and brought the results back to the company. His insights were as follows: 1. Successful professional money managers felt they had nothing to fear from transparency, since they believed it would validate their skills. 2. Money managers faced signi cant challenges in managing and scaling their own businesses. They were hampered by the di culty of servicing their own accounts and therefore had to require high minimum investments as a way to screen new clients. The second problem was so severe that Wealthfront was elding cold calls from professional managers asking out of the blue to join the platform. These were classic early adopters who had the vision to see past the current product to something they could use to achieve a competitive advantage. The second critical qualitative information came out of conversations with consumers. It turned out that they found the blending of virtual and real portfolio management on the kaChing website confusing. Far from being a clever way of acquiring customers, the freemium strategy was getting in the way by promoting confusion about the company’s positioning. This data informed the pivot or persevere meeting. With everyone present, the team debated what to do with its future. The current strategy wasn’t working, but many employees were nervous about abandoning the online game. After all, it was an important part of what they had signed on to build. They had invested signi cant time and energy building and supporting those customers. It was painful—as it always is—to realize that that energy had been wasted. Wealthfront decided it could not persevere as it existed. The company chose instead to celebrate what it had learned. If it had

not launched its current product, the team never would have learned what it needed to know to pivot. In fact, the experience taught them something essential about their vision. As Andy says, “What we really wanted to change was not who manages the money but who has access to the best possible talent. We’d originally thought we’d need to build a signi cant business with amateur managers to get professionals to come on board, but fortunately it turns out that wasn’t necessary.” The company pivoted, abandoning the gaming customers altogether and focusing on providing a service that allowed customers to invest with professional managers. On the surface, the pivot seems quite dramatic in that the company changed its positioning, its name, and its partner strategy. It even jettisoned a large proportion of the features it had built. But at its core, a surprising amount stayed the same. The most valuable work the company had done was building technology to evaluate managers’ e ectiveness, and this became the kernel around which the new business was built. This is also common with pivots; it is not necessary to throw out everything that came before and start over. Instead, it’s about repurposing what has been built and what has been learned to find a more positive direction. Today, Wealthfront is prospering as a result of its pivot, with over $180 million invested on the platform and more than forty professional managers.3 It recently was named one of Fast Company’s ten most innovative companies in nance.4 The company continues to operate with agility, scaling in line with the growth principles outlined in Chapter 12. Wealthfront is also a leading advocate of the development technique known as continuous deployment, which we’ll discuss in Chapter 9. FAILURE TO PIVOT The decision to pivot is so di cult that many companies fail to make it. I wish I could say that every time I was confronted with the need to pivot, I handled it well, but this is far from true. I

the need to pivot, I handled it well, but this is far from true. I remember one failure to pivot especially well. A few years after IMVU’s founding, the company was having tremendous success. The business had grown to over $1 million per month in revenue; we had created more than twenty million avatars for our customers. We managed to raise signi cant new rounds of nancing, and like the global economy, we were riding high. But danger lurked around the corner. Unknowingly, we had fallen into a classic startup trap. We had been so successful with our early e orts that we were ignoring the principles behind them. As a result, we missed the need to pivot even as it stared us in the face. We had built an organization that excelled at the kinds of activities described in earlier chapters: creating minimum viable products to test new ideas and running experiments to tune the engine of growth. Before we had begun to enjoy success, many people had advised against our “low-quality” minimum viable product and experimental approach, urging us to slow down. They wanted us to do things right and focus on quality instead of speed. We ignored that advice, mostly because we wanted to claim the advantages of speed. After our approach was vindicated, the advice we received changed. Now most of the advice we heard was that “you can’t argue with success,” urging us to stay the course. We liked this advice better, but it was equally wrong. Remember that the rationale for building low-quality MVPs is that developing any features beyond what early adopters require is a form of waste. However, the logic of this takes you only so far. Once you have found success with early adopters, you want to sell to mainstream customers. Mainstream customers have di erent requirements and are much more demanding. The kind of pivot we needed is called a customer segment pivot. In this pivot, the company realizes that the product it’s building solves a real problem for real customers but that they are not the customers it originally planned to serve. In other words, the product hypothesis is con rmed only partially. (This chapter described such a pivot in the Votizen story, above.) A customer segment pivot is an especially tricky pivot to execute

A customer segment pivot is an especially tricky pivot to execute because, as we learned the hard way at IMVU, the very actions that made us successful with early adopters were diametrically opposed to the actions we’d have to master to be successful with mainstream customers. We lacked a clear understanding of how our engine of growth operated. We had begun to trust our vanity metrics. We had stopped using learning milestones to hold ourselves accountable. Instead, it was much more convenient to focus on the ever-larger gross metrics that were so exciting: breaking new records in signing up paying customers and active users, monitoring our customer retention rate—you name it. Under the surface, it should have been clear that our efforts at tuning the engine were reaching diminishing returns, the classic sign of the need to pivot. For example, we spent months trying to improve the product’s activation rate (the rate at which new customers become active consumers of the product), which remained stubbornly low. We did countless experiments: usability improvements, new persuasion techniques, incentive programs, customer quests, and other gamelike features. Individually, many of these new features and new marketing tools were successful. We measured them rigorously, using A/B experimentation. But taken in aggregate, over the course of many months, we were seeing negligible changes in the overall drivers of our engine of growth. Even our activation rate, which had been the center of our focus, edged up only a few percentage points. We ignored the signs because the company was still growing, delivering month after month of “up and to the right” results. But we were quickly exhausting our early adopter market. It was getting harder and harder to nd customers we could acquire at the prices we were accustomed to paying. As we drove our marketing team to nd more customers, they were forced to reach out more to mainstream customers, but mainstream customers are less forgiving of an early product. The activation and monetization rates of new customers started to go down, driving up the cost of acquiring new customers. Pretty soon, our growth was atlining and our engine sputtered and stalled. It took us far too long to make the changes necessary to x this

It took us far too long to make the changes necessary to x this situation. As with all pivots, we had to get back to basics and start the innovation accounting cycle over. It felt like the company’s second founding. We had gotten really good at optimizing, tuning, and iterating, but in the process we had lost sight of the purpose of those activities: testing a clear hypothesis in the service of the company’s vision. Instead, we were chasing growth, revenue, and profits wherever we could find them. We needed to reacquaint ourselves with our new mainstream customers. Our interaction designers led the way by developing a clear customer archetype that was based on extensive in-person conversations and observation. Next, we needed to invest heavily in a major product overhaul designed to make the product dramatically easier to use. Because of our overfocus on ne-tuning, we had stopped making large investments like these, preferring to invest in lower-risk and lower-yield testing experiments. However, investing in quality, design, and larger projects did not require that we abandon our experimental roots. On the contrary, once we realized our mistake and executed the pivot, those skills served us well. We created a sandbox for experimentation like the one described in Chapter 12 and had a cross-functional team work exclusively on this major redesign. As they built, they continuously tested their new design head to head against the old one. Initially, the new design performed worse than the old one, as is usually the case. It lacked the features and functionality of the old design and had many new mistakes as well. But the team relentlessly improved the design until, months later, it performed better. This new design laid the foundation for our future growth. This foundation has paid o handsomely. By 2009, revenue had more than doubled to over $25 million annually. But we might have enjoyed that success earlier if we had pivoted sooner.5 A CATALOG OF PIVOTS Pivots come in di erent avors. The word pivot sometimes is used incorrectly as a synonym for change. A pivot is a special kind of

incorrectly as a synonym for change. A pivot is a special kind of change designed to test a new fundamental hypothesis about the product, business model, and engine of growth. Zoom-in Pivot In this case, what previously was considered a single feature in a product becomes the whole product. This is the type of pivot Votizen made when it pivoted away from a full social network and toward a simple voter contact product. Zoom-out Pivot In the reverse situation, sometimes a single feature is insu cient to support a whole product. In this type of pivot, what was considered the whole product becomes a single feature of a much larger product. Customer Segment Pivot In this pivot, the company realizes that the product it is building solves a real problem for real customers but that they are not the type of customers it originally planned to serve. In other words, the product hypothesis is partially con rmed, solving the right problem, but for a different customer than originally anticipated. Customer Need Pivot As a result of getting to know customers extremely well, it sometimes becomes clear that the problem we’re trying to solve for them is not very important. However, because of this customer intimacy, we often discover other related problems that are important and can be solved by our team. In many cases, these related problems may require little more than repositioning the

related problems may require little more than repositioning the existing product. In other cases, it may require a completely new product. Again, this a case where the product hypothesis is partially con rmed; the target customer has a problem worth solving, just not the one that was originally anticipated. A famous example is the chain Potbelly Sandwich Shop, which today has over two hundred stores. It began as an antique store in 1977; the owners started to sell sandwiches as a way to bolster tra c to their stores. Pretty soon they had pivoted their way into an entirely different line of business. Platform Pivot A platform pivot refers to a change from an application to a platform or vice versa. Most commonly, startups that aspire to create a new platform begin life by selling a single application, the so-called killer app, for their platform. Only later does the platform emerge as a vehicle for third parties to leverage as a way to create their own related products. However, this order is not always set in stone, and some companies have to execute this pivot multiple times. Business Architecture Pivot This pivot borrows a concept from Geo rey Moore, who observed that companies generally follow one of two major business architectures: high margin, low volume (complex systems model) or low margin, high volume (volume operations model).6 The former commonly is associated with business to business (B2B) or enterprise sales cycles, and the latter with consumer products (there are notable exceptions). In a business architecture pivot, a startup switches architectures. Some companies change from high margin, low volume by going mass market (e.g., Google’s search “appliance”); others, originally designed for the mass market, turned out to require long and expensive sales cycles.

Value Capture Pivot There are many ways to capture the value a company creates. These methods are referred to commonly as monetization or revenue models. These terms are much too limiting. Implicit in the idea of monetization is that it is a separate “feature” of a product that can be added or removed at will. In reality, capturing value is an intrinsic part of the product hypothesis. Often, changes to the way a company captures value can have far-reaching consequences for the rest of the business, product, and marketing strategies. Engine of Growth Pivot As we’ll see in Chapter 10, there are three primary engines of growth that power startups: the viral, sticky, and paid growth models. In this type of pivot, a company changes its growth strategy to seek faster or more profitable growth. Commonly but not always, the engine of growth also requires a change in the way value is captured. Channel Pivot In traditional sales terminology, the mechanism by which a company delivers its product to customers is called the sales channel or distribution channel. For example, consumer packaged goods are sold in a grocery store, cars are sold in dealerships, and much enterprise software is sold (with extensive customization) by consulting and professional services rms. Often, the requirements of the channel determine the price, features, and competitive landscape of a product. A channel pivot is a recognition that the same basic solution could be delivered through a di erent channel with greater e ectiveness. Whenever a company abandons a previously complex sales process to “sell direct” to its end users, a channel pivot is in progress.

It is precisely because of its destructive e ect on sales channels that the Internet has had such a disruptive in uence in industries that previously required complex sales and distribution channels, such as newspaper, magazine, and book publishing. Technology Pivot Occasionally, a company discovers a way to achieve the same solution by using a completely di erent technology. Technology pivots are much more common in established businesses. In other words, they are a sustaining innovation, an incremental improvement designed to appeal to and retain an existing customer base. Established companies excel at this kind of pivot because so much is not changing. The customer segment is the same, the customer’s problem is the same, the value-capture model is the same, and the channel partners are the same. The only question is whether the new technology can provide superior price and/or performance compared with the existing technology. A PIVOT IS A STRATEGIC HYPOTHESIS Although the pivots identi ed above will be familiar to students of business strategy, the ability to pivot is no substitute for sound strategic thinking. The problem with providing famous examples of pivots is that most people are familiar only with the successful end strategies of famous companies. Most readers know that Southwest or Walmart is an example of a low-cost disruption in their markets, that Microsoft an example of a platform monopoly, and that Starbucks has leveraged a powerful premium brand. What is generally less well known are the pivots that were required to discover those strategies. Companies have a strong incentive to align their PR stories around the heroic founder and make it seem that their success was the inevitable result of a good idea. Thus, although startups often pivot into a strategy that seems

similar to that of a successful company, it is important not to put too much stock in these analogies. It’s extremely di cult to know if the analogy has been drawn properly. Have we copied the essential features or just super cial ones? Will what worked in that industry work in ours? Will what has worked in the past work today? A pivot is better understood as a new strategic hypothesis that will require a new minimum viable product to test. Pivots are a permanent fact of life for any growing business. Even after a company achieves initial success, it must continue to pivot. Those familiar with the technology life cycle ideas of theorists such as Geo rey Moore know certain later-stage pivots by the names he has given them: the Chasm, the Tornado, the Bowling Alley. Readers of the disruptive innovation literature spearheaded by Harvard’s Clayton Christensen will be familiar with established companies that fail to pivot when they should. The critical skill for managers today is to match those theories to their present situation so that they apply the right advice at the right time. Modern managers cannot have escaped the deluge of recent books calling on them to adapt, change, reinvent, or upend their existing businesses. Many of the works in this category are long on exhortations and short on specifics. A pivot is not just an exhortation to change. Remember, it is a special kind of structured change designed to test a new fundamental hypothesis about the product, business model, and engine of growth. It is the heart of the Lean Startup method. It is what makes the companies that follow Lean Startup resilient in the face of mistakes: if we take a wrong turn, we have the tools we need to realize it and the agility to find another path. In Part Two, we have looked at a startup idea from its initial leaps of faith, tested it with a minimum viable product, used innovation accounting and actionable metrics to evaluate the results, and made the decision to pivot or persevere. I have treated these subjects in great detail to prepare for what

I have treated these subjects in great detail to prepare for what comes next. On the page, these processes may seem clinical, slow, and simple. In the real world, something di erent is needed. We have learned to steer when moving slowly. Now we must learn to race. Laying a solid foundation is only the rst step toward our real destination: acceleration.

Part Three

Part Three ACCELERATE

Start Your Engines Most of the decisions startups face are not clear-cut. How often should you release a product? Is there a reason to release weekly rather than daily or quarterly or annually? Product releases incur overhead, and so from an e ciency point of view, releasing often leaves less time to devote to building the product. However, waiting too long to release can lead to the ultimate waste: making something that nobody wants. How much time and energy should companies invest in infrastructure and planning early on in anticipation of success? Spend too much and you waste precious time that could have been spent learning. Spend too little and you may fail to take advantage of early success and cede market leadership to a fast follower. What should employees spend their days doing? How do we hold people accountable for learning at an organizational level? Traditional departments create incentive structures that keep people focused on excellence in their specialties: marketing, sales, product development. But what if the company’s best interests are served by cross-functional collaboration? Startups need organizational structures that combat the extreme uncertainty that is a startup’s chief enemy. The lean manufacturing movement faced similar questions on the factory oor. Their answers are relevant for startups as well, with some modifications. The critical rst question for any lean transformation is: which

activities create value and which are a form of waste? Once you understand this distinction, you can begin using lean techniques to drive out waste and increase the e ciency of the value-creating activities. For these techniques to be used in a startup, they must be adapted to the unique circumstances of entrepreneurship. Recall from Chapter 3 that value in a startup is not the creation of stu , but rather validated learning about how to build a sustainable business. What products do customers really want? How will our business grow? Who is our customer? Which customers should we listen to and which should we ignore? These are the questions that need answering as quickly as possible to maximize a startup’s chances of success. That is what creates value for a startup. I n Part Three, we will develop techniques that allow Lean Startups to grow without sacri cing the speed and agility that are the lifeblood of every startup. Contrary to common belief, lethargy and bureaucracy are not the inevitable fate of companies as they achieve maturity. I believe that with the proper foundation, Lean Startups can grow to become lean enterprises that maintain their agility, learning orientation, and culture of innovation even as they scale. In Chapter 9, we will see how Lean Startups take advantage of the counterintuitive power of small batches. Just as lean manufacturing has pursued a just-in-time approach to building products, reducing the need for in-process inventory, Lean Startups practice just-in-time scalability, conducting product experiments without making massive up-front investments in planning and design. Chapter 10 will explore the metrics startups should use to understand their growth as they add new customers and discover new markets. Sustainable growth follows one of three engines of growth: paid, viral, or sticky. By identifying which engine of growth a startup is using, it can then direct energy where it will be most e ective in growing the business. Each engine requires a focus on unique metrics to evaluate the success of new products and prioritize new experiments. When used with the innovation accounting method described in Part Two, these metrics allow

accounting method described in Part Two, these metrics allow startups to gure out when their growth is at risk of running out and pivot accordingly. Chapter 11 shows how to build an adaptive organization by investing in the right amount of process to keep teams nimble as they grow. We will see how techniques from the tool kit of lean manufacturing, such as the Five Whys, help startup teams grow without becoming bureaucratic or dysfunctional. We also will see how lean disciplines set the stage for a startup to transition into an established company driven by operational excellence. I n Chapter 12, we’ll come full circle. As startups grow into established companies, they face the same pressures that make it necessary for today’s enterprises to nd new ways to invest in disruptive innovation. In fact, we’ll see that an advantage of a successful startup’s rapid growth is that the company can keep its entrepreneurial DNA even as it matures. Today’s companies must learn to master a management portfolio of sustainable and disruptive innovation. It is an obsolete view that sees startups as going through discrete phases that leave earlier kinds of work— such as innovation—behind. Rather, modern companies must excel at doing multiple kinds of work in parallel. To do so, we’ll explore techniques for incubating innovation teams within the context of an established company. I have included an epilogue called “Waste Not” in which I consider some of the broader implications of the success of the Lean Startup movement, place it in historical context (including cautionary lessons from past movements), and make suggestions for its future direction.

9 BATCH the book Lean Thinking, James Womack and Daniel Jones Inrecount a story of stu ng newsletters into envelopes with the assistance of one of the author’s two young children. Every

envelope had to be addressed, stamped, lled with a letter, and sealed. The daughters, age six and nine, knew how they should go about completing the project: “Daddy, rst you should fold all of the newsletters. Then you should attach the seal. Then you should put on the stamps.” Their father wanted to do it the counterintuitive way: complete each envelope one at a time. They —like most of us—thought that was backward, explaining to him “that wouldn’t be efficient!” He and his daughters each took half the envelopes and competed to see who would finish first. The father won the race, and not just because he is an adult. It happened because the one envelope at a time approach is a faster way of getting the job done even though it seems ine cient. This has been con rmed in many studies, including one that was recorded on video.1 The one envelope at a time approach is called “single-piece ow” in lean manufacturing. It works because of the surprising power of small batches. When we do work that proceeds in stages, the “batch size” refers to how much work moves from one stage to the next at a time. For example, if we were stu ng one hundred envelopes, the intuitive way to do it—folding one hundred letters at a time—would have a batch size of one hundred. Single-piece ow is so named because it has a batch size of one.

is so named because it has a batch size of one. Why does stu ng one envelope at a time get the job done faster even though it seems like it would be slower? Because our intuition doesn’t take into account the extra time required to sort, stack, and move around the large piles of half-complete envelopes when it’s done the other way.2 It seems more efficient to repeat the same task over and over, in part because we expect that we will get better at this simple task the more we do it. Unfortunately, in processoriented work like this, individual performance is not nearly as important as the overall performance of the system. Even if the amount of time that each process took was exactly the same, the small batch production approach still would be superior, and for even more counterintuitive reasons. For example, imagine that the letters didn’t t in the envelopes. With the large-batch approach, we wouldn’t nd that out until nearly the end. With small batches, we’d know almost immediately. What if the envelopes are defective and won’t seal? In the large-batch approach, we’d have to unstu all the envelopes, get new ones, and restu them. In the small-batch approach, we’d nd this out immediately and have no rework required. All these issues are visible in a process as simple as stu ng envelopes, but they are of real and much greater consequence in the work of every company, large or small. The small-batch approach produces a nished product every few seconds, whereas the largebatch approach must deliver all the products at once, at the end. Imagine what this might look like if the time horizon was hours, days, or weeks. What if it turns out that the customers have decided they don’t want the product? Which process would allow a company to find this out sooner? Lean manufacturers discovered the bene ts of small batches decades ago. In the post–World War II economy, Japanese carmakers such as Toyota could not compete with huge American factories that used the latest mass production techniques. Following the intuitively e cient way of building, mass production factories built cars by using ever-larger batch sizes. They would spend huge amounts of money buying machines that could produce car parts by

amounts of money buying machines that could produce car parts by the tens, hundreds, or thousands. By keeping those machines running at peak speed, they could drive down the unit cost of each part and produce cars that were incredibly inexpensive so long as they were completely uniform. The Japanese car market was far too small for companies such as Toyota to employ those economies of scale; thus, Japanese companies faced intense pressure from mass production. Also, in the war-ravaged Japanese economy, capital was not available for massive investments in large machines. It was against this backdrop that innovators such as Taiichi Ohno, Shigeo Shingo, and others found a way to succeed by using small batches. Instead of buying large specialized machines that could produce thousands of parts at a time, Toyota used smaller generalpurpose machines that could produce a wide variety of parts in small batches. This required guring out ways to recon gure each machine rapidly to make the right part at the right time. By focusing on this “changeover time,” Toyota was able to produce entire automobiles by using small batches throughout the process. This rapid changing of machines was no easy feat. As in any lean transformation, existing systems and tools often need to be reinvented to support working in smaller batches. Shigeo Shingo created the concept of SMED (Single-Minute Exchange of Die) in order to enable a smaller batch size of work in early Toyota factories. He was so relentless in rethinking the way machines were operated that he was able to reduce changeover times that previously took hours to less than ten minutes. He did this, not by asking workers to work faster, but by reimagining and restructuring the work that needed to be done. Every investment in better tools and process had a corresponding bene t in terms of shrinking the batch size of work. Because of its smaller batch size, Toyota was able to produce a much greater diversity of products. It was no longer necessary that each product be exactly the same to gain the economies of scale that powered mass production. Thus, Toyota could serve its smaller, more fragmented markets and still compete with the mass producers. Over time, that capability allowed Toyota to move

producers. Over time, that capability allowed Toyota to move successfully into larger and larger markets until it became the world’s largest automaker in 2008. The biggest advantage of working in small batches is that quality problems can be identi ed much sooner. This is the origin of Toyota’s famous andon cord, which allows any worker to ask for help as soon as they notice any problem, such as a defect in a physical part, stopping the entire production line if it cannot be corrected immediately. This is another very counterintuitive practice. An assembly line works best when it is functioning smoothly, rolling car after car o the end of the line. The andon cord can interrupt this careful ow as the line is halted repeatedly. However, the bene ts of nding and xing problems faster outweigh this cost. This process of continuously driving out defects has been a win-win for Toyota and its customers. It is the root cause of Toyota’s historic high quality ratings and low costs. SMALL BATCHES IN ENTREPRENEURSHIP When I teach entrepreneurs this method, I often begin with stories about manufacturing. Before long, I can see the questioning looks: what does this have to do with my startup? The theory that is the foundation of Toyota’s success can be used to dramatically improve the speed at which startups find validated learning. Toyota discovered that small batches made their factories more e cient. In contrast, in the Lean Startup the goal is not to produce more stu e ciently. It is to—as quickly as possible—learn how to build a sustainable business. Think back to the example of envelope stu ng. What if it turns out that the customer doesn’t want the product we’re building? Although this is never good news for an entrepreneur, nding out sooner is much better than nding out later. Working in small batches ensures that a startup can minimize the expenditure of time, money, and e ort that ultimately turns out to have been wasted.

Small Batches at IMVU At IMVU, we applied these lessons from manufacturing to the way we work. Normally, new versions of products like ours are released to customers on a monthly, quarterly, or yearly cycle. Take a look at your cell phone. Odds are, it is not the very rst version of its kind. Even innovative companies such as Apple produce a new version of their agship phones about once a year. Bundled up in that product release are dozens of new features (at the release of iPhone 4, Apple boasted more than 1,500 changes). Ironically, many high-tech products are manufactured in advanced facilities that follow the latest in lean thinking, including small batches and single-piece ow. However, the process that is used to design the product is stuck in the era of mass production. Think of all the changes that are made to a product such as the iPhone; all 1,500 of them are released to customers in one giant batch. Behind the scenes, in the development and design of the product itself, large batches are still the rule. The work that goes into the development of a new product proceeds on a virtual assembly line. Product managers gure out what features are likely to please customers; product designers then gure out how those features should look and feel. These designs are passed to engineering, which builds something new or modi es an existing product and, once this is done, hands it o to somebody responsible for verifying that the new product works the way the product managers and designers intended. For a product such as the iPhone, these internal handoffs may happen on a monthly or quarterly basis. Think back one more time to the envelope-stu ng exercise. What is the most efficient way to do this work? At IMVU, we attempted to design, develop, and ship our new features one at a time, taking advantage of the power of small batches. Here’s what it looked like. Instead of working in separate departments, engineers and designers would work together side by side on one feature at a time. Whenever that feature was ready to be tested with customers,

they immediately would release a new version of the product, which would go live on our website for a relatively small number of people. The team would be able immediately to assess the impact of their work, evaluate its e ect on customers, and decide what to do next. For tiny changes, the whole process might be repeated several times per day. In fact, in the aggregate, IMVU makes about fty changes to its product (on average) every single day. Just as with the Toyota Production System, the key to being able to operate this quickly is to check for defects immediately, thus preventing bigger problems later. For example, we had an extensive set of automated tests that assured that after every change our product still worked as designed. Let’s say an engineer accidentally removed an important feature, such as the checkout button on one of our e-commerce pages. Without this button, customers no longer could buy anything from IMVU. It’s as if our business instantly became a hobby. Analogously to the Toyota andon cord, IMVU used an elaborate set of defense mechanisms that prevented engineers from accidentally breaking something important. We called this our product’s immune system because those automatic protections went beyond checking that the product behaved as expected. We also continuously monitored the health of our business itself so that mistakes were found and removed automatically. Going back to our business-to-hobby example of the missing checkout button, let’s make the problem a little more interesting. Imagine that instead of removing the button altogether, an engineer makes a mistake and changes the button’s color so that it is now white on a white background. From the point of view of automated functional tests, the button is still there and everything is working normally; from the customer’s point of view, the button is gone, and so nobody can buy anything. This class of problems is hard to detect solely with automation but is still catastrophic from a business point of view. At IMVU, our immune system is programmed to detect these business consequences and

programmed to detect these business consequences and automatically invoke our equivalent of the andon cord. When our immune system detects a problem, a number of things happen immediately: 1. The defective change is removed immediately and automatically. 2. Everyone on the relevant team is notified of the problem. 3. The team is blocked from introducing any further changes, preventing the problem from being compounded by future mistakes … 4. … until the root cause of the problem is found and xed. (This root cause analysis is discussed in greater detail in Chapter 11.) At IMVU, we called this continuous deployment, and even in the fast-moving world of software development it is still considered controversial.3 As the Lean Startup movement has gained traction, it has come to be embraced by more and more startups, even those that operate mission-critical applications. Among the most cutting edge examples is Wealthfront, whose pivot was described in Chapter 8. The company practices true continuous deployment— including more than a dozen releases to customers every day—in an SEC-regulated environment.4 Continuous Deployment Beyond Software When I tell this story to people who work in a slower-moving industry, they think I am describing something futuristic. But increasingly, more and more industries are seeing their design process accelerated by the same underlying forces that make this kind of rapid iteration possible in the software industry. There are three ways in which this is happening: 1. Hardware becoming software. Think about what has happened in consumer electronics. The latest phones and tablet computers are

in consumer electronics. The latest phones and tablet computers are little more than a screen connected to the Internet. Almost all of their value is determined by their software. Even old-school products such as automobiles are seeing ever-larger parts of their value being generated by the software they carry inside, which controls everything from the entertainment system to tuning the engine to controlling the brakes. What can be built out of software can be modi ed much faster than a physical or mechanical device can. 2. Fast production changes. Because of the success of the lean manufacturing movement, many assembly lines are set up to allow each new product that comes o the line to be customized completely without sacri cing quality or cost-e ectiveness. Historically, this has been used to o er the customer many choices of product, but in the future, this capability will allow the designers of products to get much faster feedback about new versions. When the design changes, there is no excess inventory of the old version to slow things down. Since machines are designed for rapid changeovers, as soon as the new design is ready, new versions can be produced quickly. 3. 3D printing and rapid prototyping tools. As just one example, most products and parts that are made out of plastic today are mass produced using a technique called injection molding. This process is extremely expensive and time-consuming to set up, but once it is up and running, it can reproduce hundreds of thousands of identical individual items at an extremely low cost. It is a classic large-batch production process. This has put entrepreneurs who want to develop a new physical product at a disadvantage, since in general only large companies can a ord these large production runs for a new product. However, new technologies are allowing entrepreneurs to build small batches of products that are of the same quality as products made with injection molding, but at much lower cost and much, much faster.

The essential lesson is not that everyone should be shipping fty times per day but that by reducing batch size, we can get through the Build-Measure-Learn feedback loop more quickly than our competitors can. The ability to learn faster from customers is the essential competitive advantage that startups must possess. SMALL BATCHES IN ACTION To see this process in action, let me introduce you to a company in Boise, Idaho, called SGW Designworks. SGW’s specialty is rapid production techniques for physical products. Many of its clients are startups. SGW Designworks was engaged by a client who had been asked by a military customer to build a complex eld x-ray system to detect explosives and other destructive devices at border crossings and in war zones. Conceptually, the system consisted of an advanced head unit that read x-ray lm, multiple x-ray lm panels, and the framework to hold the panels while the lm was being exposed. The client already had the technology for the x-ray panels and the head unit, but to make the product work in rugged military settings, SGW needed to design and deliver the supporting structure that would make the technology usable in the eld. The framework had to be stable to ensure a quality x-ray image, durable enough for use in a war zone, easy to deploy with minimal training, and small enough to collapse into a backpack. This is precisely the kind of product we are accustomed to thinking takes months or years to develop, yet new techniques are shrinking that time line. SGW immediately began to generate the visual prototypes by using 3D computer-aided design (CAD) software. The 3D models served as a rapid communication tool between the client and the SGW team to make early design decisions. The team and client settled on a design that used an advanced

locking hinge to provide the collapsibility required without compromising stability. The design also integrated a suction cup/pump mechanism to allow for fast, repeatable attachment to the x-ray panels. Sounds complicated, right? Three days later, the SGW team delivered the rst physical prototypes to the client. The prototypes were machined out of aluminum directly from the 3D model, using a technique called computer numerical control (CNC) and were hand assembled by the SGW team. The client immediately took the prototypes to its military contact for review. The general concept was accepted with a number of minor design modi cations. In the next ve days, another full cycle of design iteration, prototyping, and design review was completed by the client and SGW. The rst production run of forty completed units was ready for delivery three and a half weeks after the initiation of the development project. SGW realized that this was a winning model because feedback on design decisions was nearly instantaneous. The team used the same process to design and deliver eight products, serving a wide range of functions, in a twelve-month period. Half of those products are generating revenue today, and the rest are awaiting initial orders, all thanks to the power of working in small batches. THE PROJECT TIME LINE Design and engineering of the initial virtual prototype 1 day Production and assembly of initial hard prototypes 3 days Design iteration: two additional cycles 5 days Initial production run and assembly of initial forty units 15 days Small Batches in Education Not every type of product—as it exists today—allows for design

change in small batches. But that is no excuse for sticking to outdated methods. A signi cant amount of work may be needed to enable innovators to experiment in small batches. As was pointed out in Chapter 2, for established companies looking to accelerate their innovation teams, building this platform for experimentation is the responsibility of senior management. Imagine that you are a schoolteacher in charge of teaching math to middle school students. Although you may teach concepts in small batches, one day at a time, your overall curriculum cannot change very often. Because you must set up the curriculum in advance and teach the same concepts in the same order to every student in the classroom, you can try a new curriculum at most only once a year. How could a math teacher experiment with small batches? Under the current large-batch system for educating students, it would be quite di cult; our current educational system was designed in the era of mass production and uses large batches extensively. A new breed of startups is working hard to change all that. In School of One, students have daily “playlists” of their learning tasks that are attuned to each student’s learning needs, based on that student’s readiness and learning style. For example, Julia is way ahead of grade level in math and learns best in small groups, so her playlist might include three or four videos matched to her aptitude level, a thirty-minute one-on-one tutoring session with her teacher, and a small group activity in which she works on a math puzzle with three peers at similar aptitude levels. There are assessments built into each activity so that data can be fed back to the teacher to choose appropriate tasks for the next playlist. This data can be aggregated across classes, schools, or even whole districts. Now imagine trying to experiment with a curriculum by using a tool such as School of One. Each student is working at his or her own pace. Let’s say you are a teacher who has a new sequence in mind for how math concepts should be taught. You can see immediately the impact of the change on those of your students who are at that point in the curriculum. If you judge it to be a good change, you could roll it out immediately for every single student;

change, you could roll it out immediately for every single student; when they get to that part of the curriculum, they will get the new sequence automatically. In other words, tools like School of One enable teachers to work in much smaller batches, to the bene t of their students. (And, as tools reach wide-scale adoption, successful experiments by individual teachers can be rolled out district-, city-, or even nationwide.) This approach is having an impact and earning accolades. Time magazine recently included School of One in its “most innovative ideas” list; it was the only educational organization to make the list.5 THE LARGE-BATCH DEATH SPIRAL Small batches pose a challenge to managers steeped in traditional notions of productivity and progress, because they believe that functional specialization is more efficient for expert workers. Imagine you’re a product designer overseeing a new product and you need to produce thirty individual design drawings. It probably seems that the most e cient way to work is in seclusion, by yourself, producing the designs one by one. Then, when you’re done with all of them, you pass the drawings on to the engineering team and let them work. In other words, you work in large batches. From the point of view of individual e ciency, working in large batches makes sense. It also has other bene ts: it promotes skill building, makes it easier to hold individual contributors accountable, and, most important, allows experts to work without interruption. At least that’s the theory. Unfortunately, reality seldom works out that way. Consider our hypothetical example. After passing thirty design drawings to engineering, the designer is free to turn his or her attention to the next project. But remember the problems that came up during the envelope-stu ng exercise. What happens when engineering has questions about how the drawings are supposed to work? What if some of the drawings are unclear? What if something goes wrong when engineering attempts to use the drawings?

These problems inevitably turn into interruptions for the designer, and now those interruptions are interfering with the next large batch the designer is supposed to be working on. If the drawings need to be redone, the engineers may become idle while they wait for the rework to be completed. If the designer is not available, the engineers may have to redo the designs themselves. This is why so few products are actually built the way they are designed. When I work with product managers and designers in companies that use large batches, I often discover that they have to redo their work ve or six times for every release. One product manager I worked with was so inundated with interruptions that he took to coming into the o ce in the middle of the night so that he could work uninterrupted. When I suggested that he try switching the work process from large-batch to single-piece ow, he refused— because that would be ine cient! So strong is the instinct to work in large batches, that even when a large-batch system is malfunctioning, we have a tendency to blame ourselves. Large batches tend to grow over time. Because moving the batch forward often results in additional work, rework, delays, and interruptions, everyone has an incentive to do work in ever-larger batches, trying to minimize this overhead. This is called the largebatch death spiral because, unlike in manufacturing, there are no physical limits on the maximum size of a batch.6 It is possible for batch size to keep growing and growing. Eventually, one batch will become the highest-priority project, a “bet the company” new version of the product, because the company has taken such a long time since the last release. But now the managers are incentivized to increase batch size rather than ship the product. In light of how long the product has been in development, why not x one more bug or add one more feature? Who really wants to be the manager who risked the success of this huge release by failing to address a potentially critical flaw? I worked at a company that entered this death spiral. We had been working for months on a new version of a really cool product.

The original version had been years in the making, and expectations for the next release were incredibly high. But the longer we worked, the more afraid we became of how customers would react when they nally saw the new version. As our plans became more ambitious, so too did the number of bugs, con icts, and problems we had to deal with. Pretty soon we got into a situation in which we could not ship anything. Our launch date seemed to recede into the distance. The more work we got done, the more work we had to do. The lack of ability to ship eventually precipitated a crisis and a change of management, all because of the trap of large batches. These misconceptions about batch size are incredibly common. Hospital pharmacies often deliver big batches of medications to patient oors once a day because it’s e cient (a single trip, right?). But many of those meds get sent back to the pharmacy when a patient’s orders have changed or the patient is moved or discharged, causing the pharmacy staff to do lots of rework and reprocessing (or trashing) of meds. Delivering smaller batches every four hours reduces the total workload for the pharmacy and ensures that the right meds are at the right place when needed. Hospital lab blood collections often are done in hourly batches; phlebotomists collect blood for an hour from multiple patients and then send or take all the samples to the lab. This adds to turnaround time for test results and can harm test quality. It has become common for hospitals to bring small batches (two patients) or a single-patient ow of specimens to the lab even if they have to hire an extra phlebotomist or two to do so, because the total system cost is lower.7 PULL, DON’T PUSH Let’s say you are out for a drive, pondering the merits of small batches, and nd yourself accidentally putting a dent in your new 2011 blue Toyota Camry. You take it into the dealership for repair and wait to hear the bad news. The repair technician tells you that

you need to have the bumper replaced. He goes to check their inventory levels and tells you he has a new bumper in stock and they can complete your repair immediately. This is good news for everyone—you because you get your car back sooner and the dealership because they have a happy customer and don’t run the risk of your taking the car somewhere else for repair. Also, they don’t have to store your car or give you a loaner while they wait for the part to come in. In traditional mass production, the way to avoid stockouts—not having the product the customer wants—is to keep a large inventory of spares just in case. It may be that the blue 2011 Camry bumper is quite popular, but what about last year’s model or the model from ve years ago? The more inventory you keep, the greater the likelihood you will have the right product in stock for every customer. But large inventories are expensive because they have to be transported, stored, and tracked. What if the 2011 bumper turns out to have a defect? All the spares in all the warehouses instantly become waste. Lean production solves the problem of stockouts with a technique called pull. When you bring a car into the dealership for repair, one blue 2011 Camry bumper gets used. This creates a “hole” in the dealer’s inventory, which automatically causes a signal to be sent to a local restocking facility called the Toyota Parts Distribution Center (PDC). The PDC sends the dealer a new bumper, which creates another hole in inventory. This sends a similar signal to a regional warehouse called the Toyota Parts Redistribution Center (PRC), where all parts suppliers ship their products. That warehouse signals the factory where the bumpers are made to produce one more bumper, which is manufactured and shipped to the PRC. The ideal goal is to achieve small batches all the way down to single-piece ow along the entire supply chain. Each step in the line pulls the parts it needs from the previous step. This is the famous Toyota just-in-time production method.8 When companies switch to this kind of production, their

warehouses immediately shrink, as the amount of just-in-case inventory [called work-in-progress (WIP) inventory] is reduced dramatically. This almost magical shrinkage of WIP is where lean manufacturing gets its name. It’s as if the whole supply chain suddenly went on a diet. Startups struggle to see their work-in-progress inventory. When factories have excess WIP, it literally piles up on the factory oor. Because most startup work is intangible, it’s not nearly as visible. For example, all the work that goes into designing the minimum viable product is—until the moment that product is shipped—just WIP inventory. Incomplete designs, not-yet-validated assumptions, and most business plans are WIP. Almost every Lean Startup technique we’ve discussed so far works its magic in two ways: by converting push methods to pull and reducing batch size. Both have the net effect of reducing WIP. In manufacturing, pull is used primarily to make sure production processes are tuned to levels of customer demand. Without this, factories can wind up making much more—or much less—of a product than customers really want. However, applying this approach to developing new products is not straightforward. Some people misunderstand the Lean Startup model as simply applying pull to customer wants. This assumes that customers could tell us what products to build and that this would act as the pull signal to product development to make them.9 As was mentioned earlier, this is not the way the Lean Startup model works, because customers often don’t know what they want. Our goal in building products is to be able to run experiments that will help us learn how to build a sustainable business. Thus, the right way to think about the product development process in a Lean Startup is that it is responding to pull requests in the form of experiments that need to be run. As soon as we formulate a hypothesis that we want to test, the product development team should be engineered to design and run this experiment as quickly as possible, using the smallest batch size that will get the job done. Remember that although we write the

feedback loop as Build-Measure-Learn because the activities happen in that order, our planning really works in the reverse order: we gure out what we need to learn and then work backwards to see what product will work as an experiment to get that learning. Thus, it is not the customer, but rather our hypothesis about the customer, that pulls work from product development and other functions. Any other work is waste. Hypothesis Pull in Clean Tech To see this in action, let’s take a look at Berkeley-based startup Alphabet Energy. Any machine or process that generates power, whether it is a motor in a factory or a coal-burning power plant, generates heat as a by-product. Alphabet Energy has developed a product that can generate electricity from this waste heat, using a new kind of material called a thermoelectric. Alphabet Energy’s thermoelectric material was developed over ten years by scientists at the Lawrence Berkeley National Laboratories. As with many clean technology products, there are huge challenges in bringing a product like this to market. While working through its leap-of-faith assumptions, Alphabet gured out early that developing a solution for waste thermoelectricity required building a heat exchanger and a generic device to transfer heat from one medium to another as well as doing project-speci c engineering. For instance, if Alphabet wanted to build a solution for a utility such as Paci c Gas and Electric, the heat exchanger would have to be con gured, shaped, and installed to capture the heat from a power plant’s exhaust system. What makes Alphabet Energy unique is that the company made a savvy decision early on in the research process. Instead of using relatively rare elements as materials, they decided to base their research on silicon wafers, the same physical substance that computer central processing units (CPUs) are made from. As CEO Matthew Scullin explains, “Our thermoelectric is the only one that can use low-cost semiconductor infrastructure for manufacturing.”

This has enabled Alphabet Energy to design and build its products in small batches. By contrast, most successful clean technology startups have had to make substantial early investments. The solar panel provider SunPower had to build in factories to manufacture its panels and partner with installers before becoming fully operational. Similarly, BrightSource raised $291 million to build and operate large-scale solar plants without delivering a watt to a single customer. Instead of having to invest time and money in expensive fabrication facilities, Alphabet is able to take advantage of the massive existing infrastructure that produces silicon wafers for computer electronics. As a result, Alphabet can go from a product concept to holding a physical version in its hand in just six weeks from end to end. Alphabet’s challenge has been to nd the combination of performance, price, and physical shape that is a match for early customers. Although its technology has revolutionary potential, early adopters will deploy it only if they can see a clear return on investment. It might seem that the most obvious market for Alphabet’s technology would be power plants, and indeed, that was the team’s initial hypothesis. Alphabet hypothesized that simple cycle gas turbines would be an ideal application; these turbines, which are similar to jet engines strapped to the ground, are used by power generators to provide energy for peak demand. Alphabet believed that attaching its semiconductors to those turbines would be simple and cheap. The company went about testing this hypothesis in small batches by building small-scale solutions for its customers as a way of learning. As with many initial ideas, their hypothesis was disproved quickly. Power companies have a low tolerance for risk, making them unlikely to become early adopters. Because it wasn’t weighed down by a large-batch approach, Alphabet was ready to pivot after just three months of investigation. Alphabet has eliminated many other potential markets as well, leading to a series of customer segment pivots. The company’s current e orts are focused on manufacturing rms, which have the

ability to experiment with new technologies in separate parts of their factory; this allows early adopters to evaluate the real-world bene ts before committing to a larger deployment. These early deployments are putting more of Alphabet’s assumptions to the test. Unlike in the computer hardware business, customers are not willing to pay top dollar for maximum performance. This has required signi cant changes in Alphabet’s product, con guring it to achieve the lowest cost per watt possible. All this experimentation has cost the company a tiny fraction of what other energy startups have consumed. To date, Alphabet has raised approximately $1 million. Only time will tell if they will prevail, but thanks to the power of small batches, they will be able to discover the truth much faster.10 The Toyota Production System is probably the most advanced system of management in the world, but even more impressive is the fact that Toyota has built the most advanced learning organization in history. It has demonstrated an ability to unleash the creativity of its employees, achieve consistent growth, and produce innovative new products relentlessly over the course of nearly a century.11 This is the kind of long-term success to which entrepreneurs should aspire. Although lean production techniques are powerful, they are only a manifestation of a high-functioning organization that is committed to achieving maximum performance by employing the right measures of progress over the long term. Process is only the foundation upon which a great company culture can develop. But without this foundation, e orts to encourage learning, creativity, and innovation will fall at—as many disillusioned directors of HR can attest. The Lean Startup works only if we are able to build an organization as adaptable and fast as the challenges it faces. This requires tackling the human challenges inherent in this new way of working; that is the subject of the remainder of Part Three.

THE STARTUP WAY

10 GROW had two startups seek my advice on the same day. As Irecently types of businesses, they could not have been more di erent. The rst is developing a marketplace to help traders of collectibles

connect with one another. These people are hard-core fans of movies, anime, or comics who strive to put together complete collections of toys and other promotional merchandise related to the characters they love. The startup aspires to compete with online marketplaces such as eBay as well as physical marketplaces attached to conventions and other gatherings of fans. The second startup sells database software to enterprise customers. They have a next-generation database technology that can supplement or replace o erings from large companies such as Oracle, IBM, and SAP. Their customers are chief information officers (CIOs), IT managers, and engineers in some of the world’s largest organizations. These are long-lead-time sales that require salespeople, sales engineering, installation support, and maintenance contracts. You could be forgiven for thinking these two companies have absolutely nothing in common, yet both came to me with the exact same problem. Each one had early customers and promising early revenue. They had validated and invalidated many hypotheses in their business models and were executing against their product road maps successfully. Their customers had provided a healthy mix of positive feedback and suggestions for improvements. Both companies had used their early success to raise money from outside

companies had used their early success to raise money from outside investors. The problem was that neither company was growing. Both CEOs brought me identical-looking graphs showing that their early growth had atlined. They could not understand why. They were acutely aware of the need to show progress to their employees and investors and came to me because they wanted advice on how to jump-start their growth. Should they invest in more advertising or marketing programs? Should they focus on product quality or new features? Should they try to improve conversion rates or pricing? As it turns out, both companies share a deep similarity in the way their businesses grow—and therefore a similar confusion about what to do. Both are using the same engine of growth, the topic of this chapter. WHERE DOES GROWTH COME FROM? The engine of growth is the mechanism that startups use to achieve sustainable growth. I use the word sustainable to exclude all onetime activities that generate a surge of customers but have no longterm impact, such as a single advertisement or a publicity stunt that might be used to jump-start growth but could not sustain that growth for the long term. Sustainable growth is characterized by one simple rule: New customers come from the actions of past customers. There are four primary ways past customers drive sustainable growth: 1. Word of mouth. Embedded in most products is a natural level of growth that is caused by satis ed customers’ enthusiasm for the product. For example, when I bought my rst TiVo DVR, I couldn’t stop telling my friends and family about it. Pretty soon, my entire family was using one.

2. As a side e ect of product usage. Fashion or status, such as luxury goods products, drive awareness of themselves whenever they are used. When you see someone dressed in the latest clothes or driving a certain car, you may be in uenced to buy that product. This is also true of so-called viral products such as Facebook and PayPal. When a customer sends money to a friend using PayPal, the friend is exposed automatically to the PayPal product. 3. Through funded advertising. Most businesses employ advertising to entice new customers to use their products. For this to be a source of sustainable growth, the advertising must be paid for out of revenue, not one-time sources such as investment capital. As long as the cost of acquiring a new customer (the so-called marginal cost) is less than the revenue that customer generates (the marginal revenue), the excess (the marginal pro t) can be used to acquire more customers. The more marginal pro t, the faster the growth. 4. Through repeat purchase or use. Some products are designed to be purchased repeatedly either through a subscription plan (a cable company) or through voluntary repurchases (groceries or lightbulbs). By contrast, many products and services are intentionally designed as one-time events, such as wedding planning. These sources of sustainable growth power feedback loops that I have termed engines of growth. Each is like a combustion engine, turning over and over. The faster the loop turns, the faster the company will grow. Each engine has an intrinsic set of metrics that determine how fast a company can grow when using it. THE THREE ENGINES OF GROWTH

We saw in Part Two how important it is for startups to use the right kind of metrics—actionable metrics—to evaluate their progress. However, this leaves a large amount of variety in terms of which numbers one should measure. In fact, one of the most expensive forms of potential waste for a startup is spending time arguing about how to prioritize new development once it has a product on the market. At any time, the company could invest its energy in nding new customers, servicing existing customers better, improving overall quality, or driving down costs. In my experience, the discussions about these kinds of priority decisions can consume a substantial fraction of the company’s time. Engines of growth are designed to give startups a relatively small set of metrics on which to focus their energies. As one of my mentors, the venture capital investor Shawn Carolan, put it, “Startups don’t starve; they drown.” There are always a zillion new ideas about how to make the product better oating around, but the hard truth is that most of those ideas make a di erence only at the margins. They are mere optimizations. Startups have to focus on the big experiments that lead to validated learning. The engines of growth framework helps them stay focused on the metrics that matter. The Sticky Engine of Growth This brings us back to the two startups that kicked o this chapter. Both are using the exact same engine of growth despite being in very di erent industries. Both products are designed to attract and retain customers for the long term. The underlying mechanism of that retention is di erent in the two cases. For the collectible company, the idea is to become the number one shopping destination for fanatical collectors. These are people who are constantly hunting for the latest items and the best deals. If the company’s product works as designed, collectors who start using it will check constantly and repeatedly to see if new items are for sale as well as listing their own items for sale or trade.

The startup database vendor relies on repeat usage for a very di erent reason. Database technology is used only as the foundation for a customer’s own products, such as a website or a point of sale system. Once you build a product on top of a particular database technology, it is extremely di cult to switch. In the IT industry, such customers are said to be locked in to the vendor they choose. For such a product to grow, it has to o er such a compelling new capability that customers are willing to risk being tied to a proprietary vendor for a potentially long time. Thus, both businesses rely on having a high customer retention rate. They have an expectation that once you start using their product, you will continue to do so. This is the same dynamic as a mobile telephone service provider: when a customer cancels his or her service, it generally means that he or she is extremely dissatis ed or is switching to a competitor’s product. This is in contrast to, say, groceries on a store aisle. In the grocery retail business, customer tastes uctuate, and if a customer buys a Pepsi this week instead of Coke, it’s not necessarily a big deal. Therefore, companies using the sticky engine of growth track their attrition rate or churn rate very carefully. The churn rate is de ned as the fraction of customers in any period who fail to remain engaged with the company’s product. The rules that govern the sticky engine of growth are pretty simple: if the rate of new customer acquisition exceeds the churn rate, the product will grow. The speed of growth is determined by what I call the rate of compounding, which is simply the natural growth rate minus the churn rate. Like a bank account that earns compounding interest, having a high rate of compounding will lead to extremely rapid growth—without advertising, viral growth, or publicity stunts. Unfortunately, both of these sticky startups were tracking their progress using generic indicators such as the total number of customers. Even the actionable metrics they were using, such as the activation rate and revenue per customer, weren’t very helpful because in the sticky engine of growth, these variables have little impact on growth. (In the sticky engine of growth, they are better

suited to testing the value hypothesis that was discussed in Chapter 5.) After our meeting, one of the two startups took me up on my advice to model its customer behavior by using the sticky engine of growth as a template. The results were striking: a 61 percent retention rate and a 39 percent growth rate of new customers. In other words, its churn rate and new customer acquisition balanced each other almost perfectly, leading to a compounding growth rate of just 0.02 percent—almost zero. This is typical for companies in an engagement business that are struggling to nd growth. An insider who worked at the dot-comera company PointCast once showed me how that company su ered a similar dysfunction. When PointCast was struggling to grow, it was nonetheless incredibly successful in new customer acquisition—just like this sticky startup (39 percent every period). Unfortunately, this growth is being o set by an equivalent amount of churn. Once it is modeled this way, the good news should be apparent: there are plenty of new customers coming in the door. The way to nd growth is to focus on existing customers for the product even more engaging to them. For example, the company could focus on getting more and better listings. This would create an incentive for customers to check back often. Alternatively, the company could do something more direct such as messaging them about limited-time sales or special offers. Either way, its focus needs to be on improving customer retention. This goes against the standard intuition in that if a company lacks growth, it should invest more in sales and marketing. This counterintuitive result is hard to infer from standard vanity metrics. The Viral Engine of Growth Online social networks and Tupperware are examples of products for which customers do the lion’s share of the marketing. Awareness of the product spreads rapidly from person to person similarly to the way a virus becomes an epidemic. This is distinct from the

the way a virus becomes an epidemic. This is distinct from the simple word-of-mouth growth discussed above. Instead, products that exhibit viral growth depend on person-to-person transmission as a necessary consequence of normal product use. Customers are not intentionally acting as evangelists; they are not necessarily trying to spread the word about the product. Growth happens automatically as a side e ect of customers using the product. Viruses are not optional. For example, one of the most famous viral success stories is a company called Hotmail. In 1996, Sabeer Bhatia and Jack Smith launched a new web-based e-mail service that o ered customers free accounts. At rst, growth was sluggish; with only a small seed investment from the venture capital rm Draper Fisher Jurvetson, the Hotmail team could not a ord an extensive marketing campaign. But everything changed when they made one small tweak to the product. They added to the bottom of every single email the message “P.S. Get your free e-mail at Hotmail” along with a clickable link. Within weeks, that small product change produced massive results. Within six months, Bhatia and Smith had signed up more than 1 million new customers. Five weeks later, they hit the 2 million mark. Eighteen months after launching the service, with 12 million subscribers, they sold the company to Microsoft for $400 million.1 The same phenomenon is at work in Tupperware’s famous “house parties,” in which customers earn commissions by selling the product to their friends and neighbors. Every sales pitch is an opportunity not only to sell Tupperware products but also to persuade other customers to become Tupperware representatives. Tupperware parties are still going strong decades after they started. Many other contemporary companies, such as Pampered Chef (owned by Warren Bu ett’s Berkshire Hathaway), Southern Living, and Tastefully Simple, have adopted a similar model successfully. Like the other engines of growth, the viral engine is powered by a feedback loop that can be quanti ed. It is called the viral loop, and its speed is determined by a single mathematical term called

and its speed is determined by a single mathematical term called the viral coe cient. The higher this coe cient is, the faster the product will spread. The viral coe cient measures how many new customers will use a product as a consequence of each new customer who signs up. Put another way, how many friends will each customer bring with him or her? Since each friend is also a new customer, he or she has an opportunity to recruit yet more friends. For a product with a viral coe cient of 0.1, one in every ten customers will recruit one of his or her friends. This is not a sustainable loop. Imagine that one hundred customers sign up. They will cause ten friends to sign up. Those ten friends will cause one additional person to sign up, but there the loop will fizzle out. By contrast, a viral loop with a coe cient that is greater than 1.0 will grow exponentially, because each person who signs up will bring, on average, more than one other person with him or her. To see these effects graphically, take a look at this chart:

Companies that rely on the viral engine of growth must focus on increasing the viral coe cient more than anything else, because

even tiny changes in this number will cause dramatic changes in their future prospects. A consequence of this is that many viral products do not charge customers directly but rely on indirect sources of revenue such as advertising. This is the case because viral products cannot a ord to have any friction impede the process of signing customers up and recruiting their friends. This can make testing the value hypothesis for viral products especially challenging. The true test of the value hypothesis is always a voluntary exchange of value between customers and the startup that serves them. A lot of confusion stems from the fact that this exchange can be monetary, as in the case of Tupperware, or nonmonetary, as in the case of Facebook. In the viral engine of growth, monetary exchange does not drive new growth; it is useful only as an indicator that customers value the product enough to pay for it. If Facebook or Hotmail had started charging customers in their early days, it would have been foolish, as it would have impeded their ability to grow. However, it is not true that customers do not give these companies something of value: by investing their time and attention in the product, they make the product valuable to advertisers. Companies that sell advertising actually serve two di erent groups of customers—consumers and advertisers—and exchange a different currency of value with each.2 This is markedly di erent from companies that actively use money to fuel their expansion, such as a retail chain that can grow as fast as it can fund the opening of new stores at suitable locations. These companies are using a different engine of growth altogether. The Paid Engine of Growth Imagine another pair of businesses. The rst makes $1 on each customer it signs up; the second makes $100,000 from each customer it signs up. To predict which company will grow faster, you need to know only one additional thing: how much it costs to sign up a new customer.

Imagine that the rst company uses Google AdWords to nd new customers online and pays an average of 80 cents each time a new customer joins. The second company sells heavy goods to large companies. Each sale requires a signi cant time investment from a salesperson and on-site sales engineering to help install the product; these hard costs total up to $80,000 per new customer. Both companies will grow at the exact same rate. Each has the same proportion of revenue (20 percent) available to reinvest in new customer acquisition. If either company wants to increase its rate of growth, it can do so in one of two ways: increase the revenue from each customer or drive down the cost of acquiring a new customer. That’s the paid engine of growth at work. In relating the IMVU story in Chapter 3, I talked about how we made a major early mistake in setting up the IMVU strategy. We ultimately wound up having to make an engine of growth pivot. We originally thought that our IM add-on strategy would allow the product to grow virally. Unfortunately, customers refused to go along with our brilliant strategy. Our basic misconception was a belief that customers would be willing to use IMVU as an add-on to existing instant messaging networks. We believed that the product would spread virally through those networks, passed from customer to customer. The problem with that theory is that some kinds of products are not compatible with viral growth. IMVU’s customers didn’t want to use the product with their existing friends. They wanted to use it to make new friends. Unfortunately, that meant they did not have a strong incentive to bring new customers to the product; they viewed that as our job. Fortunately, IMVU was able to grow by using paid advertising because our customers were willing to pay more for our product than it cost us to reach them via advertising. Like the other engines, the paid engine of growth is powered by a feedback loop. Each customer pays a certain amount of money for the product over his or her “lifetime” as a customer. Once variable costs are deducted, this usually is called the customer lifetime value (LTV). This revenue can be invested in growth by buying

advertising. Suppose an advertisement costs $100 and causes fty new customers to sign up for the service. This ad has a cost per acquisition (CPA) of $2.00. In this example, if the product has an LTV that is greater than $2, the product will grow. The margin between the LTV and the CPA determines how fast the paid engine of growth will turn (this is called the marginal pro t). Conversely, if the CPA remains at $2.00 but the LTV falls below $2.00, the company’s growth will slow. It may make up the di erence with one-time tactics such as using invested capital or publicity stunts, but those tactics are not sustainable. This was the fate of many failed companies, including notable dot-com ameouts that erroneously believed that they could lose money on each customer but, as the old joke goes, make it up in volume. Although I have explained the paid engine of growth in terms of advertising, it is far broader than that. Startups that employ an outbound sales force are also using this engine, as are retail companies that rely on foot tra c. All these costs should be factored into the cost per acquisition. For example, one startup I worked with built collaboration tools for teams and groups. It went through a radical pivot, switching from a tool that was used primarily by hobbyists and small clubs to one that was sold primarily to enterprises, nongovernmental organizations (NGOs), and other extremely large organizations. However, they made that customer segment pivot without changing their engine of growth. Previously, they had done customer acquisition online, using web-based direct marketing techniques. I remember one early situation in which the company elded a call from a major NGO that wanted to buy its product and roll it out across many divisions. The startup had an “unlimited” pricing plan, its most expensive, that cost only a few hundred dollars per month. The NGO literally could not make the purchase because it had no process in place for buying something so inexpensive. Additionally, the NGO needed substantial help in managing the rollout, educating its sta on the new tool, and tracking the impact of the change; those were all services the company was ill equipped to o er.

those were all services the company was ill equipped to o er. Changing customer segments required them to switch to hiring a sizable outbound sales sta that spent time attending conferences, educating executives, and authoring white papers. Those much higher costs came with a corresponding reward: the company switched from making only a few dollars per customer to making tens and then hundreds of thousands of dollars per much larger customer. Their new engine of growth led to sustained success. Most sources of customer acquisition are subject to competition. For example, prime retail storefronts have more foot tra c and are therefore more valuable. Similarly, advertising that is targeted to more a uent customers generally costs more than advertising that reaches the general public. What determines these prices is the average value earned in aggregate by the companies that are in competition for any given customer’s attention. Wealthy consumers cost more to reach because they tend to become more pro table customers. Over time, any source of customer acquisition will tend to have its CPA bid up by this competition. If everyone in an industry makes the same amount of money on each sale, they all will wind up paying most of their marginal pro t to the source of acquisition. Thus, the ability to grow in the long term by using the paid engine requires a di erentiated ability to monetize a certain set of customers. IMVU is a case in point. Our customers were not considered very lucrative by other online services: they included a lot of teenagers, low-income adults, and international customers. Other services tended to assume those people would not pay for anything online. At IMVU, we developed techniques for collecting online payments from customers who did not have a credit card, such as allowing them to bill to their mobile phones or send us cash in the mail. Therefore, we could a ord to pay more to acquire those customers than our competitors could. A Technical Caveat

Technically, more than one engine of growth can operate in a business at a time. For example, there are products that have extremely fast viral growth as well as extremely low customer churn rates. Also, there is no reason why a product cannot have both high margins and high retention. However, in my experience, successful startups usually focus on just one engine of growth, specializing in everything that is required to make it work. Companies that attempt to build a dashboard that includes all three engines tend to cause a lot of confusion because the operations expertise required to model all these e ects simultaneously is quite complicated. Therefore, I strongly recommend that startups focus on one engine at a time. Most entrepreneurs already have a strong leap-of-faith hypothesis about which engine is most likely to work. If they do not, time spent out of the building with customers will quickly suggest one that seems pro table. Only after pursuing one engine thoroughly should a startup consider a pivot to one of the others. ENGINES OF GROWTH DETERMINE PRODUCT/MARKET FIT Marc Andreessen, the legendary entrepreneur and investor and one of the fathers of the World Wide Web, coined the term product/market t to describe the moment when a startup nally finds a widespread set of customers that resonate with its product: In a great market—a market with lots of real potential customers—the market pulls product out of the startup. This is the story of search keyword advertising, Internet auctions, and TCP/IP routers. Conversely, in a terrible market, you can have the best product in the world and an absolutely killer team, and it doesn’t matter—you’re going to fail.3 When you see a startup that has found a t with a large market, it’s exhilarating. It leaves no room for doubt. It is Ford’s Model T

ying out of the factory as fast as it could be made, Facebook sweeping college campuses practically overnight, or Lotus taking the business world by storm, selling $54 million worth of Lotus 1-23 in its first year of operation. Startups occasionally ask me to help them evaluate whether they have achieved product/market t. It’s easy to answer: if you are asking, you’re not there yet. Unfortunately, this doesn’t help companies gure out how to get closer to product/market t. How can you tell if you are on the verge of success or hopelessly far away? Although I don’t think Andreessen intended this as part of his de nition, to many entrepreneurs it implies that a pivot is a failure event—“our startup has failed to achieve product/market t.” It also implies the inverse—that once our product has achieved product/market t, we won’t have to pivot anymore. Both assumptions are wrong. I believe the concept of the engine of growth can put the idea of product/market t on a more rigorous footing. Since each engine of growth can be de ned quantitatively, each has a unique set of metrics that can be used to evaluate whether a startup is on the verge of achieving product/market t. A startup with a viral coe cient of 0.9 or more is on the verge of success. Even better, the metrics for each engine of growth work in tandem with the innovation accounting model discussed in Chapter 7 to give direction to a startup’s product development e orts. For example, if a startup is attempting to use the viral engine of growth, it can focus its development e orts on things that might a ect customer behavior—on the viral loop—and safely ignore those that do not. Such a startup does not need to specialize in marketing, advertising, or sales functions. Conversely, a company using the paid engine needs to develop those marketing and sales functions urgently. A startup can evaluate whether it is getting closer to product/market t as it tunes its engine by evaluating each trip through the Build-Measure-Learn feedback loop using innovation accounting. What really matters is not the raw numbers or vanity metrics but the direction and degree of progress.

For example, imagine two startups that are working diligently to tune the sticky engine of growth. One has a compounding rate of growth of 5 percent, and the other 10 percent. Which company is the better bet? On the surface, it may seem that the larger rate of growth is better, but what if each company’s innovation accounting dashboard looks like the following chart? COMPOUNDING GROWTH RATE AS COMPANY COMPANY OF A B Six months ago 0.1% 9.8% Five months ago 0.5% 9.6% Four months ago 2.0% 9.9% Three months ago 3.2% 9.8% Two months ago 4.5% 9.7% One month ago 5.0% 10.0% Even with no insight into these two companies’ gross numbers, we can tell that company A is making real progress whereas company B is stuck in the mud. This is true even though company B is growing faster than company A right now. WHEN ENGINES RUN OUT Getting a startup’s engine of growth up and running is hard enough, but the truth is that every engine of growth eventually runs out of gas. Every engine is tied to a given set of customers and their related habits, preferences, advertising channels, and interconnections. At some point, that set of customers will be exhausted. This may take a long time or a short time, depending on one’s industry and timing.

Chapter 6 emphasized the importance of building the minimum viable product in such a way that it contains no additional features beyond what is required by early adopters. Following that strategy successfully will unlock an engine of growth that can reach that target audience. However, making the transition to mainstream customers will require tremendous additional work.4 Once we have a product that is growing among early adopters, we could in theory stop work in product development entirely. The product would continue to grow until it reached the limits of that early market. Then growth would level o or even stop completely. The challenge comes from the fact that this slowdown might take months or even years to take place. Recall from Chapter 8 that IMVU failed this test—at first—for precisely this reason. Some unfortunate companies wind up following this strategy inadvertently. Because they are using vanity metrics and traditional accounting, they think they are making progress when they see their numbers growing. They falsely believe they are making their product better when in fact they are having no impact on customer behavior. The growth is all coming from an engine of growth that is working—running e ciently to bring in new customers—not from improvements driven by product development. Thus, when the growth suddenly slows, it provokes a crisis. This is the same problem that established companies experience. Their past successes were built on a nely tuned engine of growth. If that engine runs its course and growth slows or stops, there can be a crisis if the company does not have new startups incubating within its ranks that can provide new sources of growth. Companies of any size can su er from this perpetual a iction. They need to manage a portfolio of activities, simultaneously tuning their engine of growth and developing new sources of growth for when that engine inevitably runs its course. How to do this is the subject of Chapter 12. However, before we can manage that portfolio, we need an organizational structure, culture, and discipline that can handle these rapid and often unexpected changes. I call this an adaptive organization, and it is the subject of

Chapter 11.

11 ADAPT I was the CTO of IMVU, I thought I was doing a good job Whenmost of the time. I had built an agile engineering organization, and we were successfully experimenting with the techniques that would come to be known as the Lean Startup. However, on a couple of occasions I suddenly realized that I was failing at my job. For an achievement-oriented person, that is incredibly disarming. Worst of all, you don’t get a memo. If you did, it would read something like this: Dear Eric, Congratulations! The job you used to do at this company is no longer available. However, you have been transferred to a new job in the company. Actually, it’s not the same company anymore, even though it has the same name and many of the same people. And although the job has the same title, too, and you used to be good at your old job, you’re already failing at the new one. This transfer is e ective as of six months ago, so this is to alert you that you’ve already been failing at it for quite some time. Best of luck! Every time this happened to me, I struggled to gure out what to do. I knew that as the company grew, we would need additional processes and systems designed to coordinate the company’s operations at each larger size. And yet I had also seen many startups

become ossi ed and bureaucratic out of a misplaced desire to become “professional.” Having no system at all was not an option for IMVU and is not an option for you. There are so many ways for a startup to fail. I’ve lived through the overarchitecture failure, in which attempting to prevent all the various kinds of problems that could occur wound up delaying the company from putting out any product. I’ve seen companies fail the other way from the so-called Friendster e ect, su ering a high-pro le technical failure just when customer adoption is going wild. As a department executive, this outcome is worst of all, because the failure is both high-pro le and attributable to a single function or department—yours. Not only will the company fail, it will be your fault. Most of the advice I’ve heard on this topic has suggested a kind of split-the-di erence approach (as in, “engage in a little planning, but not too much”). The problem with this willy-nilly approach is that it’s hard to give any rationale for why we should anticipate one particular problem but ignore another. It can feel like the boss is being capricious or arbitrary, and that feeds the common feeling that management’s decisions conceal an ulterior motive. For those being managed this way, their incentives are clear. If the boss tends to split the di erence, the best way to in uence the boss and get what you want is to take the most extreme position possible. For example, if one group is advocating for an extremely lengthy release cycle, say, an annual new product introduction, you might choose to argue for an equally extremely short release cycle (perhaps weekly or even daily), knowing that the two opinions will be averaged out. Then, when the di erence is split, you’re likely to get an outcome closer to what you actually wanted in the rst place. Unfortunately, this kind of arms race escalates. Rivals in another camp are likely to do the same thing. Over time, everyone will take the most polarized positions possible, which makes splitting the di erence ever more di cult and ever less successful. Managers have to take responsibility for knowingly or inadvertently creating such incentives. Although it was not their intention to reward extreme polarization, that’s exactly what they are doing.

reward extreme polarization, that’s exactly what they are doing. Getting out of this trap requires a significant shift in thinking. BUILDING AN ADAPTIVE ORGANIZATION Should a startup invest in a training program for new employees? If you had asked me a few years ago, I would have laughed and said, “Absolutely not. Training programs are for big companies that can a ord them.” Yet at IMVU we wound up building a training program that was so good, new hires were productive on their rst day of employment. Within just a few weeks, those employees were contributing at a high level. It required a huge e ort to standardize our work processes and prepare a curriculum of the concepts that new employees should learn. Every new engineer would be assigned a mentor, who would help the new employee work through a curriculum of systems, concepts, and techniques he or she would need to become productive at IMVU. The performance of the mentor and mentee were linked, so the mentors took this education seriously. What is interesting, looking back at this example, is that we never stopped work and decided that we needed to build a great training program. Instead, the training program evolved organically out of a methodical approach to evolving our own process. This process of orientation was subject to constant experimentation and revision so that it grew more effective—and less burdensome—over time. I call this building an adaptive organization, one that automatically adjusts its process and performance to current conditions. Can You Go Too Fast? So far this book has emphasized the importance of speed. Startups are in a life-or-death struggle to learn how to build a sustainable business before they run out of resources and die. However, focusing on speed alone would be destructive. To work, startups

focusing on speed alone would be destructive. To work, startups require built-in speed regulators that help teams nd their optimal pace of work. We saw an example of speed regulation in Chapter 9 with the use of the andon cord in systems such as continuous deployment. It is epitomized in the paradoxical Toyota proverb, “Stop production so that production never has to stop.” The key to the andon cord is that it brings work to a stop as soon as an uncorrectable quality problem surfaces—which forces it to be investigated. This is one of the most important discoveries of the lean manufacturing movement: you cannot trade quality for time. If you are causing (or missing) quality problems now, the resulting defects will slow you down later. Defects cause a lot of rework, low morale, and customer complaints, all of which slow progress and eat away at valuable resources. So far I have used the language of physical products to describe these problems, but that is simply a matter of convenience. Service businesses have the same challenges. Just ask any manager of a training, sta ng, or hospitality rm to show you the playbook that speci es how employees are supposed to deliver the service under various conditions. What might have started out as a simple guide tends to grow inexorably over time. Pretty soon, orientation is incredibly complex and employees have invested a lot of time and energy in learning the rules. Now consider an entrepreneurial manager in that kind of company trying to experiment with new rules or procedures. The higher-quality the existing playbook is, the easier it will be for it to evolve over time. By contrast, a low-quality playbook will be lled with contradictory or ambiguous rules that cause confusion when anything is changed. When I teach the Lean Startup approach to entrepreneurs with an engineering background, this is one of the hardest concepts to grasp. On the one hand, the logic of validated learning and the minimum viable product says that we should get a product into customers’ hands as soon as possible and that any extra work we do beyond what is required to learn from customers is waste. On the other hand, the Build-Measure-Learn feedback loop is a continuous process. We don’t stop after one minimum viable product but use

what we have learned to get to work immediately on the next iteration. Therefore, shortcuts taken in product quality, design, or infrastructure today may wind up slowing a company down tomorrow. You can see this paradox in action at IMVU. Chapter 3 recounted how we wound up shipping a product to customers that was full of bugs, missing features, and bad design. The customers wouldn’t even try that product, and so most of that work had to be thrown away. It’s a good thing we didn’t waste a lot of time xing those bugs and cleaning up that early version. However, as our learning allowed us to build products that customers did want, we faced slowdowns. Having a low-quality product can inhibit learning when the defects prevent customers from experiencing (and giving feedback on) the product’s bene ts. In IMVU’s case, as we o ered the product to more mainstream customers, they were much less forgiving than early adopters had been. Similarly, the more features we added to the product, the harder it became to add even more because of the risk that a new feature would interfere with an existing feature. The same dynamics happen in a service business, since any new rules may con ict with existing rules, and the more rules, the more possibilities for conflict. IMVU used the techniques of this chapter to achieve scale and quality in a just-in-time fashion. THE WISDOM OF THE FIVE WHYS To accelerate, Lean Startups need a process that provides a natural feedback loop. When you’re going too fast, you cause more problems. Adaptive processes force you to slow down and invest in preventing the kinds of problems that are currently wasting time. As those preventive efforts pay off, you naturally speed up again. Let’s return to the question of having a training program for new employees. Without a program, new employees will make mistakes while in their learning curve that will require assistance and intervention from other team members, slowing everyone down.

How do you decide if the investment in training is worth the bene t of speed due to reduced interruptions? Figuring this out from a top-down perspective is challenging, because it requires estimating two completely unknown quantities: how much it will cost to build an unknown program against an unknown bene t you might reap. Even worse, the traditional way to make these kinds of decisions is decidedly large-batch thinking. A company either has an elaborate training program or it does not. Until they can justify the return on investment from building a full program, most companies generally do nothing. The alternative is to use a system called the Five Whys to make incremental investments and evolve a startup’s processes gradually. The core idea of Five Whys is to tie investments directly to the prevention of the most problematic symptoms. The system takes its name from the investigative method of asking the question “Why?” ve times to understand what has happened (the root cause). If you’ve ever had to answer a precocious child who wants to know “Why is the sky blue?” and keeps asking “Why?” after each answer, you’re familiar with it. This technique was developed as a systematic problem-solving tool by Taiichi Ohno, the father of the Toyota Production System. I have adapted it for use in the Lean Startup model with a few changes designed specifically for startups. At the root of every seemingly technical problem is a human problem. Five Whys provides an opportunity to discover what that human problem might be. Taiichi Ohno gives the following example: When confronted with a problem, have you ever stopped and asked why ve times? It is di cult to do even though it sounds easy. For example, suppose a machine stopped functioning: 1. Why did the machine stop? (There was an overload and the fuse blew.) 2. Why was there an overload? (The bearing was not su ciently lubricated.)

3. Why was it not lubricated su ciently? (The lubrication pump was not pumping sufficiently.) 4. Why was it not pumping su ciently? (The shaft of the pump was worn and rattling.) 5. Why was the shaft worn out? (There was no strainer attached and metal scrap got in.) Repeating “why” ve times, like this, can help uncover the root problem and correct it. If this procedure were not carried through, one might simply replace the fuse or the pump shaft. In that case, the problem would recur within a few months. The Toyota production system has been built on the practice and evolution of this scienti c approach. By asking and answering “why” ve times, we can get to the real cause of the problem, which is often hidden behind more obvious symptoms.1 Note that even in Ohno’s relatively simple example the root cause moves away from a technical fault (a blown fuse) and toward a human error (someone forgot to attach a strainer). This is completely typical of most problems that startups face no matter what industry they are in. Going back to our service business example, most problems that at rst appear to be individual mistakes can be traced back to problems in training or the original playbook for how the service is to be delivered. Let me demonstrate how using the Five Whys allowed us to build the employee training system that was mentioned earlier. Imagine that at IMVU we suddenly start receiving complaints from customers about a new version of the product that we have just released. 1. A new release disabled a feature for customers. Why? Because a particular server failed. 2. Why did the server fail? Because an obscure subsystem was used in the wrong way.

3. Why was it used in the wrong way? The engineer who used it didn’t know how to use it properly. 4. Why didn’t he know? Because he was never trained. 5. Why wasn’t he trained? Because his manager doesn’t believe in training new engineers because he and his team are “too busy.” What began as a purely technical fault is revealed quickly to be a very human managerial issue. Make a Proportional Investment Here’s how to use Five Whys analysis to build an adaptive organization: consistently make a proportional investment at each of the ve levels of the hierarchy. In other words, the investment should be smaller when the symptom is minor and larger when the symptom is more painful. We don’t make large investments in prevention unless we’re coping with large problems. In the example above, the answer is to x the server, change the subsystem to make it less error-prone, educate the engineer, and, yes, have a conversation with the engineer’s manager. This latter piece, the conversation with the manager, is always hard, especially in a startup. When I was a startup manager, if you told me I needed to invest in training my people, I would have told you it was a waste of time. There were always too many other things to do. I’d probably have said something sarcastic like “Sure, I’d be happy to do that—if you can spare my time for the eight weeks it’ll take to set up.” That’s manager-speak for “No way in hell.” That’s why the proportional investment approach is so important. If the outage is a minor glitch, it’s essential that we make only a minor investment in xing it. Let’s do the rst hour of the eight-week plan. That may not sound like much, but it’s a start. If the problem recurs, asking the Five Whys will require that we continue to make progress on it. If the problem does not occur again, an hour isn’t a big loss.

I used the example of engineering training because that was something I was reluctant to invest in at IMVU. At the outset of our venture, I thought we needed to focus all of our energies on building and marketing our product. Yet once we entered a period of rapid hiring, repeated Five Whys sessions revealed that problems caused by lack of training were slowing down product development. At no point did we drop everything to focus solely on training. Instead, we made incremental improvements to the process constantly, each time reaping incremental bene ts. Over time, those changes compounded, freeing up time and energy that previously had been lost to firefighting and crisis management. Automatic Speed Regulator The Five Whys approach acts as a natural speed regulator. The more problems you have, the more you invest in solutions to those problems. As the investments in infrastructure or process pay o , the severity and number of crises are reduced and the team speeds up again. With startups in particular, there is a danger that teams will work too fast, trading quality for time in a way that causes sloppy mistakes. Five Whys prevents that, allowing teams to nd their optimal pace. The Five Whys ties the rate of progress to learning, not just execution. Startup teams should go through the Five Whys whenever they encounter any kind of failure, including technical faults, failures to achieve business results, or unexpected changes in customer behavior. Five Whys is a powerful organizational technique. Some of the engineers I have trained to use it believe that you can derive all the other Lean Startup techniques from the Five Whys. Coupled with working in small batches, it provides the foundation a company needs to respond quickly to problems as they appear, without overinvesting or overengineering.

THE CURSE OF THE FIVE BLAMES When teams rst adopt Five Whys as a problem-solving tool, they encounter some common pitfalls. We need systems like Five Whys to overcome our psychological limitations because we tend to overreact to what’s happening in the moment. We also tend to get frustrated if things happen that we did not anticipate. When the Five Whys approach goes awry, I call it the Five Blames. Instead of asking why repeatedly in an attempt to understand what went wrong, frustrated teammates start pointing ngers at each other, trying to decide who is at fault. Instead of using the Five Whys to nd and x problems, managers and employees can fall into the trap of using the Five Blames as a means for venting their frustrations and calling out colleagues for systemic failures. Although it’s human nature to assume that when we see a mistake, it’s due to defects in someone else’s department, knowledge, or character, the goal of the Five Whys is to help us see the objective truth that chronic problems are caused by bad process, not bad people, and remedy them accordingly. I recommend several tactics for escaping the Five Blames. The rst is to make sure that everyone a ected by the problem is in the room during the analysis of the root cause. The meeting should include anyone who discovered or diagnosed the problem, including customer service representatives who elded the calls, if possible. It should include anyone who tried to x the symptom as well as anyone who worked on the subsystems or features involved. If the problem was escalated to senior management, the decision makers who were involved in the escalation should be present as well. This may make for a crowded room, but it’s essential. In my experience, whoever is left out of the discussion ends up being the target for blame. This is just as damaging whether the scapegoat is a junior employee or the CEO. When it’s a junior employee, it’s all too easy to believe that that person is replaceable. If the CEO is not present, it’s all too easy to assume that his or her behavior is unchangeable. Neither presumption is usually correct.

When blame inevitably arises, the most senior people in the room should repeat this mantra: if a mistake happens, shame on us for making it so easy to make that mistake. In a Five Whys analysis, we want to have a systems-level view as much as possible. Here’s a situation in which this mantra came in handy. Because of the training process we had developed at IMVU through the Five Whys, we routinely asked new engineers to make a change to the production environment on their rst day. For engineers trained in traditional development methods, this was often frightening. They would ask, “What will happen to me if I accidentally disrupt or stop the production process?” In their previous jobs, that was a mistake that could get them red. At IMVU we told new hires, “If our production process is so fragile that you can break it on your very rst day of work, shame on us for making it so easy to do so.” If they did manage to break it, we immediately would have them lead the e ort to x the problem as well as the e ort to prevent the next person from repeating their mistake. For new hires who came from companies with a very di erent culture, this was often a stressful initiation, but everyone came through it with a visceral understanding of our values. Bit by bit, system by system, those small investments added up to a robust product development process that allowed all our employees to work more creatively, with greatly reduced fear. Getting Started Here are a few tips on how to get started with the Five Whys that are based on my experience introducing this technique at many other companies. For the Five Whys to work properly, there are rules that must be followed. For example, the Five Whys requires an environment of mutual trust and empowerment. In situations in which this is lacking, the complexity of Five Whys can be overwhelming. In such situations, I’ve often used a simplified version that still allows teams to focus on analyzing root causes while developing the muscles

to focus on analyzing root causes while developing the muscles they’ll need later to tackle the full-blown method. I ask teams to adopt these simple rules: 1. Be tolerant of all mistakes the first time. 2. Never allow the same mistake to be made twice. The rst rule encourages people to get used to being compassionate about mistakes, especially the mistakes of others. Remember, most mistakes are caused by awed systems, not bad people. The second rule gets the team started making proportional investments in prevention. This simpli ed system works well. In fact, we used it at IMVU in the days before I discovered the Five Whys and the Toyota Production System. However, such a simpli ed system does not work e ectively over the long term, as I found out rsthand. In fact, that was one of the things that drove me to rst learn about lean production. The strength and weakness of the simpli ed system is that it invites questions such as What counts as the same problem? What kinds of mistakes should we focus on? and Should we x this individual problem or try to prevent a whole category of related problems? For a team that is just getting started, these questions are thought-provoking and can lay the groundwork for more elaborate methods to come. Ultimately, though, they do need answering. They need a complete adaptive process such as the Five Whys. Facing Unpleasant Truths You will need to be prepared for the fact that Five Whys is going to turn up unpleasant facts about your organization, especially at the beginning. It is going to call for investments in prevention that come at the expense of time and money that could be invested in new products or features. Under pressure, teams may feel that they don’t have time to waste on analyzing root causes even though it would give them more time in the long term. The process

sometimes will devolve into the Five Blames. At all these junctures, it is essential that someone with su cient authority be present to insist that the process be followed, that its recommendations be implemented, and to act as a referee if disagreements are up. Building an adaptive organization, in other words, requires executive leadership to sponsor and support the process. Often, individual contributors at startups come to my workshops, eager to get started with the Five Whys. I caution against attempting to do that if they do not have the buy-in of the manager or team leader. Proceed cautiously if you nd yourself in this situation. It may not be possible to get the entire team together for a true Five Whys inquiry, but you can always follow the simple two-rule version in your own work. Whenever something goes wrong, ask yourself: How could I prevent myself from being in this situation ever again? Start Small, Be Specific Once you are ready to begin, I recommend starting with a narrowly targeted class of symptoms. For example, the rst time I used the Five Whys successfully, I used it to diagnose problems with one of our internal testing tools that did not a ect customers directly. It may be tempting to start with something large and important because that is where most of the time is being wasted as a result of a awed process, but it is also where the pressure will be greatest. When the stakes are high, the Five Whys can devolve into the Five Blames quickly. It’s better to give the team a chance to learn how to do the process first and then expand into higher-stakes areas later. The more speci c the symptoms are, the easier it will be for everyone to recognize when it’s time to schedule a Five Whys meeting. Say you want to use the Five Whys to address billing complaints from customers. In that case, pick a date after which all billing complaints will trigger a Five Whys meeting automatically. Note that this requires that there be a small enough volume of complaints that having this meeting every time one comes in is

practical. If there are already too many complaints, pick a subset on which you want to focus. Make sure that the rule that determines which kinds of complaints trigger a Five Whys meeting is simple and ironclad. For example, you might decide that every complaint involving a credit card transaction will be investigated. That’s an easy rule to follow. Don’t pick a rule that is ambiguous. At rst, the temptation may be to make radical and deep changes to every billing system and process. Don’t. Instead, keep the meetings short and pick relatively simple changes at each of the ve levels of the inquiry. Over time, as the team gets more comfortable with the process, you can expand it to include more and more types of billing complaints and then to other kinds of problems. Appoint a Five Whys Master To facilitate learning, I have found it helpful to appoint a Five Whys master for each area in which the method is being used. This individual is tasked with being the moderator for each Five Whys meeting, making decisions about which prevention steps to take, and assigning the follow-up work from that meeting. The master must be senior enough to have the authority to ensure that those assignments get done but should not be so senior that he or she will not be able to be present at the meetings because of con icting responsibilities. The Five Whys master is the point person in terms of accountability; he or she is the primary change agent. People in this position can assess how well the meetings are going and whether the prevention investments that are being made are paying off. THE FIVE WHYS IN ACTION IGN Entertainment, a division of News Corporation, is an online video games media company with the biggest audience of video

game players in the world. More than 45 million gamers frequent its portfolio of media properties. IGN was founded in the late 1990s, and News Corporation acquired it in 2005. IGN has grown to employ several hundred people, including almost a hundred engineers. Recently, I had the opportunity to speak to the product development team at IGN. They had been successful in recent years, but like all the established companies we’ve seen throughout this book, they were looking to accelerate new product development and nd ways to be more innovative. They brought together their engineering, product, and design teams to talk through ways they could apply the Lean Startup model. This change initiative had the support of IGN’s senior management, including the CEO, the head of product development, the vice president of engineering, the publisher, and the head of product. Their previous efforts at Five Whys had not gone smoothly. They had attempted to tackle a laundry list of problem areas nominated by the product team. The issues varied from discrepancies in web analytics to partner data feeds that were not working. Their rst Five Whys meeting took an hour, and although they came up with some interesting takeaways, as far as the Five Whys goes, it was a disaster. None of the people who were connected to and knew the most about the issues were at the meeting, and because this was the rst time they were doing the Five Whys together, they didn’t stick to the format and went o on many tangents. It wasn’t a complete waste of time, but it didn’t have any of the bene ts of the adaptive style of management discussed in this chapter. Don’t Send Your Baggage through the Five Whys Process IGN had the experience of trying to solve all of its “baggage” issues that had been causing wasted time for many years. Because this is an overwhelming set of problems, nding xes quickly proves overwhelming.

In their zeal to get started with the Five Whys, IGN neglected three important things: 1. To introduce Five Whys to an organization, it is necessary to hold Five Whys sessions as new problems come up. Since baggage issues are endemic, they naturally come up as part of the Five Whys analysis and you can take that opportunity to x them incrementally. If they don’t come up organically, maybe they’re not as big as they seem. 2. Everyone who is connected to a problem needs to be at the Five Whys session. Many organizations face the temptation to save time by sparing busy people from the root cause analysis. This is a false economy, as IGN discovered the hard way. 3. At the beginning of each Five Whys session, take a few minutes to explain what the process is for and how it works for the bene t of those who are new to it. If possible, use an example of a successful Five Whys session from the past. If you’re brand new, you can use my earlier example about the manager who doesn’t believe in training. IGN learned that, whenever possible, it helps to use something that has personal meaning for the team. After our meeting, the IGN leadership decided to give Five Whys another try. Following the advice laid out in this chapter, they appointed a Five Whys master named Tony Ford, a director of engineering. Tony was an entrepreneur who had come to IGN through an acquisition. He got his start with Internet technology, building websites about video games in the late 1990s. Eventually that led to an opportunity at a startup, TeamXbox, where he served as the lead software developer. TeamXbox was acquired by IGN Entertainment in 2003, and since that time Tony has been a technologist, leader of innovation, and proponent of agile and lean practices there. Unfortunately, Tony started without picking a narrow problem area on which to focus. This led to early setbacks and frustration.

Tony relates, “As the new master I wasn’t very good at traversing through the Five Whys e ectively, and the problems we were trying to solve were not great candidates in the rst place. As you can imagine, these early sessions were awkward and in the end not very useful. I was getting quite discouraged and frustrated.” This is a common problem when one tries to tackle too much at once, but it is also a consequence of the fact that these skills take time to master. Luckily, Tony persevered: “Having a Five Whys master is critical in my opinion. Five Whys is easy in theory but di cult in practice, so you need someone who knows it well to shape the sessions for those who don’t.” The turnaround came when Tony led a Five Whys session involving a project that had been missing its deadlines. The session was fascinating and insightful and produced meaningful proportional investments. Tony explains: “The success had to do with a more experienced master and more experienced attendees. We all knew what the Five Whys was, and I did a really good job keeping us on track and away from tangents. This was a pivotal moment. Right then I knew the Five Whys was a new tool that was going to have a real impact on our overall success as a team and as a business.” On the surface, Five Whys seems to be about technical problems and preventing mistakes, but as teams drive out these super cial wastes, they develop a new understanding of how to work together. Tony put it this way: “I daresay that I discovered that the Five Whys transcends root cause analysis by revealing information that brings your team closer through a common understanding and perspective. A lot of times a problem can pull people apart; Five Whys does the opposite.” I asked Tony to provide an example of a recent successful Five Whys analysis from IGN. His account of it is listed in the sidebar. Why couldn’t you add or edit posts on the blogs? Answer: Any post request (write) to the article content api was

returning a 500 error. Proportional investment: Jim—We’ll work on the API, but let’s make our CMS more forgiving for the user. Allow users to add and edit drafts without errors for a better user experience. Why was the content API returning 500 errors? Answer: The bson_ext gem was incompatible with other gems it depends upon. Proportional investment: King—Remove the gem (already done to resolve the outage). Why was the gem incompatible? Answer: We added a new version of the gem in addition to the existing version and the app started using it unexpectedly. Proportional investment: Bennett—Convert our rails app to use bundler for gem management. Why did we add a new version of a gem in production without testing? Answer: We didn’t think we needed a test in these cases. Proportional investment: Bennett and Jim—Write a unit or functional test in the API and CMS that will catch this in the future. Why do we add additional gems that we don’t intend to use right away? Answer: In preparation for a code push we wanted to get all new gems ready in the production environment. Even though our code deployments are fully automated, gems are not. Proportional investment: Bennett—Automate gem management

and installation into Continuous Integration and Continuous Deployment process. Bonus—Why are we doing things in production on Friday nights? Answer: Because no one says we can’t and it was a convenient time for the developer to prepare for a deployment we’d be doing on Monday. Proportional investment: Tony—Make an announcement to the team. There will be no production changes on Friday, Saturday, or Sunday unless an exception has been made and approved by David (VP Engineering). We will reevaluate this policy when we have a fully automated continuous deployment process in place. As a result of this Five Whys session and the proportional investments we made, our deployments are easier, quicker, and never again will our process allow a developer to place gems into production systems with unintended consequences. Indeed, we have not had another issue like this. We strengthened our “cluster immune system” as you would say. Without the Five Whys, we would have never discovered all of the information we did here. My guess is that we would have told that one developer to not do stupid things on Friday nights and moved on. This is what I emphasized earlier, where a good Five Whys session has two outputs, learning and doing. The proportional investments that came out of this session are obviously valuable, but the learnings are much more subtle, but amazing for growing as developers and as a team.

ADAPTING TO SMALLER BATCHES Before leaving the topic of building an adaptive organization, I want to introduce one more story. This one concerns a product that you’ve probably used if you’ve ever run your own business. It’s called QuickBooks, and it is one of Intuit’s flagship products. QuickBooks has been the leading product in its category for many years. As a result, it has a large and dedicated customer base, and Intuit expects it to contribute signi cantly to its bottom line. Like most personal computer (PC) software of the last two decades, QuickBooks has been launched on an annual cycle, in one giant batch. This was how it worked three years ago, when Greg Wright, the director of product marketing for QuickBooks, joined the team. As you can imagine, there were lots of existing processes in place to ensure a consistent product and an on-time release. The typical release approach was to spend signi cant up-front time to identify the customers’ need: Typically the rst three to four months of each annual cycle was spent strategizing and planning, without building new features. Once a plan and milestones were established, the team would spend the next six to nine months building. This would culminate in a big launch, and then the team would get its rst feedback on whether it had successfully delivered on customers’ needs at the end of the process. So here was the time line: start process in September, first beta release is in June, second beta is in July. The beta is essentially testing to make sure it doesn’t crash people’s computers or cause them to lose their data—by that time in the process, only major bugs can be xed. The design of the product itself is locked. This is the standard “waterfall” development methodology that product development teams have used for years. It is a linear, largebatch system that relies for success on proper forecasting and planning. In other words, it is completely maladapted for today’s

planning. In other words, it is completely maladapted for today’s rapidly changing business environment. Year One: Achieving Failure Greg witnessed this breakdown in 2009, his rst year on the QuickBooks team. That year, the company shipped an entirely new system in QuickBooks for online banking, one of its most important features. The team went through rounds of usability testing using mock-ups and nonfunctional prototypes, followed by signi cant beta testing using sample customer data. At the moment of the launch, everything looked good. The rst beta release was in June, and customer feedback started coming in negative. Although customers were complaining, there wasn’t su cient cause to stop the release because it was technically awless—it didn’t crash computers. At that point, Greg was in a bind. He had no way of knowing how the feedback would translate to real customer behavior in the market. Were these just isolated complaints, or part of a widespread problem? He did know one thing for sure, though: that his team could not a ord to miss the deadline. When the product nally shipped, the results were terrible. It took customers four to ve times longer to reconcile their banking transactions than it had with the older version. In the end, Greg’s team had failed to deliver on the customer need they were trying to address (despite building the product to speci cation), and because the next release had to go through the same waterfall process, it took the team nine months to x. This is a classic case of “achieving failure”—successfully executing a flawed plan. Intuit uses a tracking survey called the Net Promoter Score2 to evaluate customer satisfaction with its many products. This is a great source of actionable metrics about what customers really think about a product. In fact, I used it at IMVU, too. One thing that is nice about NPS is that it is very stable over time. Since it is measuring core customer satisfaction, it is not subject to minor uctuations; it registers only major changes in customer sentiment.

uctuations; it registers only major changes in customer sentiment. That year, the QuickBooks score dropped 20 points, the rst time the company had ever moved the needle with the Net Promoter Score. That 20-point drop resulted in significant losses for Intuit and was embarrassing for the company—all because customer feedback came too late in the process, allowing no time to iterate. Intuit’s senior management, including the general manager of the small business division and the head of small business accounting, recognized the need for change. To their credit, they tasked Greg with driving that change. His mission: to achieve startup speed for the development and deployment of QuickBooks. Year Two: Muscle Memory The next chapter of this story illustrates how hard it is to build an adaptive organization. Greg set out to change the QuickBooks development process by using four principles: 1. Smaller teams. Shift from large teams with uniform functional roles to smaller, fully engaged teams whose members take on different roles. 2. Achieve shorter cycle times. 3. Faster customer feedback, testing both whether it crashes customers’ computers and the performance of new features/customer experience. 4. Enable and empower teams to make fast and courageous decisions. On the surface, these goals seem to be aligned with the methods and principles described in previous chapters, but Greg’s second year with QuickBooks was not a marked success. For example, he decreed that the team would move to a midyear release milestone, e ectively cutting the cycle time and batch size in half. However, this was not successful. Through sheer determination, the team tried valiantly to get an alpha release out in January. However, the problems that a ict large-batch development were still present,

problems that a ict large-batch development were still present, and the team struggled to complete the alpha by April. That represented an improvement over the past system because issues could be brought to the surface two months earlier than under the old way, but it did not produce the dramatically better results Greg was looking for. In fact, over the course of the year, the team’s process kept looking more and more like it had in prior years. As Greg put it, “Organizations have muscle memory,” and it is hard for people to unlearn old habits. Greg was running up against a system, and making individual changes such as arbitrarily changing the release date were no match for it. Year Three: Explosion Frustrated by the limited progress in the previous year, Greg teamed up with the product development leader Himanshu Baxi. Together they tossed out all the old processes. They made a public declaration that their combined teams would be creating new processes and that they were not going to go back to the old way. Instead of focusing on new deadlines, Greg and Himanshu invested in process, product, and technology changes that enabled working in smaller batches. Those technical innovations helped them get the desktop product to customers faster for feedback. Instead of building a comprehensive road map at the beginning of the year, Greg kicked o the year with what they called idea/code/solution jams that brought engineers, product managers, and customers together to create a pipeline of ideas. It was scary for Greg as a product manager to start the year without a de ned list of what would be in the product release, but he had con dence in his team and the new process. There were three differences in year three: • Teams were involved in creating new technologies, processes, and systems. • Cross-functional teams were formed around new great ideas.

• Customers were involved from the inception of each feature concept. It’s important to understand that the old approach did not lack customer feedback or customer involvement in the planning process. In the true spirit of genchi gembutsu, Intuit product managers (PMs) would do “follow-me-homes” with customers to identify problems to solve in the next release. However, the PMs were responsible for all the customer research. They would bring it back to the team and say, “This is the problem we want to solve, and here are ideas for how we could solve it.” Changing to a cross-functional way of working was not smooth sailing. Some team members were skeptical. For example, some product managers felt that it was a waste of time for engineers to spend time in front of customers. The PMs thought that their job was to gure out the customer issue and de ne what needed to be built. Thus, the reaction of some PMs to the change was: “What’s my job? What am I supposed to be doing?” Similarly, some on the engineering side just wanted to be told what to do; they didn’t want to talk to customers. As is typically the case in large-batch development, both groups had been willing to sacri ce the team’s ability to learn in order to work more “efficiently.” Communication was critical for this change process to succeed. All the team leaders were open about the change they were driving and why they were driving it. Much of the skepticism they faced was based on the fact that they did not have concrete examples of where this had worked in the past; it was an entirely new process for Intuit. They had to explain clearly why the old process didn’t work and why the annual release “train” was not setting them up for success. Throughout the change they communicated the process outcomes they were shooting for: earlier customer feedback and a faster development cycle that was decoupled from the annual release time line. They repeatedly emphasized that the new approach was how startup competitors were working and iterating. They had to follow suit or risk becoming irrelevant.

Historically, QuickBooks had been built with large teams and long cycle times. For example, in earlier years the ill-fated online banking team had been composed of fteen engineers, seven quality assurance specialists, a product manager, and at times more than one designer. Now no team is bigger than ve people. The focus of each team is iterating with customers as rapidly as possible, running experiments, and then using validated learning to make real-time investment decisions about what to work on. As a result, whereas they used to have ve major “branches” of QuickBooks that merged features at the time of the launch, now there are twenty to twenty- ve branches. This allows for a much larger set of experiments. Each team works on a new feature for approximately six weeks end to end, testing it with real customers throughout the process. Although the primary changes that are required in an adaptive organization are in the mind-set of its employees, changing the culture is not su cient. As we saw in Chapter 9, lean management requires treating work as a system and then dealing with the batch size and cycle time of the whole process. Thus, to achieve lasting change, the QuickBooks team had to invest in tools and platform changes that would enable the new, faster way of working. For example, one of the major stress points in the attempt to release an early alpha version the previous year was that QuickBooks is a mission-critical product. Many small businesses use it as their primary repository for critical nancial data. The team was extremely wary of releasing a minimum viable product that had any risk of corrupting customer data. Therefore, even if they worked in smaller teams with a smaller scope, the burden of all that risk would have made it hard to work in smaller batches. To get the batch size down, the QuickBooks team had to invest in new technology. They built a virtualization system that allowed them to run multiple versions of QuickBooks on a customer’s computer. The second version could access all the customer’s data but could not make permanent changes to it. Thus, there was no risk of the new version corrupting the customer’s data by accident.

This allowed them to isolate new releases to allow selected real customers to test them and provide feedback. The results in year three were promising. The version of QuickBooks that shipped that year had signi cantly higher customer satisfaction ratings and sold more units. If you’re using QuickBooks right now, odds are you are using a version that was built in small batches. As Greg heads into his fourth year with the QuickBooks team, they are exploring even more ways to drive down batch size and cycle time. As usual, there are possibilities that go beyond technical solutions. For example, the annual sales cycle of boxed desktop software is a signi cant barrier to truly rapid learning, and so the team has begun experimenting with subscription-based products for the most active customers. With customers downloading updates online, Intuit can release software on a more frequent basis. Soon this program will see the QuickBooks team releasing to customers quarterly.3 As Lean Startups grow, they can use adaptive techniques to develop more complex processes without giving up their core advantage: speed through the Build-Measure-Learn feedback loop. In fact, one of the primary bene ts of using techniques that are derived from lean manufacturing is that Lean Startups, when they grow up, are well positioned to develop operational excellence based on lean principles. They already know how to operate with discipline, develop processes that are tailor-made to their situation, and use lean techniques such as the Five Whys and small batches. As a successful startup makes the transition to an established company, it will be well poised to develop the kind of culture of disciplined execution that characterizes the world’s best firms, such as Toyota. However, successfully growing into an established company is not the end of the story. A startup’s work is never done, because as was discussed in Chapter 2, even established companies must struggle to nd new sources of growth through disruptive innovation. This imperative is coming earlier in companies’ lives. No longer can a

successful startup expect to have years after its initial public o ering to bask in market-leading success. Today successful companies face immediate pressure from new competitors, fast followers, and scrappy startups. As a result, it no longer makes sense to think of startups as going through discrete phases like the proverbial metamorphosis of a caterpillar to a butter y. Both successful startups and established companies alike must learn to juggle multiple kinds of work at the same time, pursuing operational excellence and disruptive innovation. This requires a new kind of portfolio thinking, which is the subject of Chapter 12.

12 INNOVATE wisdom holds that when companies become larger, Conventional they inevitably lose the capacity for innovation, creativity, and growth. I believe this is wrong. As startups grow, entrepreneurs can build organizations that learn how to balance the needs of existing customers with the challenges of nding new customers to serve, managing existing lines of business, and exploring new business models—all at the same time. And, if they are willing to change their management philosophy, I believe even large, established companies can make this shift to what I call portfolio thinking. HOW TO NURTURE DISRUPTIVE INNOVATION Successful innovation teams must be structured correctly in order to succeed. Venture-backed and bootstrapped startups naturally have some of these structural attributes as a consequence of being small, independent companies. Internal startup teams require support from senior management to create these structures. Internal or external, in my experience startup teams require three structural attributes: scarce but secure resources, independent authority to develop their business, and a personal stake in the outcome. Each of these requirements is di erent from those of established company divisions. Keep in mind that structure is merely a prerequisite—it does not guarantee success. But getting the structure wrong can lead to almost certain failure.

Scarce but Secure Resources Division leaders in large, established organizations are adept at using politics to enlarge their budgets but know that those budgets are somewhat loose. They often acquire as large a budget as possible and prepare to defend it against incursions from other departments. Politics means that they sometimes win and sometimes lose: if a crisis emerges elsewhere in the organization, their budget might suddenly be reduced by 10 percent. This is not a catastrophe; teams will have to work harder and do more with less. Most likely, the budget has some padding in anticipation of this kind of eventuality. Startups are di erent: too much budget is as harmful as too little —as countless dot-com failures can attest—and startups are extremely sensitive to midcourse budgetary changes. It is extremely rare for a stand-alone startup company to lose 10 percent of its cash on hand suddenly. In a large number of cases, this would be a fatal blow, as independent startups are run with little margin for error. Thus, startups are both easier and more demanding to run than traditional divisions: they require much less capital overall, but that capital must be absolutely secure from tampering. Independent Development Authority Startup teams need complete autonomy to develop and market new products within their limited mandate. They have to be able to conceive and execute experiments without having to gain an excessive number of approvals. I strongly recommend that startup teams be completely crossfunctional, that is, have full-time representation from every functional department in the company that will be involved in the creation or launch of their early products. They have to be able to build and ship actual functioning products and services, not just prototypes. Hando s and approvals slow down the Build-Measure-

prototypes. Hando s and approvals slow down the Build-MeasureLearn feedback loop and inhibit both learning and accountability. Startups require that they be kept to an absolute minimum. Of course, this level of development autonomy is liable to raise fears in a parent organization. Alleviating those fears is a major goal of the method recommended below. A Personal Stake in the Outcome Third, entrepreneurs need a personal stake in the outcome of their creations. In stand-alone new ventures, this usually is achieved through stock options or other forms of equity ownership. Where a bonus system must be used instead, the best incentives are tied to the long-term performance of the new innovation. However, I do not believe that a personal stake has to be nancial. This is especially important in organizations, such as nonpro ts and government, in which the innovation is not tied to nancial objectives. In these cases, it is still possible for teams to have a personal stake. The parent organization has to make it clear who the innovator is and make sure the innovator receives credit for having brought the new product to life—if it is successful. As one entrepreneur who ran her own division at a major media company told me, “Financial incentives aside, I always felt that because my name was on the door, I had more to lose and more to prove than someone else. That sense of ownership is not insignificant.” This formula is e ective in for-pro t companies as well. At Toyota, the manager in charge of developing a new vehicle from start to finish is called the shusa, or chief engineer: Shusa are often called heavy-weight project managers in the U.S. literature, but this name understates their real roles as design leaders. Toyota employees translate the term as chief engineer, and they refer to the vehicle under development as the shusa’s car. They assured us that the shusa has nal, absolute authority over every aspect of vehicle

development.1 On the ip side, I know an extremely high-pro le technology company that has a reputation for having an innovative culture, yet its track record of producing new products is disappointing. The company boasts an internal reward system that is based on large nancial and status awards to teams that do something extraordinary, but those awards are handed out by senior management on the basis of—no one knows what. There are no objective criteria by which a team can gauge whether it will win this coveted lottery. Teams have little con dence that they will receive any long-term ownership of their innovations. Thus, teams rarely are motivated to take real risks, instead focusing their energies on projects that are expected to win the approval of senior management. CREATING A PLATFORM FOR EXPERIMENTATION Next, it is important to focus on establishing the ground rules under which autonomous startup teams operate: how to protect the parent organization, how to hold entrepreneurial managers accountable, and how to reintegrate an innovation back into the parent organization if it is successful. Recall the “island of freedom” that enabled the SnapTax team—in Chapter 2—to successfully create a startup within Intuit. That’s what a platform for experimentation can do. Protecting the Parent Organization Conventionally, advice about internal innovators focuses on protecting the startup from the parent organization. I believe it is necessary to turn this model on its head. Let me begin by describing a fairly typical meeting from one of my consulting clients, a large company. Senior management had gathered to make decisions about what to include in the next

version of its product. As part of the company’s commitment to being data-driven, it had tried to conduct an experiment on pricing. The rst part of the meeting was taken up with interpreting the data from the experiment. One problem was that nobody could agree on what the data meant. Many custom reports had been created for the meeting; the data warehouse team was at the meeting too. The more they were asked to explain the details of each row on the spreadsheet, the more evident it became that nobody understood how those numbers had been derived. What we were left looking at was the number of gross sales of the product at a variety of di erent price points, broken down by quarter and by customer segment. It was a lot of data to try to comprehend. Worse, nobody was sure which customers had been exposed to the experiment. Di erent teams had been responsible for implementing it, and so di erent parts of the product had been updated at di erent times. The whole process had taken many months, and by this point, the people who had conceived the experiment had been moved to a division separate from that of the people who had executed it. You should be able to spot the many problems with this situation: the use of vanity metrics instead of actionable metrics, an overly long cycle time, the use of large batch sizes, an unclear growth hypothesis, a weak experimental design, a lack of team ownership, and therefore very little learning. Listening in, I assumed this would be the end of the meeting. With no agreed-on facts to help make the decision, I thought nobody would have any basis for making the case for a particular action. I was wrong. Each department simply took whatever interpretation of the data supported its position best and started advocating on its own behalf. Other departments would chime in with alternative interpretations that supported their positions, and so on. In the end, decisions were not made based on data. Instead, the executive running the meeting was forced to base decisions on the most plausible-sounding arguments. It seemed wasteful to me how much of the meeting had been

spent debating the data because, in the end, the arguments that carried the day could have been made right at the start. It was as if each advocate sensed that he or she was about to be ambushed; if another team managed to bring clarity to the situation, it might undermine that person, and so the rational response was to obfuscate as much as possible. What a waste. Ironically, meetings like this had given data-driven decision making and experimentation a bad name inside the company, and for good reason. The data warehousing team was producing reports that nobody read or understood. The project teams felt the experiments were a waste of time, since they involved building features halfway, which meant they were never any good. “Running an experiment” seemed to them to be code for postponing a hard decision. Worst of all, the executive team experienced the meetings as chronic headaches. Their old product prioritization meetings might have been little more than a battle of opinions, but at least the executives understood what was going on. Now they had to go through a ritual that involved complex math and reached no de nite outcome, and then they ended up having a battle of opinions anyway. Rational Fears However, at the heart of this departmental feud was a very rational fear. This company served two customer segments: a business-tobusiness enterprise segment and a consumer segment. In the B2B segment, the company employed sales sta to sell large volumes of the product to other companies, whereas the consumer segment was driven mostly by one-o purchases made by individuals. The bulk of the company’s current revenue came from B2B sales, but growth in that segment had been slowing. Everyone agreed there was tremendous potential for growth in the consumer segment, but so far little had materialized. Part of the cause of this lack of growth was the current pricing structure. Like many companies that sell to large enterprises, this

structure. Like many companies that sell to large enterprises, this one published a high list price and then provided heavy discounts to “favored” corporate clients who bought in bulk. Naturally, every salesperson was encouraged to make all of his or her clients feel favored. Unfortunately, the published list price was much too high for the consumer segment. The team in charge of growing the consumer segment wanted to run experiments with a lower price structure. The team in charge of the enterprise segment was nervous that this would cannibalize or otherwise diminish its existing relationships with its customers. What if those customers discovered that individuals were getting a lower price than they were? Anyone who has been in a multisegment business will recognize that there are many possible solutions to this problem, such as creating tiered feature sets so that di erent customers are able to purchase di erent “levels” of the product (as in airline seating) or even supporting different products under separate brand names. Yet the company was struggling to implement any of those solutions. Why? Out of fear of endangering the current business, each proposed experiment would be delayed, sabotaged, and obfuscated. It’s important to emphasize that this fear is well founded. Sabotage is a rational response from managers whose territory is threatened. This company is not a random, tiny startup with nothing to lose. An established company has a lot to lose. If the revenue from the core business goes down, heads will roll. This is not something to be taken lightly. The Dangers of Hiding Innovation inside the Black Box The imperative to innovate is unrelenting. Without the ability to experiment in a more agile manner, this company eventually would su er the fate described in The Innovator’s Dilemma: ever-higher pro ts and margins year after year until the business suddenly collapsed. We often frame internal innovation challenges by asking, How can we protect the internal startup from the parent organization? I

can we protect the internal startup from the parent organization? I would like to reframe and reverse the question: How can we protect the parent organization from the startup? In my experience, people defend themselves when they feel threatened, and no innovation can ourish if defensiveness is given free rein. In fact, this is why the common suggestion to hide the innovation team is misguided. There are examples of one-time successes using a secret skunkworks or o -site innovation team, such as the building of the original IBM PC in Boca Raton, Florida, completely separate from mainline IBM. But these examples should serve mostly as cautionary tales, because they have rarely led to sustainable innovation.2 Hiding from the parent organization can have longterm negative consequences. Consider it from the point of view of the managers who have the innovation sprung on them. They are likely to feel betrayed and more than a little paranoid. After all, if something of this magnitude could be hidden, what else is waiting in the shadows? Over time, this leads to more politics as managers are incentivized to ferret out threats to their power, in uence, and careers. The fact that the innovation was a success is no justi cation for this dishonest behavior. From the point of view of established managers, the message is clear: if you are not on the inside, you are liable to be blindsided by this type of secret. It is unfair to criticize these managers for their response; the criticism should be aimed at senior executives who failed to design a supportive system in which to operate and innovate. I believe this is one reason why companies such as IBM lost their leadership position in the new markets that they developed using a black box such as the PC business; they are unable to re-create and sustain the culture that led to the innovation in the first place. Creating an Innovation Sandbox The challenge here is to create a mechanism for empowering innovation teams out in the open. This is the path toward a sustainable culture of innovation over time as companies face

repeated existential threats. My suggested solution is to create a sandbox for innovation that will contain the impact of the new innovation but not constrain the methods of the startup team. It works as follows: 1. Any team can create a true split-test experiment that a ects only the sandboxed parts of the product or service (for a multipart product) or only certain customer segments or territories (for a new product). However: 2. One team must see the whole experiment through from end to end. 3. No experiment can run longer than a speci ed amount of time (usually a few weeks for simple feature experiments, longer for more disruptive innovations). 4. No experiment can a ect more than a speci ed number of customers (usually expressed as a percentage of the company’s total mainstream customer base). 5. Every experiment has to be evaluated on the basis of a single standard report of five to ten (no more) actionable metrics. 6. Every team that works inside the sandbox and every product that is built must use the same metrics to evaluate success. 7. Any team that creates an experiment must monitor the metrics and customer reactions (support calls, social media reaction, forum threads, etc.) while the experiment is in progress and abort it if something catastrophic happens. At the beginning, the sandbox has to be quite small. In the company above, the sandbox initially contained only the pricing page. Depending on the types of products the company makes, the size of the sandbox can be de ned in di erent ways. For example, an online service might restrict it to certain pages or user ows. A retail operation might restrict it to certain stores or geographic areas. Companies trying to bring an entirely new product to market might build the restriction around customers in certain segments. Unlike in a concept test or market test, customers in the sandbox

are considered real and the innovation team is allowed to attempt to establish a long-term relationship with them. After all, they may be experimenting with those early adopters for a long time before their learning milestones are accomplished. Whenever possible, the innovation team should be crossfunctional and have a clear team leader, like the Toyota shusa. It should be empowered to build, market, and deploy products or features in the sandbox without prior approval. It should be required to report on the success or failure of those e orts by using standard actionable metrics and innovation accounting. This approach can work even for teams that have never before worked cross-functionally. The rst few changes, such as a price change, may not require great engineering e ort, but they require coordination across departments: engineering, marketing, customer service. Teams that work this way are more productive as long as productivity is measured by their ability to create customer value and not just stay busy. True experiments are easy to classify as successes or failures because top-level metrics either move or they don’t. Either way, the team learns immediately whether its assumptions about how customers will behave are correct. By using the same metrics each time, the team builds literacy about those metrics across the company. Because the innovation team is reporting on its progress by using the system of innovation accounting described in Part Two, anyone who reads those reports is getting an implicit lesson in the power of actionable metrics. This effect is extremely powerful. Even if someone wants to sabotage the innovation team, he or she will have to learn all about actionable metrics and learning milestones to do it. The sandbox also promotes rapid iteration. When people have a chance to see a project through from end to end and the work is done in small batches and delivers a clear verdict quickly, they benefit from the power of feedback. Each time they fail to move the numbers, they have a real opportunity to act on their ndings immediately. Thus, these teams tend to converge on optimal solutions rapidly even if they start out with really bad ideas.

As we saw earlier, this is a manifestation of the principle of small batches. Functional specialists, especially those steeped in waterfall or stage-gate development, have been trained to work in extremely large batches. This causes even good ideas to get bogged down by waste. By making the batch size small, the sandbox method allows teams to make cheap mistakes quickly and start learning. As we’ll see below, these small initial experiments can demonstrate that a team has a viable new business that can be integrated back into the parent company. Holding Internal Teams Accountable We already discussed learning milestones in detail in Chapter 7. With an internal startup team, the sequence of accountability is the same: build an ideal model of the desired disruption that is based on customer archetypes, launch a minimum viable product to establish a baseline, and then attempt to tune the engine to get it closer to the ideal. Operating in this framework, internal teams essentially act as startups. As they demonstrate success, they need to become integrated into the company’s overall portfolio of products and services. CULTIVATING THE MANAGEMENT PORTFOLIO There are four major kinds of work that companies must manage.3 As an internal startup grows, the entrepreneurs who created the original concept must tackle the challenge of scale. As new mainstream customers are acquired and new markets are conquered, the product becomes part of the public face of the company, with important implications for PR, marketing, sales, and business development. In most cases, the product will attract competitors: copycats, fast followers, and imitators of all stripes. Once the market for the new product is well established,

Once the market for the new product is well established, procedures become more routine. To combat the inevitable commoditization of the product in its market, line extensions, incremental upgrades, and new forms of marketing are essential. In this phase, operational excellence takes on a greater role, as an important way to increase margins is to lower costs. This may require a di erent type of manager: one who excels in optimization, delegation, control, and execution. Company stock prices depend on this kind of predictable growth. There is a fourth phase as well, one dominated by operating costs and legacy products. This is the domain of outsourcing, automation, and cost reduction. Nonetheless, infrastructure is still missioncritical. Failure of facilities or important infrastructure or the abandonment of loyal customers could derail the whole company. However, unlike the growth and optimization phase, investments in this area will not help the company achieve top-line growth. Managers of this kind of organization su er the fate of baseball umpires: criticized when something goes wrong, unappreciated when things are going well. We tend to speak of these four phases of businesses from the perspective of large companies, in which they may represent entire divisions and hundreds or even thousands of people. That’s logical, as the evolution of the business in these kinds of extreme cases is the easiest to observe. However, all companies engage in all four phases of work all the time. As soon as a product hits the marketplace, teams of people work hard to advance it to the next phase. Every successful product or feature began life in research and development (R&D), eventually became a part of the company’s strategy, was subject to optimization, and in time became old news. The problem for startups and large companies alike is that employees often follow the products they develop as they move from phase to phase. A common practice is for the inventor of a new product or feature to manage the subsequent resources, team, or division that ultimately commercializes it. As a result, strong creative managers wind up getting stuck working on the growth and optimization of products rather than creating new ones. This tendency is one of the reasons established companies

struggle to nd creative managers to foster innovation in the rst place. Every new innovation competes for resources with established projects, and one of the scarcest resources is talent. Entrepreneur Is a Job Title The way out of this dilemma is to manage the four kinds of work di erently, allowing strong cross-functional teams to develop around each area. When products move from phase to phase, they are handed o between teams. Employees can choose to move with the product as part of the hando or stay behind and begin work on something new. Neither choice is necessarily right or wrong; it depends on the temperament and skills of the person in question. Some people are natural inventors who prefer to work without the pressure and expectations of the later business phases. Others are ambitious and see innovation as a path toward senior management. Still others are particularly skilled at the management of running an established business, outsourcing, and bolstering e ciencies and wringing out cost reductions. People should be allowed to find the kinds of jobs that suit them best. In fact, entrepreneurship should be considered a viable career path for innovators inside large organizations. Managers who can lead teams by using the Lean Startup methodology should not have to leave the company to reap the rewards of their skills or have to pretend to t into the rigid hierarchies of established functional departments. Instead, they should have a business card that says simply “Entrepreneur” under the name. They should be held accountable via the system of innovation accounting and promoted and rewarded accordingly. After an entrepreneur has incubated a product in the innovation sandbox, it has to be reintegrated into the parent organization. A larger team eventually will be needed to grow it, commercialize it, and scale it. At rst, this team will require the continued leadership of the innovators who worked in the sandbox. In fact, this is a positive part of the process in that it gives the innovators a chance

to train new team members in the new style of working that they mastered in the original sandbox. Ideally, the sandbox will grow over time; that is, rather than move the team out of the sandbox and into the company’s standard routines, there may be opportunities to enlarge the scope of the sandbox. For example, if only certain aspects of the product were subject to experimentation in the sandbox, new features can be added. In the online service described earlier, this could be accomplished by starting with a sandbox that encompassed the product pricing page. When those experiments succeeded, the company could add the website’s home page to the sandbox. It subsequently might add the search functionality or the overall web design. If only certain customers or certain numbers of customers were targeted initially, the product’s reach could be increased. When such changes are contemplated, it’s important that senior management consider whether the teams working in the sandbox can fend for themselves politically in the parent organization. The sandbox was designed to protect them and the parent organization, and any expansion needs to take this into account. Working in the innovation sandbox is like developing startup muscles. At rst, the team will be able to take on only modest experiments. The earliest experiments may fail to produce much learning and may not lead to scalable success. Over time, those teams are almost guaranteed to improve as long as they get the constant feedback of small-batch development and actionable metrics and are held accountable to learning milestones. Of course, any innovation system eventually will become the victim of its own success. As the sandbox expands and the company’s revenue grows as a result of the sandbox’s innovations, the cycle will have to begin again. The former innovators will become guardians of the status quo. When the product makes up the whole sandbox, it inevitably will become encumbered with the additional rules and controls needed for mission-critical operation. New innovation teams will need a new sandbox within which to play.

Becoming the Status Quo This last transition is especially hard for innovators to accept: their transformation from radical outsiders to the embodiment of the status quo. I have found it disturbing in my career. As you can guess from the techniques I advocate as part of the Lean Startup, I have always been a bit of a troublemaker at the companies at which I have worked, pushing for rapid iteration, data-driven decision making, and early customer involvement. When these ideas were not part of the dominant culture, it was simple (if frustrating) to be an advocate. All I had to do was push as hard as humanly possible for my ideas. Since the dominant culture found them heretical, they would compromise with me a “reasonable” amount. Thanks to the psychological phenomenon of anchoring, this led to a perverse incentive: the more radical my suggestion was, the more likely it was that the reasonable compromise would be closer to my true goal. Fast-forward several years to when I was running product development. When we’d hire new people, they had to be indoctrinated into the Lean Startup culture. Split testing, continuous deployment, and customer testing were all standard practice. I needed to continue to be a strong advocate for my ideas, making sure each new employee was ready to give them a try. But for the people who had been working there awhile, those ideas had become part of the status quo. Like many entrepreneurs, I was caught between constant evangelizing for my ideas and constantly entertaining suggestions for ways they could be improved. My employees faced the same incentive I had exploited years before: the more radical the suggestion is, the more likely it is that the compromise will move in the direction they desire. I heard it all: suggestions that we go back to waterfall development, use more quality assurance (QA), use less QA, have more or less customer involvement, use more vision and less data, or interpret data in a more statistically rigorous way.

It took a constant e ort to consider these suggestions seriously. However, responding dogmatically is unhelpful. Compromising by automatically splitting the difference doesn’t work either. I’ve found that every suggestion should be subjected to the same rigorous scienti c inquiry that led to the creation of the Lean Startup in the rst place. Can we use the theory to predict the results of the proposed change? Can we incubate the change in a small team and see what happens? Can we measure its impact? Whenever they could be implemented, these approaches have allowed me to increase my own learning and, more important, the productivity of the companies I have worked with. Many of the Lean Startup techniques that we pioneered at IMVU are not my original contributions. Rather, they were conceived, incubated, and executed by employees who brought their own creativity and talent to the task. Above all, I faced this common question: How do we know that “your way” of building a company will work? What other companies are using it? Who has become rich and famous as a result? These questions are sensible. The titans of our industry are all working in a slower, more linear way. Why are we doing something different? It is these questions that require the use of theory to answer. Those who look to adopt the Lean Startup as a de ned set of steps or tactics will not succeed. I had to learn this the hard way. In a startup situation, things constantly go wrong. When that happens, we face the age-old dilemma summarized by Deming: How do we know that the problem is due to a special cause versus a systemic cause? If we’re in the middle of adopting a new way of working, the temptation will always be to blame the new system for the problems that arise. Sometimes that tendency is correct, sometimes not. Learning to tell the di erence requires theory. You have to be able to predict the outcome of the changes you make to tell if the problems that result are really problems. For example, changing the de nition of productivity for a team from functional excellence—excellence in marketing, sales, or product development—to validated learning will cause problems.

As was indicated earlier, functional specialists are accustomed to measuring their e ciency by looking at the proportion of time they are busy doing their work. A programmer expects to be coding all day long, for example. That is why many traditional work environments frustrate these experts: the constant interruption of meetings, cross-functional hando s, and explanations for endless numbers of bosses all act as a drag on e ciency. However, the individual e ciency of these specialists is not the goal in a Lean Startup. Instead, we want to force teams to work cross-functionally to achieve validated learning. Many of the techniques for doing this —actionable metrics, continuous deployment, and the overall BuildMeasure-Learn feedback loop—necessarily cause teams to suboptimize for their individual functions. It does not matter how fast we can build. It does not matter how fast we can measure. What matters is how fast we can get through the entire loop. In my years teaching this system, I have noticed this pattern every time: switching to validated learning feels worse before it feels better. That’s the case because the problems caused by the old system tend to be intangible, whereas the problems of the new system are all too tangible. Having the bene t of theory is the antidote to these challenges. If it is known that this loss of productivity is an inevitable part of the transition, it can be managed actively. Expectations can be set up front. In my consulting practice, for example, I have learned to raise these issues from day one; otherwise, they are liable to derail the whole e ort once it is under way. As the change progresses, we can use the root cause analysis and fast response techniques to gure out which problems need prevention. Ultimately, the Lean Startup is a framework, not a blueprint of steps to follow. It is designed to be adapted to the conditions of each speci c company. Rather than copy what others have done, techniques such as the Five Whys allow you to build something that is perfectly suited to your company. The best way to achieve mastery of and explore these ideas is to embed oneself in a community of practice. There is a thriving community of Lean Startup meetups around the world as well as

online, and suggestions for how you can take advantage of these resources listed in the last chapter of this book, “Join the Movement.”

13 EPILOGUE: WASTE NOT year marks the one hundredth anniversary of Frederick ThisWinslow Taylor’s The Principles of Scienti c Management, rst published in 1911. The movement for scienti c management

changed the course of the twentieth century by making possible the tremendous prosperity that we take for granted today. Taylor e ectively invented what we now consider simply management: improving the e ciency of individual workers, management by exception (focusing only on unexpectedly good or bad results), standardizing work into tasks, the task-plus-bonus system of compensation, and—above all—the idea that work can be studied and improved through conscious e ort. Taylor invented modern white-collar work that sees companies as systems that must be managed at more than the level of the individual. There is a reason all past management revolutions have been led by engineers: management is human systems engineering. In 1911 Taylor wrote: “In the past, the man has been rst; in the future, the system must be rst.” Taylor’s prediction has come to pass. We are living in the world he imagined. And yet, the revolution that he unleashed has been—in many ways—too successful. Whereas Taylor preached science as a way of thinking, many people confused his message with the rigid techniques he advocated: time and motion studies, the di erential piece-rate system, and—most galling of all—the idea that workers should be treated as little more than automatons. Many of these ideas proved extremely harmful and required the e orts of later theorists and

extremely harmful and required the e orts of later theorists and managers to undo. Critically, lean manufacturing rediscovered the wisdom and initiative hidden in every factory worker and redirected Taylor’s notion of e ciency away from the individual task and toward the corporate organism as a whole. But each of these subsequent revolutions has embraced Taylor’s core idea that work can be studied scienti cally and can be improved through a rigorous experimental approach. In the twenty- rst century, we face a new set of problems that Taylor could not have imagined. Our productive capacity greatly exceeds our ability to know what to build. Although there was a tremendous amount of invention and innovation in the early twentieth century, most of it was devoted to increasing the productivity of workers and machines in order to feed, clothe, and house the world’s population. Although that project is still incomplete, as the millions who live in poverty can attest, the solution to that problem is now strictly a political one. We have the capacity to build almost anything we can imagine. The big question of our time is not Can it be built? but Should it be built? This places us in an unusual historical moment: our future prosperity depends on the quality of our collective imaginations. In 1911, Taylor wrote: We can see our forests vanishing, our water-powers going to waste, our soil being carried by oods into the sea; and the end of our coal and our iron is in sight. But our larger wastes of human e ort, which go on every day through such of our acts as are blundering, ill-directed, or ine cient … are less visible, less tangible, and are but vaguely appreciated. We can see and feel the waste of material things. Awkward, ine cient, or ill-directed movements of men, however, leave nothing visible or tangible behind them. Their appreciation calls for an act of memory, an e ort of the imagination. And for this reason, even though our daily loss from this source is greater than from our waste of material things, the one has stirred us deeply, while the

other has moved us but little.1 A century on, what can we say about those words? On the one hand, they feel archaic. We of the twenty- rst century are hyperaware of the importance of e ciency and the economic value of productivity gains. Our workplaces are—at least when it comes to the building of material objects—incredibly well organized compared with those of Taylor’s day. On the other hand, Taylor’s words strike me as completely contemporary. For all of our vaunted e ciency in the making of things, our economy is still incredibly wasteful. This waste comes not from the ine cient organization of work but rather from working on the wrong things—and on an industrial scale. As Peter Drucker said, “There is surely nothing quite so useless as doing with great efficiency what should not be done at all.”2 And yet we are doing the wrong things e ciently all the time. It is hard to come by a solid estimate of just how wasteful modern work is, but there is no shortage of anecdotes. In my consulting and travels talking about the Lean Startup, I hear the same message consistently from employees of companies big and small. In every industry we see endless stories of failed launches, ill-conceived projects, and large-batch death spirals. I consider this misuse of people’s time a criminally negligent waste of human creativity and potential. What percentage of all this waste is preventable? I think a much larger proportion than we currently realize. Most people I meet believe that in their industry at least, projects fail for good reasons: projects are inherently risky, market conditions are unpredictable, “big company people” are intrinsically uncreative. Some believe that if we just slowed everything down and used a more careful process, we could reduce the failure rate by doing fewer projects of higher quality. Others believe that certain people have an innate gift of knowing the right thing to build. If we can nd enough of these visionaries and virtuosos, our problems will be solved. These “solutions” were once considered state of the art in the nineteenth century, too, before people knew about modern management.

century, too, before people knew about modern management. The requirements of an ever-faster world make these antique approaches unworkable, and so the blame for failed projects and businesses often is heaped on senior management, which is asked to do the impossible. Alternatively, the nger of blame is pointed at nancial investors or the public markets for overemphasizing quick xes and short-term results. We have plenty of blame to go around, but far too little theory to guide the actions of leaders and investors alike. The Lean Startup movement stands in contrast to this handwringing. We believe that most forms of waste in innovation are preventable once their causes are understood. All that is required is that we change our collective mind-set concerning how this work is to be done. It is insu cient to exhort workers to try harder. Our current problems are caused by trying too hard—at the wrong things. By focusing on functional e ciency, we lose sight of the real goal of innovation: to learn that which is currently unknown. As Deming taught, what matters is not setting quantitative goals but xing the method by which those goals are attained. The Lean Startup movement stands for the principle that the scienti c method can be brought to bear to answer the most pressing innovation question: How can we build a sustainable organization around a new set of products or services? ORGANIZATIONAL SUPERPOWERS A participant at one of my workshops came up to me a few months afterward to relate the following story, which I am paraphrasing: “Knowing Lean Startup principles makes me feel like I have superpowers. Even though I’m just a junior employee, when I meet with corporate VPs and GMs in my large company, I ask them simple questions and very quickly help them see how their projects are based on fundamental hypotheses that are testable. In minutes, I can lay out a plan they could follow to scienti cally validate their plans before it’s too late. They consistently respond with ‘Wow, you

are brilliant. We’ve never thought to apply that level of rigor to our thinking about new products before.’ ” As a result of these interactions, he has developed a reputation within his large company as a brilliant employee. This has been good for his career but very frustrating for him personally. Why? Because although he is quite brilliant, his insights into awed product plans are due not to his special intelligence but to having a theory that allows him to predict what will happen and propose alternatives. He is frustrated because the managers he is pitching his ideas to do not see the system. They wrongly conclude that the key to success is nding brilliant people like him to put on their teams. They are failing to see the opportunity he is really presenting them: to achieve better results systematically by changing their beliefs about how innovation happens. Putting the System First: Some Dangers Like Taylor before us, our challenge is to persuade the managers of modern corporations to put the system rst. However, Taylorism should act as a cautionary tale, and it is important to learn the lessons of history as we bring these new ideas to a more mainstream audience. Taylor is remembered for his focus on systematic practice rather than individual brilliance. Here is the full quote from The Principles of Scienti c Management that includes the famous line about putting the system first: In the future it will be appreciated that our leaders must be trained right as well as born right, and that no great man can (with the old system of personal management) hope to compete with a number of ordinary men who have been properly organized so as efficiently to cooperate. In the past the man has been first; in the future the system must be rst. This in no sense, however, implies that great men are not needed. On the contrary, the rst object of any

good system must be that of developing rst-class men; and under systematic management the best man rises to the top more certainly and more rapidly than ever before.3 Unfortunately, Taylor’s insistence that scientific management does not stand in opposition to nding and promoting the best individuals was quickly forgotten. In fact, the productivity gains to be had through the early scienti c management tactics, such as time and motion study, task-plus-bonus, and especially functional foremanship (the forerunner of today’s functional departments), were so signi cant that subsequent generations of managers lost sight of the importance of the people who were implementing them. This has led to two problems: (1) business systems became overly rigid and thereby failed to take advantage of the adaptability, creativity, and wisdom of individual workers, and (2) there has been an overemphasis on planning, prevention, and procedure, which enable organizations to achieve consistent results in a mostly static world. On the factory oor, these problems have been tackled head on by the lean manufacturing movement, and those lessons have spread throughout many modern corporations. And yet in new product development, entrepreneurship, and innovation work in general we are still using an outdated framework. My hope is that the Lean Startup movement will not fall into the same reductionist trap. We are just beginning to uncover the rules that govern entrepreneurship, a method that can improve the odds of startup success, and a systematic approach to building new and innovative products. This in no way diminishes the traditional entrepreneurial virtues: the primacy of vision, the willingness to take bold risks, and the courage required in the face of overwhelming odds. Our society needs the creativity and vision of entrepreneurs more than ever. In fact, it is precisely because these are such precious resources that we cannot afford to waste them. Product Development Pseudoscience

I believe that if Taylor were alive today, he would chuckle at what constitutes the management of entrepreneurs and innovators. Although we harness the labor of scientists and engineers who would have dazzled any early-twentieth-century person with their feats of technical wizardry, the management practices we use to organize them are generally devoid of scienti c rigor. In fact, I would go so far as to call them pseudoscience. We routinely green-light new projects more on the basis of intuition than facts. As we’ve seen throughout this book, that is not the root cause of the problem. All innovation begins with vision. It’s what happens next that is critical. As we’ve seen, too many innovation teams engage in success theater, selectively nding data that support their vision rather than exposing the elements of the vision to true experiments, or, even worse, staying in stealth mode to create a data-free zone for unlimited “experimentation” that is devoid of customer feedback or external accountability of any kind. Anytime a team attempts to demonstrate cause and e ect by placing highlights on a graph of gross metrics, it is engaging in pseudoscience. How do we know that the proposed cause and e ect is true? Anytime a team attempts to justify its failures by resorting to learning as an excuse, it is engaged in pseudoscience as well. If learning has taken place in one iteration cycle, let us demonstrate it by turning it into validated learning in the next cycle. Only by building a model of customer behavior and then showing our ability to use our product or service to change it over time can we establish real facts about the validity of our vision. Throughout our celebration of the success of the Lean Startup movement, a note of caution is essential. We cannot a ord to have our success breed a new pseudoscience around pivots, MVPs, and the like. This was the fate of scienti c management, and in the end, I believe, that set back its cause by decades. Science came to stand for the victory of routine work over creative work, mechanization over humanity, and plans over agility. Later movements had to be spawned to correct those deficiencies.

spawned to correct those deficiencies. Taylor believed in many things that he dubbed scienti c but that our modern eyes perceive as mere prejudice. He believed in the inherent superiority in both intelligence and character of aristocratic men over the working classes and the superiority of men over women; he also thought that lower-status people should be supervised strictly by their betters. These beliefs are part and parcel of Taylor’s time, and it is tempting to forgive him for having been blind to them. Yet when our time is viewed through the lens of future practice, what prejudices will be revealed? In what forces do we place undue faith? What might we risk losing sight of with this initial success of our movement? It is with these questions that I wish to close. As gratifying as it is for me to see the Lean Startup movement gain fame and recognition, it is far more important that we be right in our prescriptions. What is known so far is just the tip of the iceberg. What is needed is a massive project to discover how to unlock the vast stores of potential that are hidden in plain sight in our modern workforce. If we stopped wasting people’s time, what would they do with it? We have no real concept of what is possible. Starting in the late 1880s, Taylor began a program of experimentation to discover the optimal way to cut steel. In the course of that research, which lasted more than twenty- ve years, he and his colleagues performed more than twenty thousand individual experiments. What is remarkable about this project is that it had no academic backing, no government R&D budget. Its entire cost was paid by industry out of the immediate pro ts generated from the higher productivity the experiments enabled. This was only one experimental program to uncover the hidden productivity in just one kind of work. Other scienti c management disciples spent years investigating bricklaying, farming, and even shoveling. They were obsessed with learning the truth and were not satis ed with the folk wisdom of craftspersons or the parables of experts. Can any of us imagine a modern knowledge-work manager with the same level of interest in the methods his or her employees use?

How much of our current innovation work is guided by catchphrases that lack a scientific foundation? A New Research Program What comparable research programs could we be engaged in to discover how to work more effectively? For one thing, we have very little understanding of what stimulates productivity under conditions of extreme uncertainty. Luckily, with cycle times falling everywhere, we have many opportunities to test new approaches. Thus, I propose that we create startup testing labs that could put all manner of product development methodologies to the test. How might those tests be conducted? We could bring in small cross-functional teams, perhaps beginning with product and engineering, and have them work to solve problems by using di erent development methodologies. We could begin with problems with clear right answers, perhaps drawn from the many international programming competitions that have developed databases of well-de ned problems with clear solutions. These competitions also provide a clear baseline of how long it should take for various problems to be solved so that we could establish clearly the individual problem-solving prowess of the experimental subjects. Using this kind of setup for calibration, we could begin to vary the conditions of the experiments. The challenge will be to increase the level of uncertainty about what the right answer is while still being able to measure the quality of the outcome objectively. Perhaps we could use real-world customer problems and then have real consumers test the output of the teams’ work. Or perhaps we could go so far as to build minimum viable products for solving the same set of problems over and over again to quantify which produces the best customer conversion rates. We also could vary the all-important cycle time by choosing more or less complex development platforms and distribution channels to

test the impact of those factors on the true productivity of the teams. Most of all, we need to develop clear methods for holding teams accountable for validated learning. I have proposed one method in this book: innovation accounting using a well-de ned financial model and engine of growth. However, it is naive to assume that this is the best possible method. As it is adopted in more and more companies, undoubtedly new techniques will be suggested, and we need to be able to evaluate the new ideas as rigorously as possible. All these questions raise the possibilities of public-private partnerships between research universities and the entrepreneurial communities they seek to foster. It also suggests that universities may be able to add value in more ways than by being simply nancial investors or creators of startup incubators, as is the current trend. My prediction is that wherever this research is conducted will become an epicenter of new entrepreneurial practice, and universities conducting this research therefore may be able to achieve a much higher level of commercialization of their basic research activities.4 THE LONG-TERM STOCK EXCHANGE Beyond simple research, I believe our goal should be to change the entire ecosystem of entrepreneurship. Too much of our startup industry has devolved into a feeder system for giant media companies and investment banks. Part of the reason established companies struggle to invest consistently in innovation is intense pressure from public markets to hit short-term pro tability and growth targets. Mostly, this is a consequence of the accounting methods we have developed for evaluating managers, which focus on the kinds of gross “vanity” metrics discussed in Chapter 7. What is needed is a new kind of stock exchange, designed to trade in the stocks of companies that are organized to sustain long-term thinking. I propose that we create a Long-Term Stock Exchange (LTSE).

In addition to quarterly reports on pro ts and margins, companies on the LTSE would report using innovation accounting on their internal entrepreneurship e orts. Like Intuit, they would report on the revenue they were generating from products that did not exist a few years earlier. Executive compensation in LTSE companies would be tied to the company’s long-term performance. Trading on the LTSE would have much higher transaction costs and fees to minimize day trading and massive price swings. In exchange, LTSE companies would be allowed to structure their corporate governance to facilitate greater freedom for management to pursue long-term investments. In addition to support for long-term thinking, the transparency of the LTSE will provide valuable data about how to nurture innovation in the real world. Something like the LTSE would accelerate the creation of the next generation of great companies, built from the ground up for continuous innovation. IN CONCLUSION As a movement, the Lean Startup must avoid doctrines and rigid ideology. We must avoid the caricature that science means formula or a lack of humanity in work. In fact, science is one of humanity’s most creative pursuits. I believe that applying it to entrepreneurship will unlock a vast storehouse of human potential. What would an organization look like if all of its employees were armed with Lean Startup organizational superpowers? For one thing, everyone would insist that assumptions be stated explicitly and tested rigorously not as a stalling tactic or a form of make-work but out of a genuine desire to discover the truth that underlies every project’s vision. We would not waste time on endless arguments between the defenders of quality and the cowboys of reckless advance; instead, we would recognize that speed and quality are allies in the pursuit of the customer’s long-term bene t. We would race to test our vision but not to abandon it. We would look to eliminate waste not

vision but not to abandon it. We would look to eliminate waste not to build quality castles in the sky but in the service of agility and breakthrough business results. We would respond to failures and setbacks with honesty and learning, not with recriminations and blame. More than that, we would shun the impulse to slow down, increase batch size, and indulge in the curse of prevention. Instead, we would achieve speed by bypassing the excess work that does not lead to learning. We would dedicate ourselves to the creation of new institutions with a long-term mission to build sustainable value and change the world for the better. Most of all, we would stop wasting people’s time.


Introduction: Why Products Fail and How Lean Changes the Game Building great products is hard. We’re all familiar with the sobering statistics about the high percentage of new products that fail. For every Apple, Google, Facebook, and other success story you hear, there are countless failed products causing companies to shutter their doors. Think of all the products you’ve used in the last year. How many of those products do you love? How many do you hate? How many can you even remember? If you’re like most people, you actually love a very small number of the products you use. If you’ve been on a team that has built a product that customers love, you know how great that feels. Passionate users can’t stop raving about your product. Your business metrics are growing up and to the right exponentially. You’re struggling to keep up with the high demand. Customers want and value your product. But the unfortunate reality is that very few products are like that. Why is it so hard to build a product that customers love? Why do so many products fail? WHY PRODUCTS FAIL

Throughout my career, I’ve worked on and studied many different products. When I analyze the root causes of why products fail, a common pattern emerges. The main reason products fail is because they don’t meet customer needs in a way that is better than other alternatives. This is the essence of product-market fit. Marc Andreessen of Netscape fame coined the term in 2007. In the same blog post he also contends, as I do, that startups “fail because they never get to product-market fit.” The Lean Startup movement begun by Eric Ries has helped popularize the idea of product-market fit and the importance of achieving


xviii

Introduction: Why Products Fail and How Lean Changes the Game

it. One reason Lean Startup has such wide appeal is because people know how difficult it is to build successful products. I am a strong advocate of Lean Startup principles. Many people get excited when they first hear Lean Startup ideas and are eager to try them out. However, I’ve spoken with many of these enthusiasts who struggle to figure out exactly what they should be doing. They understand the high-level concepts, but don’t know how to apply them. This reminds me of many people who decide they want to get in better physical shape. They are highly motivated to start working out more. They join a gym. They buy new workout clothes. They show up to the gym raring to go—but realize that they have no idea what to do when they get there. What exercises should I be doing? What equipment should I be using? What’s the right way to work out? They have plenty of motivation, but lack the specific knowledge about what exactly to do. WHY THIS BOOK?

I wrote The Lean Product Playbook to fill the knowledge gaps faced by many people who want to create a product using Lean Startup principles. This book provides clear, step-by-step guidance to help you build successful products. In working with so many product teams, I have witnessed the various challenges they faced and seen numerous examples of what worked well and what didn’t. Over the course of this experience, I developed a framework and process for how to achieve product-market fit. The Product-Market Fit Pyramid

The framework, which I call the Product-Market Fit Pyramid, breaks product-market fit down into five key components: your target customer, your customer’s underserved needs, your value proposition, your feature set, and your user experience (UX). Each of these is actually a testable hypothesis. There is a logical sequence to the five hypotheses based on how they relate to each other, resulting in the hierarchy shown in the pyramid (see Figure I.1).

Introduction: Why Products Fail and How Lean Changes the Game

FIGURE I.1

The Product-Market Fit Pyramid

The Lean Product Process

After developing the Product-Market Fit Pyramid, I designed a simple, iterative process to take advantage of it, called the Lean Product Process. This process guides you through each layer of the pyramid from the bottom up. It helps you articulate and test your key hypotheses for each of the five key components of product-market fit. The Lean Product Process consists of six steps: 1. Determine your target customers 2. Identify underserved customer needs 3. Define your value proposition 4. Specify your minimum viable product (MVP) feature set 5. Create your MVP prototype 6. Test your MVP with customers This book describes each step of the process in detail with relevant real-world examples. I also devote a chapter to share an in-depth, end-to-end case study of the process being applied.



xx

Introduction: Why Products Fail and How Lean Changes the Game

A Comprehensive Guide

I wrote this book as a comprehensive guide because you have to get so many things right to build a great product. I cover a range of important topics in addition to the Lean Product Process. The book walks you through detailed explanations of UX design and Agile development. It also provides in-depth coverage of analytics and how to use metrics to optimize your product. The Lean Product Process and the rest of the advice in this book come from hands-on experience and lessons learned throughout my career of building high-tech products—both successes and failures. About Me

My background is a mix of technical and business skills that I began to develop when my parents gave me my first computer at the age of 10. I started my first business a few years later. I studied electrical engineering at Northwestern University and then started my high-tech career designing nuclear-powered submarines. While working, I earned a Master’s degree in industrial engineering from Virginia Tech at night, where I learned about the Lean manufacturing principles that inspired the Lean Startup movement. I moved to Silicon Valley to attend Stanford Business School and then joined Intuit, which provided an incredible post-MBA training ground in product management, product development, customer research, user experience design, and marketing. I led and grew the Quicken product team to record sales and profit. As I learned more, I had a growing desire to take what I had learned and apply it at startups. Since leaving Intuit, I’ve spent a lot of time working at and with startups. For years now, I have consulted to numerous companies, helping them apply Lean principles to create successful products. I take a hands-on approach in my consulting: I work closely with CEOs and their management teams and also get in the trenches with product managers, designers, and developers. I usually serve as interim VP of Product for my clients and am often the first product person on their team. I’ve tested and refined the advice in this book while working with a wide range of companies. My client list includes Facebook, Box,

Introduction: Why Products Fail and How Lean Changes the Game

YouSendIt (now Hightail), Microsoft, Epocrates, Medallia, Chartboost, XING, Financial Engines, and One Medical Group. I’ve found these ideas applicable to all my clients, even though they vary in size from small early stage startups to large public companies and span a variety of vertical industries, target customers, product types, and business models. I enjoy sharing and discussing my Lean Product ideas with as many people as I can. I regularly give talks and workshops and post my slides on SlideShare at http://slideshare.net/dan_o/presentations. I also host a monthly Lean Product meetup in Silicon Valley, which I invite you to check out at http://meetup.com/lean-product. The audiences in those forums have also helped me hone the guidance provided in this book with their questions, suggestions, and feedback. WHO IS THIS BOOK FOR?

If you are interested in Lean Startup, Customer Development, Lean UX, Design Thinking, product management, user experience design, Agile development, or analytics, then this book is for you. It will equip you with the “how-to” manual you need, and provide a step-by-step process you can follow to ensure you’re building a product that customers will find valuable. This book is for: Anyone trying to build a new product or service ● Anyone trying to improve their existing product or service ● Entrepreneurs ● Product managers, designers, and developers ● Marketers, analysts, and program managers ● CEOs and other executives ● People working in companies of any size ● Anyone who is passionate about building great products ●

The guidance in this book is most valuable for software products. However, it is also relevant to other product categories such as hardware and wearables, and even nontechnical products. The guidance in this book is also applicable to a wide range of business contexts, including business-to-consumer (B2C) and business-to-business (B2B).



xxii

Introduction: Why Products Fail and How Lean Changes the Game

HOW THIS BOOK IS ORGANIZED

The book is organized in three parts. Part I, “Core Concepts,” explains the foundational ideas of product-market fit and problem space versus solution space. Part II of this book, “The Lean Product Process,” describes each of the six steps of the process in detail, devoting a chapter to each step. Part II also includes chapters on: The principles of great UX design How to iteratively improve your product-market fit ● A detailed, end-to-end case study using the Lean Product Process ● ●

Part III, “Building and Optimizing Your Product,” provides guidance that applies after you have validated product-market fit with your MVP prototype. It includes a chapter on how to build your product using Agile development, which also covers testing, continuous integration, and continuous deployment. In addition, it contains two chapters on analytics, which describe a methodology for using metrics to optimize your product and include another in-depth, real-world case study. Writing this book has given me the opportunity to share the ideas, lessons learned, and advice accumulated over my career with a broader audience. My experience has been informed and influenced by my mentors, colleagues, and many other people passionate about sharing ideas and comparing notes on the discipline of building great products. Our field continues to evolve, with new ideas emerging all the time. That’s why I’ll use the companion website for this book, http://leanproductplaybook.com, as a place to share and discuss those new ideas. I invite you to visit the website to read the latest information and contribute to the conversation.

Part I

Core Concepts

Chapter 1

Achieving Product-Market Fit with the Lean Product Process Product-market fit is a wonderful term because it captures the essence of what it means to build a great product. The concept nicely encapsulates all the factors that are critical to achieving product success. Product-market fit is one of the most important Lean Startup ideas, and this playbook will show you how to achieve it. Given the number of people who have written about productmarket fit, you can find a range of interpretations. Real-world examples are a great way to help explain such concepts—throughout this book, I walk through many examples of products that did or didn’t achieve product-market fit. But let’s start out by clarifying what product-market fit means. WHAT IS PRODUCT-MARKET FIT?

As I mention in the introduction, Marc Andreessen coined the term product-market fit in a well-known blog post titled “The only thing that matters.” In that post he writes, “Product-market fit means being in a good market with a product that can satisfy that market.” My definition of product-market fit—which is consistent with his—is that you have built a product that creates significant customer value. This means that your product meets real customer needs and does so in a way that is better than the alternatives. Some people interpret product-market fit much more broadly, going beyond the core definition to also include having a validated revenue model—that is, that you can successfully monetize your product. For others, product-market fit also includes having a costeffective customer acquisition model. Such definitions basically equate product-market fit with having a profitable business. I believe using “product-market fit” as another way of saying “profitable”


4

The Lean Product Playbook

glosses over the essential aspects of the idea, which can stand on its own. In this book, I use the core definition above. In business, there is a distinction between creating value and capturing value. In order to capture value, you must first create it. To be clear, topics such as business model, customer acquisition, marketing, and pricing are critical to a successful business. Each is also worthy of its own book. This book touches on those subjects, and you can use the qualitative and quantitative techniques in it to improve those aspects of your business. In fact, Chapters 13 and 14 discuss how to optimize your business metrics, but the majority of this book focuses on the core definition of product-market fit and gives you a playbook for how to achieve it. THE PRODUCT-MARKET FIT PYRAMID

If you’re trying to achieve product-market fit, a definition alone doesn’t give you enough guidance. That’s why I created an actionable framework called the Product-Market Fit Pyramid, shown in Figure 1.1. This hierarchical model decomposes product-market

FIGURE 1.1

The Product-Market Fit Pyramid

Achieving Product-Market Fit with the Lean Product Process

into its five key components, each a layer of the pyramid. Your product is the top section, consisting of three layers. The market is the bottom section of the pyramid, consisting of two layers. Within the product and market sections, each layer depends on the layer immediately beneath it. Product-market fit lies between the top and bottom sections of the pyramid. The Market

Given the pyramid’s hierarchy, let’s start with the bottom section, which is the market. A market consists of all the existing and potential customers that share a particular customer need or set of related needs. For example, all the people in the United States who need to prepare their income taxes are in the U.S. tax preparation market. You can describe the size of a market by the total number of customers in the market or the total revenue generated by those customers. For either of those two measures, you can refer to the current size or the potential future size of the market. Different customers within a market choose different solutions to meet their needs. For example, some customers in the tax preparation market may use a professional service such as H&R Block. Others may choose to prepare their taxes themselves, either by hand or by using software such as TurboTax. Within a given market, you can analyze the market share of each competing product—that is, what percentage of the market each product has. For example, you could compare the smartphone market share of Apple versus Samsung. Or you could segment the smartphone market by operating system (iOS, Android, and so forth). Browsers are another example where the market shares of each different product are closely watched. As you walk down the aisles of a supermarket, you see products in many different market categories: toothpaste, shampoo, laundry detergent, cereal, yogurt, and beer, to name a few. The life cycle stage of a market can vary. Many of the products you see—such as milk, eggs, and bread—are in relatively mature markets, with little innovation or change. That being said, new markets do emerge. For example, Febreze basically created its own market with a new product that eliminates odors from fabrics without washing them.



6

The Lean Product Playbook

Prior to its launch, that market didn’t exist. You also see active competition in many markets, with companies trying to gain market share through product innovation. The Product-Market Fit Pyramid separates the market into its two distinct components: the target customers and their needs. The needs layer is above the target customers layer in the model because it’s their needs that are relevant to achieving product-market fit. As you try to create value for customers, you want to identify the specific needs that correspond to a good market opportunity. For example, you probably don’t want to enter a market where customers are extremely happy with how the existing solutions meet their needs. When you develop a new product or improve an existing product, you want to address customer needs that aren’t adequately met. That’s why I use “underserved needs” as the label for this layer. Customers are going to judge your product in relation to the alternatives. So the relative degree to which your product meets their needs depends on the competitive landscape. Let’s move now to the product section of the pyramid. Your Product

A product is a specific offering intended to meet a set of customer needs. From this definition, it’s clear that the concept of product-market fit applies to services as well as products. The typical distinction between a product and service is that a product is a physical good while a service is intangible. However, with products delivered via the web and mobile devices, the distinction between product and service has been blurred, as indicated by the popular term software as a service (SaaS). For software, the product itself is intangible code, often running on servers that the customer never sees. The real-world manifestation of software products that customers see and use is the user experience (UX), which is the top layer of the Product-Market Fit Pyramid. Beyond software, this is also true for any product with which the customer interacts. The UX is what brings a product’s functionality to life for the user. The functionality that a product provides consists of multiple features, each built to meet a customer need. Taken together, they

Achieving Product-Market Fit with the Lean Product Process

form the product’s feature set, which is the layer just below the UX layer. To decide which features to build, you need to identify the specific customer needs your product should address. In doing so, you want to determine how your product will be better than the others in the market. This is the essence of product strategy. The set of needs that you aspire to meet with your product forms your value proposition, which is the layer just below “feature set” in the Product-Market Fit Pyramid. Your value proposition is also the layer just above customer needs, and fundamentally determines how well the needs addressed by your product match up with the customer’s. Taken together, the three layers of value proposition, feature set, and UX define your product. As shown in Figure 1.1, your product and the market are separate sections of the Product-Market Fit Pyramid. Your goal in creating customer value is to make them fit nicely together. Product-Market Fit

Viewing product-market fit in light of this model, it is the measure of how well your product (the top three layers of the pyramid) satisfies the market (the bottom two layers of the pyramid). Your target customers determine how well your product fits their needs. Again, customers will judge your product’s fit in relation to the other products in the market. To achieve product-market fit, your product should meet underserved needs better than the competition. Let’s discuss a product that managed to do that. QUICKEN: FROM #47 TO #1

A great example of a product that achieved product-market fit while entering an already crowded market is Intuit’s Quicken personal finance software. Scott Cook and Tom Proulx practiced Lean principles even though they founded Intuit years before Lean Startup ideas were put forth. When they launched Quicken, there were already 46 personal finance products in the market. However, after conducting customer research, the cofounders concluded that none of the existing products had achieved product-market fit.



8

The Lean Product Playbook

The products didn’t meet customer needs and were difficult to use. The cofounders had a hypothesis that a checkbook-based design would do well, since everyone at the time was familiar with writing checks. Their hypothesis proved right: the UX they built using the checkbook conceptual design resonated with customers and Quicken rapidly became the leading personal finance software. A large part of Quicken’s success was the fact that Intuit adopted principles that would be called Lean today. The company pioneered the use of customer research and user testing to inform software development. They routinely conducted usability testing of each version before launching it and organized public betas years before those ideas became mainstream. They invented the “follow me home” concept, where Intuit employees would go to retail stores, wait for customers to buy a copy of Quicken, and then ask to follow them home to see how they used the software. This helped immensely in understanding the customer’s initial impressions of the product. Let’s assess Quicken using the Product-Market Fit Pyramid. There were many customers in its market, and the product definitely addressed real customer needs: People needed help balancing their checkbook, tracking their balances, and seeing where their money was going. Computer software was well suited to help on that front, but despite 46 products in the market, customer needs were still underserved. By talking with customers, the cofounders ensured Quicken’s feature set addressed those needs. Their design insights led to an innovative UX that customers found much easier to use. This dramatic improvement in ease of use was, in fact, the main differentiator in Quicken’s value proposition. By achieving product-market fit, Quicken succeeded in the face of stiff competition, which led the founders to joke about having “47th mover advantage.” THE LEAN PRODUCT PROCESS

Now that we have a detailed model for product-market fit, how do we go about achieving it? Based on my experience using the Product-Market Fit Pyramid with many teams on numerous products, I designed a simple, iterative process for achieving product-market fit. The Lean Product Process, shown in Figure 1.2, guides you through each layer of the pyramid from the bottom up.

Achieving Product-Market Fit with the Lean Product Process

6. Test with Customers 5. UX

Product:

4. Feature Set

3. Value Proposition

Product-Market Fit 2. Underserved Needs

Market: 1. Target Customer

FIGURE 1.2

The Lean Product Process

It helps you articulate and test your key hypotheses for each of the five components of product-market fit. I describe the six steps of the Lean Product Process in detail in Part II of this book, with a chapter devoted to each one: 1. Determine your target customers 2. Identify underserved customer needs 3. Define your value proposition 4. Specify your minimum viable product (MVP) feature set 5. Create your MVP prototype 6. Test your MVP with customers The last three steps reference the important Lean concept of a minimum viable product (MVP). I discuss MVP in detail in Chapters 6 and 7, but it’s basically the minimum amount of functionality that your target customer considers viable, that is, providing enough value. When you are building a new product, you want to avoid building more than is required to test your hypotheses with customers. The term MVP clearly applies when you’re building a completely



10

The Lean Product Playbook

new version 1 product (v1 for short). In addition, the idea of an MVP makes sense if you are redesigning an existing product or building v2. The Lean Product Process also applies when you are not building a whole product, such as when you add functionality to or improve an existing product. In those situations, you can think of the process steps applying to a “minimum viable feature” instead, if that’s clearer. Step 5 also refers to your MVP prototype. I intentionally use this broad term to capture the wide range of product-related artifacts you can test with customers. While the first “prototype” you test could be your live product, you can gain faster learning with fewer resources by testing your hypotheses before you build your product. Not all six steps are required for every product or feature. Certain steps are required only when you’re building a completely new product. Take, for example, determining your target customers, identifying underserved needs, and defining your value proposition. Once you’ve successfully completed those steps for your product, you may not need to revisit those areas for a while. But after launching your v1 product, you would continue to improve and add functionality by looping through the three remaining steps: specifying which features to pursue, creating the features, and testing the features with customers. To increase your chances of achieving product-market fit, the process is designed to encourage a certain amount of rigor in product thinking. In a sense, the process is a checklist to help make sure you’ve thought about the key assumptions and decisions to be made when creating a product. If you are not making these assumptions or decisions explicitly, then you are making them implicitly. The Lean Product Process helps you articulate the assumptions and hypotheses in your head (which you can revise later as you iterate). If you skip these critical thinking steps, you leave important elements—such as target customer and product strategy—to chance. A key concept in Lean manufacturing, which inspired Lean Startup, is the concept of rework: having to spend time fixing something that you did not build correctly the first time. Minimizing rework is a key tactic for eliminating waste. In addition to helping you achieve product-market fit, the Lean Product Process also enables you to do so more quickly by reducing rework.

Achieving Product-Market Fit with the Lean Product Process

To be clear, you will have some rework with the Lean Product Process. It is an iterative process that requires you to revise your hypotheses, designs, and product as you make progress—all of which could be considered rework. The goal of the process is to achieve product-market fit as quickly as possible. Quick but rigorous thinking that avoids or reduces rework helps achieve that goal. You can think of the Lean Product Process like the drills that karate students learn and practice as they make progress earning higher and higher belts. After mastering the core techniques from their drills and becoming black belts, students are able to mix, match, and modify what they have learned to create their own custom style. Martial arts master Bruce Lee eloquently said, “Obey the principles without being bound by them.” He also said, “Adapt what is useful, reject what is useless, and add what is specifically your own.” I encourage you to heed his advice as you read and practice the ideas and guidance in this book. Along those lines, I would enjoy hearing any questions or feedback you have, as well as your experiences applying the ideas in this book. Please feel free to share them at the companion website for this book: http://leanproductplaybook.com. There, you can also see the latest information related to the book and contribute to the conversation about how to build great products. Before jumping to the first step of the Lean Product Process, I discuss in the next chapter the important concept of problem space versus solution space. Understanding this fundamental idea will help clarify our thinking as we work our way up the Product-Market Fit Pyramid.



Chapter 2

Problem Space versus Solution Space The Lean Product Process will guide you through the critical thinking steps required to achieve product-market fit. In the next chapter, I begin describing the details of the process, but before I do, I want to share an important high-level concept: separating problem space from solution space. I have been discussing this concept in my talks for years and am glad to see those terms used more frequently these days. Any product that you actually build exists in solution space, as do any product designs that you create—such as mockups, wireframes, or prototypes. Solution space includes any product or representation of a product that is used by or intended for use by a customer. It is the opposite of a blank slate. When you build a product, you have chosen a specific implementation. Whether you’ve done so explicitly or not, you’ve determined how the product looks, what it does, and how it works. In contrast, there is no product or design that exists in problem space. Instead, problem space is where all the customer needs that you’d like your product to deliver live. You shouldn’t interpret the word “needs” too narrowly: Whether it’s a customer pain point, a desire, a job to be done, or a user story, it lives in problem space. THE SPACE PEN

My favorite story to illustrate the concept of problem space versus solution space is the space pen. When NASA was preparing to send astronauts into space, they knew that ballpoint pens would not work because they rely on gravity in order for the ink to flow. One of NASA’s contractors, Fisher Pen Company, decided to pursue a research and development program to create a pen that would work in the zero


14

The Lean Product Playbook

gravity of space. After spending $1 million of his own money, the company’s president, Paul Fisher, invented the Space Pen in 1965: a wonderful piece of technology that works great in zero gravity. Faced with the same challenge, the Russian space agency equipped their astronauts with pencils. You can actually buy a “Russian space pen” (which is just a cleverly packaged red pencil). This story shows the risk of jumping into the solution space prematurely and the advantage of starting in the problem space. If we constrain our thinking to “a pen that works in zero gravity,” we may not consider creative, less-expensive solutions such as a pencil. In contrast, having a clear understanding of the problem space (devoid of any solution space ideas), allows for a wider range of creative solutions that potentially offer a higher return-on-investment. If the pencil and space pen were equally adequate solutions, then avoiding one million dollars of research and development cost would clearly be the preferable alternative. To avoid fixating on pen-based solutions, we could rephrase the problem space as: “a writing instrument that works in zero gravity.” That would allow for a pencil as a solution. But that’s still anchored on “a writing instrument” solution. We can do even better than that: “a way to record notes in zero gravity for later reference that is easy to use.” That problem space statement would allow for more creative solutions such as voice recording with playback. In fact, considering out-of-the-box solution ideas can help you refine your problem space definition, even if they aren’t feasible. In this case, a voice recorder would probably not be as good a solution as a Space Pen. It would need a power source and would require playback to refer to the notes again, which would be less convenient than being able to scan and read them. But undergoing this thought exercise would allow us to further refine our problem space definition to: “a way to record notes in zero gravity for convenient reference later on that is easy to use, is inexpensive, and does not require an external power source.” I always like to clarify that this example is by no means an attempt to make fun of NASA. I tell the story a certain way to highlight the point I want to make. Indeed, the conclusion that NASA came to turned out to be the best one. There are good reasons not to use pencils in space: the lead tips can break off and float into an astronaut’s eye or cause a short in an electrical connection. After the tragic

Problem Space versus Solution Space

Apollo 1 fire in 1967, NASA required all objects in the cabin to be nonflammable, including the writing instruments. So the Space Pen actually was a useful innovation, which the Russian space agency also adopted. When I mention the space pen in my talks, there is often someone who claims that the story is an urban legend. However, it isn’t, as NASA explains at http://history.nasa.gov/spacepen.html, and the Fisher Space Pen Company confirms at http://fisherspacepen.com/ pages/company-overview. The key point of debate usually is, who spent the money on research and development: NASA or Fisher? Fisher did, as I pointed out above. PROBLEMS DEFINE MARKETS

Early in my product career, Intuit’s founder Scott Cook helped me solidify the concept of problem space versus solution space when I heard him talk about TurboTax. Speaking to a group of product managers, Scott asked us, “Who is TurboTax’s biggest competitor?” Multiple hands shot up. At the time, the other major tax preparation software in the market was TaxCut by H&R Block. After someone confidently answered, “TaxCut,” Scott surprised us all by saying that the biggest competitor to TurboTax was actually pen and paper. He pointed out that, at the time, more Americans were still preparing their taxes by hand using IRS forms than all tax software combined. This example highlights another advantage of clear problem space thinking: having a more accurate understanding of the market in which your product is really competing. Those of us in the audience were narrowly thinking in solution space of the “tax preparation software” market, as defined by the two main software products. Scott was thinking in problem space of the broader “tax preparation” market—one that would also include tax accountants to whom customers delegate their tax preparation. As the previous chapter discusses, a market is a set of related customer needs, which rests squarely in problem space. A market is not tied to any specific solutions that meet those needs. That is why you see “market disruptions”: when a new type of product (solution space) better meets the market needs (problem space). New technology can often



16

The Lean Product Playbook

enable a market disruption to deliver similar benefits at a much lower cost. Voice-over-Internet-Protocol (VOIP) is a great example of a disruptive technology that has replaced traditional telephone service. At first, the sound quality of VOIP calls couldn’t compare to that of traditional phone lines, but the cost was so much lower that it offered a superior solution for much of the telephone market. THE WHAT AND THE HOW

As a product manager at Intuit, I learned to write detailed product requirements that stayed in the problem space without getting into the solution space. We were trained to first focus on “what” the product needed to accomplish for customers before getting into “how” the product would accomplish it. You often hear strong product teams distinguishing between the “what” versus the “how.” The “what” describes the benefits that the product should give the customer—what the product will accomplish for the user or allow the user to accomplish. The “how” is the way in which the product delivers the “what” to the customer. The “how” is the design of the product and the specific technology used to implement the product. “What” is problem space and “how” is solution space. OUTSIDE-IN PRODUCT DEVELOPMENT

A failure to gain a clear understanding of the problem space before proceeding to the solution space is prevalent in companies and teams that practice “inside-out” product development, where “inside” refers to the company and “outside” refers to customers and the market. In such teams, the genesis of product ideas is what one or more employees think would be good to build. They don’t test the ideas with customers to verify if the product would solve actual customer needs. The best way to mitigate the risk of an “inside-out” mindset is to ensure your team is talking with customers. That’s why Steve Blank urges product teams to “get out of the building” (GOOB for short). In contrast, “outside-in” product development starts with an understanding of the customer’s problem space. By talking with

Problem Space versus Solution Space

customers to understand their needs, as well as what they like and don’t like about existing solutions, outside-in product teams can form a robust problem-space definition before starting product design. Lean product teams articulate the hypotheses they have made and solicit customer feedback on early design ideas to test those hypotheses. This approach is the essence of Lean—and was actually first advocated for years ago by practitioners of user-centered design. SHOULD YOU LISTEN TO CUSTOMERS?

Some people criticize user-centered design by saying that talking with users will not lead you to come up with new, breakthrough solutions. Those critics like to quote Henry Ford, who famously said: “If I had asked people what they wanted, they would have said a faster horse.” They also like to point out the example of Steve Jobs and how Apple has launched many successful products using what seems to be a very “inside-out” product development process. In fact, Steve Jobs cited the same Henry Ford quote in a 2008 interview with Forbes. It is true that customers are not likely to identify the next breakthrough solution in your product category. But why would anyone expect them to? They are not product designers, product managers, or technologists. The fallacious thinking comes in when people use this argument to rationalize why it’s not important to talk with customers or to understand their needs and preferences. Most people who make that argument are really using it as an excuse to not talk with customers because they want to adopt an “inside-out” philosophy. They think that they have all the answers and that talking with customers is a waste of time. They don’t understand problem space versus solution space. It’s likely true that customers won’t invent a breakthrough product for you; but that doesn’t mean it’s a waste of time to understand their needs and preferences. On the contrary, a good understanding of customer needs and preferences helps product teams explore new potential solutions and estimate how valuable customers are likely to find each one to be. Critics of user-centered design like to justify their views by saying, “Apple doesn’t talk to customers.” At Apple’s 1997 Worldwide Developers Conference, Steve Jobs shared a more



18

The Lean Product Playbook

enlightened perspective that is consistent with the Lean Product Process when he said: You’ve got to start with the customer experience and work backwards to the technology. You can’t start with the technology and try to figure out where you’re going to try to sell it. . . . As we have tried to come up with a strategy and a vision for Apple, it started with: What incredible benefits can we give to the customer? . . . Not starting with: Let’s sit down with the engineers and figure out what awesome technology we have and then how we’re going to market that. And I think that’s the right path to take. A TALE OF TWO APPLE FEATURES

Even though Apple does indeed have a reputation for not soliciting customer feedback on products before they’re launched, a large part of why their products are so successful is because, despite that, they have an in-depth understanding of customer needs. Consider the Touch ID fingerprint sensor that Apple introduced with the iPhone 5S. Touch ID utilizes advanced technology: the high-resolution sensor is only 170 microns thick and captures 500 dots per inch. The button is made of sapphire crystal—one of the clearest, hardest materials available—to protect the sensor. The button also acts as a lens to precisely focus the sensor on the user’s finger. Touch ID maps out individual details in the ridges of fingerprints that are smaller than the human eye can see and can recognize multiple fingerprints in any orientation. It’s unlikely that any iPhone customer would have come up with such a solution. I would guess that Apple didn’t test the solution with many customers before launching it. Despite that, I argue that the iPhone team had a good understanding of the problem space and could be confident that customers would consider Touch ID valuable. Touch ID offered a new alternative to the traditional way of unlocking your iPhone and logging in to the App Store to make a purchase. Touch ID is better because what matters to customers when they’re authenticating is how convenient and how secure it is. Usually, there is a tension between those two customer benefits, with more convenient authentication mechanisms being less secure (and vice versa).

Problem Space versus Solution Space

Most iPhone users will tell you that they unlock their phones quite frequently, often multiple times per day. Because people value their time, reducing the time it takes to unlock is a clear benefit. iPhone users value security, too. They don’t want unauthorized people to be able to access their phone, especially if it is lost or stolen. With a four-digit passcode, the odds of someone guessing your passcode are 1 in 10,000. According to Apple, the odds that two fingerprints are similar enough for Touch ID to consider them the same is 1 in 50,000 (and it’s much harder to try different fingers than it is to type in different numbers). Touch ID makes authenticating much quicker than having to enter an unlock passcode or App Store password. It’s also more convenient because users no longer have to worry about forgetting these passcodes. Because Touch ID clearly saves time, is more convenient, and is more secure than the previous solution, the iPhone team could be confident that customers would consider the feature valuable, even without explicitly validating it with them. However, if Apple didn’t test Touch ID with customers, it still ran the risk of some unforeseen negative consequence. It’s worth pointing out that Apple does test their products internally with their employees (who are often a good proxy for customers). This internal testing tactic where you use your own product is called “dogfooding.” That being said, Apple isn’t perfect. For example, customers were not happy with a product “improvement” that Apple made with the power button on the 2013 MacBook Pro. In the prior version of the laptop, the power button was located away from the keyboard keys, was smaller, had a different color, and was inset, all of which made it difficult to press by accident. When users pressed the button in the prior version, a dialog window would appear, providing options to restart, sleep, or shut down their laptop, along with the option to cancel any action. But Apple decided to change the power button design for the 2013 version: they made it look like the other keys and incorporated it into the keyboard (in the upper right, where the eject key used to be). The new power button was placed right next to the “delete” key as well as the key that increases the sound volume, both of which are used frequently. As a result, users started accidentally pressing the power button (and then had to click the cancel button).



20

The Lean Product Playbook

To add insult to injury, Apple’s subsequent operating system update—OS X Mavericks—changed the behavior of the power button. When the power button is pressed in Mavericks, you no longer get the dialog window with its various choices; instead your computer goes right to sleep. The combined effect of those two changes (moving the power button and changing its behavior) resulted in frustrated users whose laptops would suddenly go to sleep unexpectedly. Usability issues such as this are easy to identify through customer testing—even with a small number of testers. Let’s compare these two Apple examples. In the case of the Touch ID, there were clear benefits and no unforeseen risks arose. In the case of the power button changes, what were the intended customer benefits? It’s unclear what they were. Perhaps the new power button design addressed internal company objectives related to aesthetics or reduced cost. Regardless, the button’s new design and behavior resulted in dissatisfaction for customers. It’s true that customers aren’t going to lead you to the Promised Land of a breakthrough innovative product, but customer feedback is like a flashlight in the night: it keeps you from falling off a cliff as you try to find your way there. USING THE SOLUTION SPACE TO DISCOVER THE PROBLEM SPACE

Customers are also not likely to serve you their problem space needs on a silver platter. It’s hard for them to talk about abstract benefits and the relative importance of each—and when they do, it’s often fraught with inaccuracies. It’s therefore the product team’s job to unearth these needs and define the problem space. One way is to interview customers and observe them using existing products. Such techniques are called “contextual inquiry” or “customer discovery.” You can observe what pain points they run into even if they don’t explicitly mention them to you. You can ask them what they like and don’t like about the current solutions. As you form hypotheses about the customer needs and their relative importance, you can validate and improve your hypotheses using these techniques. The reality is that customers are much better at giving you feedback in the solution space. If you show them a new product or design, they can tell you what they like and don’t like. They can compare it to other solutions and identify pros and cons. Having solution

Problem Space versus Solution Space

space discussions with customers is much more fruitful than trying to explicitly discuss the problem space with them. The feedback you gather in the solution space actually helps you test and improve your problem space hypotheses. The best problem space learning often comes from feedback you receive from customers on the solution space artifacts you have created. Problem space and solution space are an integral part of the Product-Market Fit Pyramid, as shown in Figure 2.1. Your product’s feature set and UX live in solution space—they’re what customers can see and react to. The other three layers of the pyramid live in problem space. The important interface between problem space and solution space occurs between your value proposition and your feature set. It is, of course, within your control to change your feature set and UX as you like. Unlike customers and their needs, which you can target but can’t change, value proposition is the problem space layer over which you have the most control. As Dave McClure of 500 Startups said, “Customers don’t care about your solution. They care about their problems.” Keeping problem space and solution space separate and alternating between them as you iteratively test and improve your hypotheses is the best way to achieve product-market fit. The Lean Product Process gives you step-by-step guidance on how to do that. Let’s jump into the first step of the process: identifying your target customer.

UX

Product:

Feature Set

Solution Space

Value Proposition

Product-Market Fit Underserved Needs

Market: Target Customer

FIGURE 2.1



Problem Space



Part II

The Lean Product Process

Chapter 3

Determine Your Target Customer (Step 1) You begin the Lean Product Process by identifying your target customer, which is the bottom layer of the Product-Market Fit Pyramid. The problem space benefits you’re going to identify pertain to a specific customer segment. Different customers will have different needs—and even those who have the same needs can have distinct views on their relative importance. FISHING FOR CUSTOMERS

Many companies have launched products without any explicit discussion of the target customer. Sometimes, a company will introduce a product with one target customer in mind but end up attracting a somewhat different customer segment. Matching a product with its target customer is like fishing. Your product is the bait that you put out there and the fish that you catch is your target customer. Sometimes you catch the type of fish you were going after and sometimes you catch a different type of fish. You can develop hypotheses about your target market, but you won’t truly know who your customers actually are until you throw your hook into the water and see what kind of fish bite. Once you have a product or a prototype to show customers, then you can gain clarity about the target market you’re attracting. Of course, your bait can attract more than one type of customer. For example, Quicken was designed so that individual consumers could easily manage their household finances. But it was so easy to use that small business owners started using it to manage their companies’ finances. To the Quicken team’s surprise, they had caught a second type of fish with the same bait. Through customer research, Intuit discovered that almost a third of Quicken users were using it to track


26

The Lean Product Playbook

the finances for their business. So Intuit developed a new Quicken Home and Business version of their software to better address the distinct needs of small business owners. This discovery about their target market also led Intuit to launch QuickBooks, an accounting software application designed exclusively for managing business finances. Companies often want to expand or change their target market. Some are looking to move “up market.” For example, they are currently selling to small businesses and want to sell to larger businesses. Some start out selling to large enterprises and then want to move “down market.” A company that has achieved success in one industry vertical may want to expand into adjacent vertical markets. For example, if you’ve built some course management software that has achieved product-market fit with college professors, you might try to expand into the professional training market. If the needs of the adjacent market are similar, your product may need only minor changes to be a good fit. HOW TO SEGMENT YOUR TARGET MARKET

You define your target customer by capturing all of the relevant customer attributes that identify someone as being in your target market. These attributes can be demographic, psychographic, behavioral, or based on needs. Dividing a broad market into specific subsets based on attributes is called market segmentation. Demographic Segmentation

Demographics are quantifiable statistics of a group of people, such as age, gender, marital status, income, and education level. Say you were developing an app for moms to easily share photos of their babies with friends and family. You could describe your target customers demographically as women 20 to 40 years old who have one or more children under the age of three. If you are targeting businesses, you’ll use firmographics instead; these are to organizations what demographics are to people, and include traits such as company size and industry. Two well-known systems used to identify industries are the Standard Industrial Classification (SIC) codes and the North American Industry Classification System (NAICS).

Determine Your Target Customer (Step 1)

Psychographic Segmentation

Psychographics are statistics that classify a group of people according to psychological variables such as attitudes, opinions, values, and interests. For the same app, you might describe your target customers as moms who enjoy using social media and like sharing pictures of their babies with friends and family. Looking back on the demographic description, you’ll see that it didn’t say anything about whether or not the 20- to 40-year-old women wanted to share photos or not. Psychographic attributes are more useful than demographics for many products. Rather than being the primary reason why someone is in your target market, demographics are often incidental. In this case, your app is targeted at moms with babies who want to share pictures. The fact that you’re targeting moms drives the “women” part, and the statistical data on the age at which women give birth determines the “20 to 40 years old” range. Behavioral Segmentation

You can also use relevant behavioral attributes to describe your target customer: whether or not someone takes a particular action or how frequently they do. You might define your target market as moms who currently share an average of three or more baby pictures per week on social media (e.g., Facebook, Instagram, etc.). If you were working on a stock trading app for active investors, you might define your target market as investors who place 10 or more stock trades per week. Needs-Based Segmentation

Another powerful market segmentation technique is needs-based segmentation. With this approach, you divide the market into customer segments that each have distinct needs. Let’s take Dropcam, for example, which offers an affordable, easy-to-use wireless camera. As a parent, I use Dropcam to monitor my children while they sleep: I can conveniently see and hear them on my smartphone app without having to go into their rooms. Others use Dropcam as a security camera for their home. Pet owners use it to check on their pets while



28

The Lean Product Playbook

they’re away. Businesses use it as a security camera while they’re closed and also while they’re open, to catch any dishonest behavior such as shoplifting. You couldn’t come up with a single, tidy demographic or psychographic description to accommodate all four of those disparate customer segments. But when viewed through the lens of needs-based segmentation, they appear as a unified group of customers that have the need to easily capture and view video remotely. Even though they share that common high-level need, each of these customer segments has different detailed needs. Dropcam understands this and tailors how they market their product to each segment. Under the “Uses” tab on their website, they have dedicated pages for “Home Security,” “Baby Monitor,” “Pets,” and “Business.” And Dropcam provides different product features tailored to different segments. For example, they offer cloud recording of the video stream for playback later. Because I use Dropcam only for real-time monitoring while my children sleep, that feature isn’t valuable to me. But it’s critical for security-minded customers, who are willing to pay Dropcam a monthly service fee on top of the cost of the camera. This example is a brief snapshot of what I discuss in the next chapter—connecting your target customers with what you believe their needs are. USERS VERSUS BUYERS

In some cases, especially for business-to-business products, the customer who will use your product (the user) is not the same person who makes the purchase decision (the buyer). For example, Salesforce.com is an application used by salespeople in a company. The VP of Sales is often the buyer. But in a particular company, the Chief Technology Officer may be who makes the purchase decision instead. There may be multiple stakeholders in a company who have to agree to a particular purchase decision: the Chief Financial Officer, the General Manager of a business unit, the General Counsel, the Chief Security Officer, and so forth. In such cases, it is useful to distinguish the economic buyer—the decision-maker who controls the budget and writes the check—from the other stakeholders involved in the decision-making process. The others are often potential “blockers” who have the ability to veto your product if they object to unmet

Determine Your Target Customer (Step 1)

requirements. The buyer often has distinct needs from the end user that need to be addressed to achieve product-market fit, and you should define your target buyer in addition to your target customer when warranted. TECHNOLOGY ADOPTION LIFE CYCLE

You may have heard of Crossing the Chasm, Geoffrey Moore’s classic book on how to market high-tech products. In his book, Moore helped popularize another important concept to consider when defining your target market—the technology adoption life cycle, which divides a market into five distinct customer segments based on their risk aversion towards adopting new technologies. Here are descriptions of the five customer segments: 1. Innovators are technology enthusiasts who pride themselves on being familiar with the latest and greatest innovation. They enjoy fiddling with new products and exploring their intricacies. They are more willing to use an unpolished product that may have some shortcomings or tradeoffs, and are fine with the fact that many of these products will ultimately fail. 2. Early Adopters are visionaries who want to exploit new innovations to gain an advantage over the status quo. Unlike innovators, their interest in being first is not driven by an intrinsic love of technology but rather the opportunity to gain an edge. 3. The Early Majority are pragmatists that have no interest in technology for its own sake. These individuals adopt new products only after a proven track record of delivering value. Because they are more risk averse than the first two segments, they feel more comfortable having strong references from trusted sources and tend to buy from the leading company in the product category. 4. The Late Majority are risk-averse conservatives who are doubtful that innovations will deliver value and only adopt them when pressured to do so, for example, for financial reasons, due to competitive threats, or for fear of being reliant on an older, dying technology that will no longer be supported. 5. Laggards are skeptics who are very wary of innovation. They hate change and have a bias for criticizing new technologies even after they have become mainstream.



30

The Lean Product Playbook

Moore noted that for many disruptive products that innovators and early adopters have embraced, it is very difficult to gain traction with the early majority. Therefore, he added a gap—or chasm—between early adopters and the early majority, hence the name of his book, which gives advice on how to successfully make that transition. When you are defining your target customer, it is important to understand the current stage of your product market in the technology adoption life cycle. You may initially target innovators for a new market, since they embrace new solutions, are willing to pay a premium to have them, and are willing to overlook product shortcomings outside the core area of innovation. As you try to gain adoption by additional segments over time, you will discover that they have different needs and preferences—such as increased ease of use, higher reliability, and lower price—that require you to change your product before they will adopt it. PERSONAS

The persona is a useful tool for describing your target customer. Alan Cooper championed the use of personas as part of his “Goal-Directed Design” process. In his book The Inmates are Running the Asylum, he describes personas as “a precise definition of our user and what he wishes to accomplish.” Cooper explains, “personas are not real people” but rather “hypothetical archetypes of actual users.” Personas have become quite prevalent and are used by many UX designers and product teams that embrace user-centered design. While personas are mainly used during the design phase, I advocate using them earlier in your product process because they are a good way to capture your hypotheses about your target customer. You will put your personas to work again a few steps later in the Lean Product Process when you start to create your initial designs; by starting your personas now, you’ll be well prepared. Personas also help to ensure that everyone in your company who’s involved with the product is aligned on the same customer. As with most endeavors involving a large number of people, if you don’t write it down, share it, and discuss it, chances are that everyone won’t be on the same page. At the end of the day, personas help

Determine Your Target Customer (Step 1)

people on the product team make decisions about which features are important and about how to design the user experience. A good persona empowers everyone on the team with the same solid foundation of information and reasoning. It should facilitate alignment when you’re making product decisions as a group. In addition, as each person works on their own, independently making lots of little product decisions, personas should make the results more congruous and additive instead of discordant and counterproductive. What Info Should a Persona Provide?

Good personas convey the relevant demographic, psychographic, behavioral, and needs-based attributes of your target customer. Personas should fit on a single page and provide a snapshot of the customer archetype that’s quick to digest, and usually include the following information: Name Representative photograph ● Quote that conveys what they most care about ● Job title ● Demographics ● Needs/goals ● Relevant motivations and attitudes ● Related tasks and behaviors ● Frustrations/pain points with current solution ● Level of expertise/knowledge (in the relevant domain, e.g., level of computer savvy) ● Product usage context/environment (e.g., laptop in a loud, busy office or tablet on the couch at home) ● Technology adoption life cycle segment (for your product category) ● Any other salient attributes ● ●

The two things on this list that really bring a persona to life are the photograph of the hypothetical person and the quote expressing what’s most important to them. Your team members will usually remember the name, photograph, and quote the most, especially when they are not looking at the persona. See Figure 3.1 for a



32

The Lean Product Playbook

FIGURE 3.1

Persona

sample persona. I adapted this example from a persona created by talented UX designer Becca Tetzlaff. You can see other examples of Tetzlaff’s work at http://beccatetzlaff.com. How to Create Personas

So how do you obtain the information to create your persona? If you have customers, you can use interviews and surveys. Talking

Determine Your Target Customer (Step 1)

to customers in one-on-one interviews is the best way to build this knowledge. Once you know the right questions to ask, surveys can help you collect data from many customers at once. When you use survey data, it is critical that you not use averages of the collected data to populate your persona. You want your persona to represent a real person and should not design your product for some nonexistent “average” customer. As Cooper illustrates, “The average person in my community has 2.3 children, but not a single person in my community has exactly 2.3 children.” Clearly, it would be better to specify a persona that has either two or three children. You can use the aggregate survey data to help ensure your persona represents a meaningful portion of your customer base. Reading through the individual survey responses from people who match your target customer profile can be enlightening. Of course, if you are launching a new product or trying to expand to a new target market, you won’t have existing customers. You can always use your judgment to make initial hypotheses about your target customer’s attributes, and then test those hypotheses by talking to prospective customers who match that profile. Before you have any designs or product on which to solicit feedback, you will mainly talk with prospective customers to gain a deeper understanding of their needs, usage of current solutions, and pain points so you can identify potential product opportunities. I and other Lean practitioners call these “customer discovery” interviews. In user-centered design, they are often called “contextual inquiry” or “ethnographic research.” As with all steps in the Lean Product Process, you should adopt an iterative approach. As you talk with more customers, you learn more and revise your persona to make it more accurate and robust. Your goal is to iterate until you feel confident that you have identified a target customer with an underserved customer need that you believe you can address. The next chapter covers customer needs in depth. Potential Problems with Personas

Many product teams have experienced success with personas. However, personas have a bad rap with some people. Those people usually haven’t seen high quality personas used in a strong



34

The Lean Product Playbook

user-centered design process. Like any tool, personas can be misused. Weak personas can lack key information, be poorly written, or be based purely on speculation versus grounded in real customer data. At the other extreme are personas that contain too many superfluous details that don’t add value. To be useful, a persona should be pragmatic and provide useful information that can help inform product design decisions. Developing a persona should not slow down your product process, and you should not spend an inordinate amount of time trying to perfect your initial persona. Instead, you should view it as a first draft that you will revise as you iterate through the process. No one starts out with a persona as robust and as honed as the one shown in Figure 3.1; that is the result of numerous rounds of iterative customer discovery. You will improve the fidelity of your persona over time as you learn more. Even if a persona is well written, the rest of the product team might ignore it. They should be referring to the persona as they make various design decisions and evaluate proposed designs. If your team members aren’t using the persona, you should try providing some education about personas, the benefits they provide, and how the team should use them. I have also seen companies develop a set of personas and then stop talking with customers. Over time, the company loses touch with its customers, especially as new product team members join. Personas are a great tool; however, they are no substitute for talking to customers on an ongoing basis. It’s good to talk to prospective customers early in your process. But once you have a product or prototype ready, you can gain a more accurate view of your target customer by putting your bait out there to see which kind of fish you actually catch. Like Quicken or Dropcam, you may find that you are attracting more than one distinct type of customer, in which case you should create a new persona for each type. Some people within your customer base will like your product more than others. Those people likely use it more frequently and recommend it to others, which you can see on social media. Talking with those passionate customers can especially help sharpen

Determine Your Target Customer (Step 1)

your hypotheses about your target market and gain insights into what underserved needs your product is meeting for them. Now that you have determined—or at least have a set of hypotheses about—your target customer, you are ready to move on to the next step in the Lean Product Process, which focuses on understanding customer needs. As I discuss in the next chapter, you care most about underserved needs, which form the next layer of the Product-Market Fit Pyramid.



Chapter 4

Identify Underserved Customer Needs (Step 2) Now that you have determined your target customers—or at least have a set of hypotheses about them—you should focus on identifying what needs they have that your product could satisfy. The goal is to build and validate your knowledge of the problem space before you set out to design a solution. Since customer needs can seem somewhat fuzzy when we talk about them, let’s start off by clarifying our terminology. A CUSTOMER NEED BY ANY OTHER NAME

I use the word “needs” to refer to what customers want or value. I also use the term customer benefits interchangeably with needs. Sometimes customers can tell you what they want, but a need does not have to be something about which the user literally says, “I need [______________].” There are unarticulated needs—those that the customer has but doesn’t express in an interview. Unknown needs can arise as well; this happens when a customer doesn’t even realize they value something until you interview them about it, or expose them to some new breakthrough product. Customers are generally not skilled at discussing the problem space; they are better at telling you what they like and dislike about a particular solution. Good interviewers excel at listening closely to what customers say, repeating statements back to ensure understanding, and asking additional probing questions to illuminate the problem space. You’ve probably heard some people speak of customer desires or wants as distinct from needs. Though all three terms represent customer value, some people perceive a need as critical, whereas desires and wants are just “nice-to-haves.” However, that distinction doesn’t add much value and results in confusion. In order to talk


38

The Lean Product Playbook

about needs, we don’t really need multiple terms that differ in importance—especially since I will be describing a framework for quantifying how important different needs are. When discussing user-centered design or persona development, people frequently use the term user goal. A user goal is no different from a customer need. In Agile development, user stories are used to convey what the customer wants. A well-written user story follows the format “As a [type of user], I want to [do something], so that I can [desired benefit].” For example: “As a Dropcam user worried about the security of my business, I want to quickly view only the suspicious activity that took place without having to watch the whole video, so that I can know what’s going on in my store without spending too much time watching security videos.” Good user stories reflect customer needs. Customer pain point is another frequently used term that also fits under the umbrella of “customer needs.” A pain point is just a customer need that is not adequately met, resulting in customer dissatisfaction. I’ll be discussing customer satisfaction as part of my framework. CUSTOMER NEEDS EXAMPLE: TURBOTAX

Let’s discuss an example of customer needs. In the United States, most working adults are required to file their personal income taxes each year—not something that most people enjoy. It can take a long time and feel overwhelming for most people since they don’t have in-depth knowledge about taxes. Because the rules are complex and change frequently, people often lack confidence that they have prepared their taxes accurately. If the Internal Revenue Service audits your tax return and finds it inaccurate, you have to pay a fine; you could face jail time in cases considered to be tax fraud. So customers clearly have a need to prepare their taxes. As discussed in Chapter 1, they can meet that need in various ways: by manually filling out the IRS forms, hiring a professional accountant, or using tax preparation software such as TurboTax. One thing to know about customer needs is that they are like onions: they have multiple layers, each with a deeper layer just below it. To fully grasp the problem space that TurboTax addresses requires getting much more detailed than “prepare my tax return.”

Identify Underserved Customer Needs (Step 2)

Tax preparation software can go well beyond the IRS tax forms, which are just instructions for how to prepare your tax return. Tax software can check the accuracy of your return. TurboTax can also file your taxes for you electronically, which is more convenient than having to print out and mail your return. It can help you maximize your deductions and reduce your audit risk. It can even download your tax information from your employer, banks, and brokerages so that you don’t have to enter it manually. Each of those items is a distinct customer benefit. Let’s list them explicitly: 1. Help me prepare my tax return 2. Check the accuracy of my tax return 3. Reduce my audit risk 4. Reduce the time it takes me to enter my tax information 5. Reduce the time it takes me to file my taxes 6. Maximize my tax deductions This is by no means an exhaustive list of the customer benefits that TurboTax provides. We could easily keep peeling the onion and identify many more benefits. For example, state tax returns are completely separate from federal returns. Also, TurboTax offers a service that lets you receive your tax refund more quickly. But for the purposes of this discussion, let’s focus on the six benefits listed above. Hopefully the way I’ve written them strikes you as sounding like customer benefits. One of the easiest ways to tell that a product team is starting with the solution space is that instead of articulating customer benefits, they list product features. As with well-written Agile user stories, benefits should be written from the customer’s perspective (using “I” and “my”). You’ll also notice that each benefit begins with a verb: help, check, reduce, maximize. A benefit conveys value, which means it’s doing something for the customer. Finally, many of the benefits speak to increasing something that’s desired (tax deductions) or decreasing something that is not desired (audit risk, time required to accomplish a task). You should strive to state your benefits in such a precise manner whenever possible. This makes the benefit very clear and often enables you to objectively measure the performance improvement your product is providing. As with everything in the Lean Product Process, customer benefits start out as hypotheses. You are saying, “I think that target customer



40

The Lean Product Playbook

X would find customer benefit Y valuable.” Once you have an initial set of hypothetical customer benefits you feel good about, it’s time to test them with users. The best way to do so is via one-on-one, in-person customer discovery interviews. CUSTOMER DISCOVERY INTERVIEWS

You should share each of your customer benefit hypotheses with the customer during the interviews. You should ask a set of questions about each benefit statement, such as: What does this statement mean to you? (to check their understanding) ● How might this help you? ● If a product delivered this benefit, how valuable would that be to you? ●

(Possible responses: no value, low value, medium value, high value, or very high value) For a response of high or very high value: Why would this be of value to you? ● For a response of low or no value: Why wouldn’t this be of value to you? ●

These questions help you to see if the way you’re describing the benefit is clear to users. They also help you learn how valuable the benefit is and why. The reasons why customers find certain benefits valuable are the gold nuggets you want to mine, since those comments help you gain a better understanding of how customers think and what’s important to them. If we asked customers in TurboTax’s target market about the six benefits we listed, they might respond with comments such as those in Table 4.1. You’ll find when conducting customer discovery interviews that different customers can use different words to describe the same idea. You will also find that statements made by customers can vary quite a bit in how high-level or specific they are. For example, if you asked

Identify Underserved Customer Needs (Step 2)

TABLE 4.1

Customer Beneﬁts and Related Comments

Customer Benefit

Typical Customer Comment

1. Help me prepare my tax return

“I don’t really know much about taxes. I try to follow the instructions but they’re confusing. I’m not sure which forms I should be filling out.” 2. Check the accuracy “I’m not that great at math, so I know I’m of my tax return probably making several mistakes when I’m adding and subtracting all those numbers on my tax forms.” 3. Reduce my audit “I’m worried about being audited but don’t risk really know how risky my tax return is. It would be great to know if it would raise any yellow flags with the IRS so I could fix those parts.” 4. Reduce the time it “I spend lots of time each year entering data takes me to enter from all the tax forms I receive from my my tax information employer, bank, and brokerages.” 5. Reduce the time it “I normally print my tax return and then go takes me to file my to the post office, wait in line, and mail it taxes so I can get delivery confirmation. It would be great if I could avoid that hassle.” 6. Maximize my tax “I don’t know about all the deductions that deductions I’m eligible to take. I’m probably leaving some money on the table.”

two customers why they like TurboTax, one may say, “Because it makes taxes much less of a hassle,” whereas the other may say, “I like how it checks my return for errors before I e-file it.” CUSTOMER BENEFIT LADDERS

As you talk with customers, you can keep asking them, “Why is that important to you?” until it doesn’t lead to any new answers. This helps elevate the discussion from more granular, detailed benefits to higher-level benefits. This market research technique is called “laddering”; as you ask more questions, you are climbing up rungs on a ladder of related benefits. As you move up, ladders



42

The Lean Product Playbook

can converge, until you eventually reach the top of that particular benefit ladder. Let’s walk through an example of a benefit ladder. Say we’re trying to understand why some drivers prefer a sports utility vehicle (SUV) to a minivan. We interview a customer and start by asking him that very question. He answers that he prefers SUVs because he doesn’t like sliding doors. When we ask why, he tells us that he prefers vehicles that have a stylish design. When we ask why again, he says, “Because I want to feel trendy.” When we ask why again, we learn that the ultimate motivation is that he wants to be accepted by his peers. When I look at the six benefits in Table 4.1, I see three distinct benefits ladders. The three benefits “help me prepare my tax return,” “check the accuracy of my tax return,” and “reduce my audit risk” all ladder up to making you feel confident in your taxes. The two benefits “reduce the time it takes me to enter my tax information” and “reduce the time it take me to file my taxes” both have to do with saving time. Finally, the benefit “maximize my tax deductions” ladders up to a higher level “save money” benefit. Table 4.2 shows how each of the detailed benefits maps to the corresponding benefit ladder. The laddering interview technique is similar to the “Five Whys” tool promoted by Eric Ries. Originally developed by Toyota, the Five Whys is an iterative question-asking technique to explore the root cause of a problem.

TABLE 4.2

Customer Beneﬁt Ladders

Benefit at Top of Ladder Feel confident

Save time

Save money

Detailed Customer Benefit 1. Help me prepare my tax return 2. Check the accuracy of my tax return 3. Reduce my audit risk 4. Reduce the time it takes me to enter my tax information 5. Reduce the time it takes me to file my taxes 6. Maximize my tax deductions

Identify Underserved Customer Needs (Step 2)

HIERARCHIES OF NEEDS

In addition to ladders, another complexity you will often encounter in the problem space is that customer needs can have hierarchies. These hierarchies create dependencies between needs, where the value created by addressing one need is a function of how much another need is being met. Maslow’s Hierarchy of Human Needs

Let’s discuss a well-known example of this phenomenon: Maslow’s hierarchy of human needs, shown in Figure 4.1. Abraham Maslow was a famous twentieth-century American psychologist. In Maslow’s five-level hierarchy, physiological needs such as eating, drinking, and sleeping come first, forming the base. The second tier is safety and security needs. The third tier is love and belonging needs such as family, friends, and intimacy. The fourth level is

FIGURE 4.1

Maslow’s Hierarchy of Human Needs



44

The Lean Product Playbook

esteem needs: achievement and respect. The top level of the pyramid is self-actualization needs: fulfillment through realizing your potential. The implication of the hierarchy is that a higher-level need doesn’t really matter unless the more basic needs below it are met. As you explore the problem space for your product, you will likely encounter similar hierarchies. You’ll find situations where customer benefit B doesn’t matter if benefit A—which is at a lower level on the needs hierarchy—hasn’t yet been met. My Hierarchy of Web User Needs

When I led product management at Friendster, I learned about such hierarchies the hard way. Friendster was the first popular social networking site. Social networking sites (and social products in general) are known for the explosive viral growth they can experience. Friendster experienced a rapid growth in users, so much so that the volume of usage began to outstrip the ability of our web servers to keep up with traffic. Many Friendster users loved our product; however, they didn’t like it when our website was loading slowly or just not available due to these technical performance issues. To help our team prioritize its work, I created a hierarchy of web user needs—with a tip of the hat to Maslow—shown in Figure 4.2. The left side of Figure 4.2 shows the five-level hierarchy from the customer’s perspective. To the right of each tier is what it means to us at the company. As entrepreneurs, product managers, developers, and designers, we love to spend our time coming up with cool new feature ideas and designing great user experiences. However, those items sit at the top two levels of the pyramid of user needs. First and foremost, the product needs to be available when the user wants to use it. After that, the product’s response time needs to be fast enough to be deemed adequate. The next tier pertains to the product’s quality: Does it work as it is supposed to? We then arrive at the feature set tier, which deals with functionality. At the top, we have user experience (UX) design, which governs how easy—and hopefully how enjoyable—your product is to use. As with Maslow’s hierarchy, lower-level needs have to be met before higher-level needs matter. How your product stacks up against this hierarchy of needs is not static, it changes over time. Let’s assume you are fortunate enough to have a highly available, fast, bug-free product at a certain point

Identify Underserved Customer Needs (Step 2)

FIGURE 4.2

Olsen’s Hierarchy of Web User Needs

in time. You then launch a new feature. It will probably have some bugs, reducing your product’s quality. The new feature may place higher demands on your database, degrading your product’s performance. Or it may be so popular that your usage spikes, overloading your servers and causing a slowdown. As you work on features and UX design, it’s important to keep this hierarchy in mind and proactively “dip down” to address deficiencies at lower levels when they occur. THE IMPORTANCE VERSUS SATISFACTION FRAMEWORK

Once you’ve explored the problem space and identified the various customer needs that your product could meet, you have to decide which ones you want to address. So you need a good way to prioritize among the different needs—and prioritizing based on customer value is a good approach. That begs the question: How do you determine customer value? I faced that question when I led product management for Quicken. We launched a new version of Quicken every year, and I had to determine the plan for the next version of the product. Intuit excels at customer research, so I had the opportunity to design both quantitative and qualitative research



46

The Lean Product Playbook

to capture the information I wanted from customers. I used the results of this research to create a framework based on importance and satisfaction. I’ve found this framework provides the best way to think about how to create customer value in a rigorous, analytical manner. My framework worked well in prioritizing opportunities to create customer value for that version of Quicken, which achieved new records for sales volume, revenue, and profit. Since then, I’ve been excited to discover other frameworks that are also based on importance and satisfaction (which I’ll discuss later). Not surprisingly, importance is a measure of how important a particular customer need is to a customer. Importance is a problem space concept, separate from any specific solution space implementation. For a given customer, different needs will have different levels of importance. For example, some people consider the need for privacy more important than the need to share updates and pictures with their friends. I know several people who don’t use social media for this reason. The same need will have different levels of importance across different customers. Among my friends, there is a range of how important they consider sharing updates and pictures with their friends. For some, it’s so important that they post multiple updates a day. Others post updates very rarely. Differences in the importance of needs influence a customer’s decisions and preferences. Satisfaction is a measure of how satisfied a customer is with a particular solution that provides a certain customer benefit. It indicates how well that solution meets their needs. Different products will have different levels of satisfaction for the same customer, and the same product can provide different levels of satisfaction to different customers. The power of the framework comes when you look at importance and satisfaction together, as shown in Figure 4.3. Importance is on the vertical axis (from low to high) and satisfaction is on the horizontal axis (from low to high). Let’s divide the graph into four quadrants. You can use this framework to evaluate potential product opportunities, either for new products or for additions or improvements to an existing product. Let’s start with the bottom two quadrants. The bottom left quadrant represents low satisfaction and the bottom right represents high satisfaction, but both represent low importance. There is not much point in pursuing low importance ideas, regardless of the satisfaction

Identify Underserved Customer Needs (Step 2)

FIGURE 4.3

The Importance versus Satisfaction Framework

level, since they just won’t create enough customer value. You want to address high importance customer needs. Later in this chapter, I will show you an importance versus satisfaction chart populated with actual data for a real product. The upper right quadrant is high importance as well as high satisfaction. This would be the case in a market where the leading products are robust and do a good job of meeting customer needs. Microsoft Excel comes to mind as a relevant example, since it does pretty much everything people expect a spreadsheet application to do. Wikipedia refers to the program as “the industry standard for spreadsheets.” The feature set and user interface stabilized years ago with no major innovations in a while—so much so that Excel hasn’t had any competition from other desktop spreadsheet applications in a long time. The top competitive threat is from cloud-based application providers, some of which provide a subset of Excel’s functionality for free. It’s important to note that it doesn’t always have to be the case that markets in the upper right quadrant have a single, dominant product. A market could have several, quite similar products. The leading all-in-one printers fit the bill of high importance and high satisfaction; however, there are comparable models from many



48

The Lean Product Playbook

manufacturers including Hewlett Packard, Epson, Canon, Brother, and Lexmark. If you are using the framework to assess a product’s features, the upper right quadrant represents a feature that is performing well. It is addressing a high importance need and customers are very satisfied with it. For example, in a user survey for one of my products, I discovered such a feature that scored 100 percent on importance and 98 percent on satisfaction. The upper left quadrant is high importance of need but low satisfaction with current solutions. Customer needs in this quadrant are important but underserved. As a result, they offer excellent opportunities to create customer value. A good example of this quadrant is the ride service app Uber. Uber’s Success: Meeting Underserved Needs

Uber has experienced spectacular success and growth. Achieving such results clearly requires great execution, and the company has an attractive business model. But looking at Uber through the lens of the importance versus satisfaction framework provides insight into another fundamental reason for the company’s success. Many people have an important need to be driven from one place to another, whether it’s on short notice or scheduled in advance. Taxis are a very common and traditional solution for this need. But few taxi riders would say they are very satisfied with their customer experience. Common complaints include dirty cars, rude drivers, problems communicating with drivers, concerns about unsafe driving, uncertainty about the cost of trips, and the hassle of payment and tipping. People also complain about taxis that arrive late or never at all. While taxis (usually) meet the basic overall need of getting people from point A to point B, these complaints reveal many important underserved needs related to safety, comfort, convenience, affordability, and reliability. The needs of taxi customers are clearly in the upper left quadrant. The combination of the high importance of this set of needs, the low satisfaction with existing solutions such as taxis, and the large number of people with these needs points to a substantial market opportunity—one that Uber saw and acted upon. Uber used technology to capitalize on the opportunity with a mobile app that allows you to easily hail a car on your smartphone.

Identify Underserved Customer Needs (Step 2)

The app starts with a map of your local area that shows the location of nearby Uber cars. It then matches you with a specific driver, showing you his or her name, photo, rating, car model, and license plate number. The app tells you the estimated time that the driver will take to arrive and shows you the car’s location on a map in real-time. This increased transparency compared to calling a taxi or trying to hail one significantly reduces the anxiety about arriving at your destination on time. Uber includes a feedback system that requires riders to rate their drivers. This ratings data empowers riders to be informed about their prospective drivers, and Uber uses it to weed out drivers that aren’t up to snuff. Uber also improves the financial aspects of the customer experience. The app lets riders check the estimated fare before a trip, which helps avoid surprises at the end of their trip. The app stores your credit card information so that it can automatically handle payment at the end of your ride without any effort required. In contrast, dealing with payment at the end of a traditional taxi ride can be a hassle. Having to wait for the driver to run your credit card and print out the credit slip and receipt causes delays. Some drivers accept only cash, which can cause a problem if you don’t have enough on hand. With Uber, at the end of your trip you just leave the car and don’t have to worry about any of that. It’s clear that Uber addressed multiple underserved needs that were in the upper left quadrant of high importance and low satisfaction. As a result, Uber has seen incredible success since starting in 2009. Though it is a privately held company, financial data leaked in December 2013 showed that Uber had over 400,000 active clients taking over 800,000 rides per week. The gross revenue run rate at the time exceeded $1 billion per year, of which Uber keeps 20 percent. In December 2014, Uber raised $1.2 billion in investment at a valuation of $40 billion. You might not achieve the same level of success as Uber, but the high importance, low satisfaction quadrant is the best place to pursue opportunities. Disruptive Innovation versus Incremental Innovation

When discussing innovation, it’s common to distinguish between disruptive innovation and incremental innovation. Incremental innovation occurs when you make minor improvements that add small



50

The Lean Product Playbook

amounts of customer value with each new version of your product. You can do so, for example, by increasing satisfaction for the established set of benefits or addressing additional benefits. It’s clear that Uber is disrupting the mature and well-established taxi market. Most people would consider the app an example of disruptive innovation. Their product offering provides significantly more customer value than the alternative solutions that existed when it was launched. People often refer to a “10×” improvement as disruptive innovation. When a new product enables such a better way of doing something that people can’t imagine going back to the old way, that’s disruptive innovation. A disruptive innovation such as Uber can emerge from an upper left quadrant opportunity where there was low satisfaction with a high importance need. Disruptive innovations can also redefine the existing satisfaction scale for their market. Consider the example of a mature, competitive market with one or more leading products in the upper right corner of the importance versus satisfaction framework. A disruptive innovation can come along and push all of those leading products to the left by offering a higher level of satisfaction that wasn’t available before. In doing so, it changes the scale of the satisfaction axis. Disruptive Innovation: Music on the Go

Let’s discuss a string of innovations where disruption occurred in the market for portable music listening. The high-level customer benefit could be stated as: “Allow me to listen to music on the go.” The first product that addressed this benefit was the transistor radio in the 1950s. Prior to that, radios relied on vacuum tubes, which were larger, required more power, and were fragile, making portability infeasible. But while portable radios allowed you to listen to music, you couldn’t select the songs you wanted. That changed in 1979 when Sony introduced the first portable cassette audio player, the Walkman. Listeners could now listen to the music they wanted to hear by playing tapes. The Walkman was a disruptive innovation that shifted the satisfaction scale, displacing portable radios in the upper right quadrant and pushing them to the left.

Identify Underserved Customer Needs (Step 2)

Several years later, Sony launched the first portable CD player, the Discman, which offered additional benefits: higher sound quality and the ability to easily and quickly jump from one song to the next versus having to fast-forward or rewind a cassette tape. Although it was eventually solved later, one negative of the earlier Discman models is that the CD would skip when jostled. I would consider the portable CD player an incremental innovation over the portable cassette player. For some target customers, the portable CD player would be located slightly to the right of the Walkman in my framework, but it didn’t change the satisfaction scale. The next portable music innovation was the MP3 player, first launched in 1998. The first models didn’t have a large capacity to store songs, but that changed over time. Apple entered the MP3 player market with the iPod in 2001. It wasn’t a runaway hit at first, but the company made major improvements in subsequent models. The combination of its large storage capacity, intuitive user interface, and integration with the iTunes jukebox software and digital music store led it to become the leading MP3 player, with over 70 percent market share. The iPod was a disruptive innovation that yet again redefined the satisfaction scale for portable music listening. Reflecting on this example, the scale for the satisfaction axis is defined by the solutions that exist in the market—more specifically, by the “high water mark” of current solutions. When better solutions that deliver more customer value come out, the upper value on the right side of the scale gets redefined, shifting everything to the left. In contrast, the importance axis is more stable. The customer need to listen to music on the go was a constant throughout the four waves of new technical solutions across over 50 years. The importance of that need may have changed slowly over time with societal and cultural trends, increasing as more people are on the go—but nowhere near as drastically as the satisfaction scale. That being said, the iPod and other MP3 players have been on the decline. So who is eating the iPod’s lunch? It’s the iPhone and other smartphones, which have incorporated everything an MP3 player can do (and more) into their feature set. Interestingly, the need to listen to music on the go has morphed from a stand-alone benefit and has become subsumed by a set of many related needs that customers have



52

The Lean Product Playbook

when they’re on the go—making phone calls, sending text messages, browsing the web, playing games, using apps, and so forth—all of which are addressed by a single solution: the smartphone. Measuring Importance and Satisfaction

In my workshops, most people see the value that the importance versus satisfaction framework provides. However, one area that I receive a lot of questions about is how to measure values for importance and satisfaction. The easiest way to think about this is a question that you ask your customers (or prospective customers). You can ask the question in person or in a survey. Let’s pretend we’re on the Uber product team. Imagine we survey a thousand of our target customers and ask them: “When you take a ride in a taxi or other hired car, how important is it to you that the driver is polite?” We could use a five-point response scale: 1. Not at all important 2. Slightly important 3. Moderately important 4. Very important 5. Extremely important We could use the average of all the scores as the importance rating. If we wanted to, we could map this five-point scale to a scale from 0 to 100 (or 0 to 10) for easier interpretation. We would then ask similar questions about the importance of car cleanliness, car comfort, driver punctuality, safe driving, and so forth. For satisfaction, we could ask: “How satisfied are you with how polite your driver was during the taxi rides you’ve taken in the past six months?” We could use a seven-point response scale: 1. Completely dissatisfied 2. Mostly dissatisfied 3. Somewhat dissatisfied 4. Neither satisfied nor dissatisfied 5. Somewhat satisfied 6. Mostly satisfied 7. Completely satisfied

Identify Underserved Customer Needs (Step 2)

We could use the average of the scores as the satisfaction rating. If we wanted to, we could map this seven-point scale to a scale from 0 to 100 (or 0 to 10) for easier interpretation. We would ask similar questions about their satisfaction with car cleanliness, car comfort, driver punctuality, safe driving, and so forth. In addition to prospective customers, we could also survey current Uber users—and ask the same exact importance questions. We would ask them similar satisfaction questions, but they would instead be in reference to Uber (as opposed to about traditional taxis). Comparing satisfaction ratings with competitive products is a good way to identify where your product is perceived as better or worse. You might wonder why I used different rating scales for importance and satisfaction. Part of the reason is that there are two types of rating scales: unipolar and bipolar. A bipolar scale goes from negative to positive, whereas a unipolar scale goes from 0 to 100 percent of an attribute. It’s usually best to measure satisfaction using a bipolar scale; since people can be satisfied or dissatisfied, a negative score makes sense. In contrast, importance is just a matter of degree— without any negative value—and therefore better measured with a unipolar scale. You could choose to use different scales with customers, for example, 1 to 10 or 0 to 10. Using more than 11 choices will overwhelm customers, while using fewer than 5 won’t achieve enough granularity. For any bipolar scale, I recommend using an odd number of choices so that there is a neutral option in the middle. Significant research has been performed on the reliability and validity of various scales, and it is generally agreed that 5-point scales are best for unipolar and 7-point scales are best for bipolar—which explains why I recommend the choices above. As I mentioned, you can map the scale you use with customers to another scale to make interpretation and calculations easier. For example, you could map the values of a 5-point scale to 0, 25, 50, 75, and 100. Or to 0, 2.5, 5, 7.5, and 10. Likewise, you could map the values of a 7-point scale to 0, 16.7, 33.3, 50, 66.7, 83.3, and 100. Since you can easily transform the scores, you should use a scale with customers that is easy for them to understand and that doesn’t try to ask them for more precision than they can realistically provide.



54

The Lean Product Playbook

An Importance and Satisfaction Example with Real Data

To make these concepts more tangible, let’s look at some real data from a real product. For one of the products I worked on, we surveyed our users periodically to have them rate the key product features. In one such survey, we asked users to rate the importance of and their satisfaction with 13 key features. We averaged the ratings for each feature and plotted them, as seen in Figure 4.4. Each of the 13 points you see is one of the key features. The number plotted next to each point is the satisfaction rating for that feature. In the upper right corner, you can see the feature I mentioned earlier, with 100 percent importance and 98 percent satisfaction. As a product manager, I was very happy with that result. Because the feature was already doing so well, I didn’t want to expend any of our team’s precious resources on trying to improve that feature further. Instead, I focused on improving the feature that is closest to the upper left corner of high importance and low satisfaction. See the point labeled “55”: that feature had 82 percent importance and only 55 percent satisfaction. There was only one other feature with lower satisfaction (of 41 percent).

FIGURE 4.4

Real Data for Importance versus Satisfaction

Identify Underserved Customer Needs (Step 2)

However, that feature was much less important (only 53 percent). It’s worth pointing out that customers can only rate the satisfaction of a solution if they’ve used it. I often hear people say that they’re building a new product and don’t yet have a customer base that they can survey. They’re concerned about not being able to reach enough customers to achieve statistical significance. But even if you can’t easily reach thousands of people, you can still obtain meaningful results. A Sample Size of Zero Is Okay

Let’s return to Uber. Assume we conduct one-on-one interviews with 25 people who frequently use taxis and ask them our importance and satisfaction survey questions. What percentage of them would you expect to be “completely satisfied” with their taxi experiences? It’s easy to envision that few or even none of the 25 would say that. In our discovery interviews, we would uncover the more detailed needs of comfort, convenience, safety, reliability, and so forth. Imagine we asked the 25 customers to rate the importance of each of those benefits, along with the corresponding satisfaction level when they’ve taken taxis. Meaningful patterns could emerge in the results—even though we haven’t surveyed thousands of people. For example, if a very large percentage of people you interview rate something high or low, there’s a decent chance you’ve uncovered something that will be proven out as you gain more data points. I call this technique “doing quant on qual”—quantitative analysis on qualitative data. While you must use it with care, it is an underutilized tool. Statistical analysis is a powerful tool that has its place; but too many product people have convinced themselves that they need to prove things beyond a shadow of a doubt. That’s just not the case—and often, especially in the early stages of working on a v1 product, not even possible. Statistical significance is great when you have the sample size to achieve it, but it isn’t an all or nothing proposition. I would go one step further and say that you can even use the importance versus satisfaction framework before you talk to a single customer. That’s right; you can make progress with a sample size of zero. How? By using the framework to formulate and clarify your hypotheses. The Lean approach is all about articulating clear hypotheses and



56

The Lean Product Playbook

then designing tests to determine if they are valid. Before you do your first customer interview, you can form hypotheses about what needs are most important to your target customers. You can also hypothesize about what they like and don’t like about current solutions and their level of satisfaction. You could lay out each of your hypotheses on the four-quadrant framework—either digitally or with Post-it notes—and then move them around, revise them, and add new ones as you learn and iterate. RELATED FRAMEWORKS

I mentioned that I was excited to discover other frameworks that were also based on importance and satisfaction after I came up with my framework at Intuit. Gap analysis and jobs to be done both use importance and satisfaction to quantify the size of different product opportunities to inform your prioritization. Gap Analysis

The first related framework is “gap analysis.” Now, you will find more than one definition of gap analysis if you search online. The version to which I’m referring is based on calculating the “gap” between importance and satisfaction. So you simply take the rating for importance and subtract from it the rating for satisfaction. Gap = Importance − Satisfaction The bigger the gap, the more underserved the need. With this framework, situations where the satisfaction is greater than the importance will result in a negative gap. The strength of gap analysis is that it produces a single number that is very easy to calculate. However, its biggest shortcoming is that it treats all gaps of equal size the same. For example, using a 0 to 10 scale, if a need had an importance of 10 and a satisfaction of 5, the gap would be 5. If another need had an importance of 6 and a satisfaction of 1, the gap would also be 5. But this doesn’t make intuitive sense because a gap of 5 on a need with an importance of 10 should be more important than the same size gap on a need with

Identify Underserved Customer Needs (Step 2)

an importance of 6. Let’s move on to another framework based on importance and satisfaction that addresses this deficiency. Jobs to Be Done

I was delighted to discover Anthony Ulwick’s book What Customers Want. In it, he describes his outcome-driven innovation approach, which also uses importance and satisfaction to quantify opportunities. Ulwick utilizes a slightly more complex calculation for his opportunity score that addresses the problem with the gap analysis calculation: Opportunity Score = Importance + Maximum (Importance − Satisfaction, 0)

His calculation subtracts satisfaction from importance, as in gap analysis. However, he does not allow that difference to become negative; it can only go as low as zero. To that difference he adds importance so that it becomes a tiebreaker for gaps with the same size. Using 0 to 10 for each rating, the resulting score can vary from 0 (when importance is zero) to 20 (when importance is 10 and satisfaction is 0). Ulwick considers opportunities with scores greater than 15 to be very attractive, and those below 10 to be unattractive. Let’s calculate the opportunity score using the same two needs from the previous example. The first need with an importance of 10 and a satisfaction of 5 would have an opportunity score of 10 + Maximum (10 – 5, 0) = 10 + 5 = 15. For the second need with an importance of 6 and a satisfaction of 1, the opportunity score would be 6 + Maximum (6 – 1, 0) = 6 + 5 = 11. Using Ulwick’s formula, even though the gap in importance and satisfaction is the same between the two needs, the first need with the higher importance has the higher opportunity score. Central to Ulwick’s methodology is the idea that customers buy products and services to help them get a task or job done. Customers decide which product to buy based on how well it delivers their “desired outcomes” for the “job to be done.” Clayton Christensen and others have also promoted this approach, commonly referred to as “jobs to be done.”



58

The Lean Product Playbook

Ulwick explains why he considers outcomes to be superior to customer needs or benefits. His main criticism of needs and benefits is that they are usually stated imprecisely. He warns against a “customer-driven” approach that relies too heavily on the “voice of the customer,” since customers are often imprecise or ambiguous in their language. I agree that customer needs and benefits should be precisely defined—and it is the job of the product team, not customers, to define them. In order to identify innovative solutions, the product team needs to create a rich definition of the problem space. I share Ulwick’s concern that all too often, product objectives or requirements are far too “fuzzy”—too high-level or vague. He explains that “For most jobs, even those that may seem somewhat trivial, there are typically 50 to 150 or more desired outcomes—not just a handful.” That sentence resonated strongly with my own belief that it is possible—and actually essential to successful innovation—for product teams to create a detailed and precise definition of their problem space. What Ulwick calls outcomes, I would call well-defined customer benefits. Rather than just scratching the surface, good product teams are able to iteratively peel the onion to gain deeper and deeper insights. Steve Jobs shared a similar view, saying: When you first start off trying to solve a problem, the first solutions you come up with are very complex, and most people stop there. But if you keep going, and live with the problem and peel more layers of the onion off, you can oftentimes arrive at some very elegant and simple solutions. Most people just don’t put in the time or energy to get there. VISUALIZING CUSTOMER VALUE

If the idea of quantifying product opportunities by using measurements of importance and satisfaction resonates with you, I recommend you read What Customers Want by Anthony Ulwick. Using my importance versus satisfaction framework, I have developed my own quantitative approach that I find a bit more visually intuitive. My approach goes beyond just quantifying opportunities;

Identify Underserved Customer Needs (Step 2)

it provides a broader view that visually explains customer value and how it is created. Customer Value Delivered by a Product or Feature

Let’s return to the importance versus satisfaction framework introduced in Figure 4.3 and get more precise about the values on the axes. Instead of going from low to high, think of importance and satisfaction as ranging from 0 to 100 percent, as shown in Figure 4.5. That way, whether you measure values using a 5-point, 7-point, 10-point, or 100-point scale, you can always be consistent. Each point that can be plotted on the graph represents a need with a certain importance and level of customer satisfaction with the product or feature addressing that need. Consider the point at the lower left corner of the graph where importance and satisfaction are both zero. A product or feature at that point would not be delivering any customer value. In contrast, a product or feature at the upper right corner where importance and satisfaction are both 100 percent would deliver the maximum amount of customer value for that need.

FIGURE 4.5

Visualizing Customer Value



60

The Lean Product Playbook

The higher the importance of the need that a product or feature meets, the more customer value it delivers. And the higher the satisfaction with the product, the more customer value it provides. When a product or feature is plotted as a point on the graph, the amount of customer value it provides is the area of the rectangle the point creates with the origin. So you can calculate the customer value delivered with the equation: Customer Value Delivered = Importance × Satisfaction Consider the product shown in the Figure 4.5 plot. The need has an importance of 70 percent, and the product addressing that need has a satisfaction of 70 percent. Therefore, the customer value this product delivers in meeting that need is 0.7 × 0.7 = 0.49. This approach is visually intuitive. If you plot multiple products or features on the same chart, as shown in Figure 4.4, it’s pretty easy to see which delivers the most value. The larger the area of the rectangle, the more customer value the product or feature creates. Opportunity to Add Customer Value

You can also easily assess the opportunity associated with a given product or feature that is represented by a point in the importance versus satisfaction space. The opportunity for each point is simply the maximum amount of customer value that can be added to it. Customer value can be added by increasing satisfaction, up to the maximum value of 100 percent (or 1). This can be expressed quantitatively as: Opportunity to Add Value = Importance × (1 − Satisfaction) This approach makes it easy to visually assess the opportunity to create additional customer value that is associated with a given product or feature. The opportunity for a product or feature represented by a point is just the area of the rectangle to the right of it, which is the maximum customer value that can be added to better address that need.

Identify Underserved Customer Needs (Step 2)

Figure 4.6 shows two different product opportunities. Opportunity A (corresponding to the product shown in Figure 4.5) has an importance of 70 percent and a satisfaction of 70 percent, so its opportunity score is: Opportunity A = 0.7 × (1 − 0.7) = 0.7 × 0.3 = 0.21 Opportunity B has an importance of 90 percent and a satisfaction of 30 percent, so its opportunity score is: Opportunity B = 0.9 × (1 − 0.3) = 0.9 × 0.7 = 0.63 Opportunity B offers the potential to create three times as much customer value as Opportunity A. When you are evaluating opportunities to pursue, you should pursue the ones with the highest opportunity scores. As a reminder, they occur in the upper left quadrant, as did the opportunity that Uber pursued. If you refer back to Figure 4.4, which shows real data from an actual product, you’ll recall that I gave the highest priority to the feature that had an importance of 82 percent and a satisfaction of

FIGURE 4.6

Measuring Opportunity



62

The Lean Product Playbook

55 percent; let’s call it “Feature X.” Take a look at all the points plotted in the figure. Once you know how to assess opportunity, it’s easy to see that Feature X offers the greatest opportunity. The Feature X opportunity score is: Feature X Opportunity = 0.82 × (1 − 0.55) = 0.82 × 0.45 = 0.37 All 11 of the features to the right of Feature X in the chart have opportunity scores less than 0.25. And the feature with an importance of 53 percent and a satisfaction of 41 percent only offers an opportunity score of 0.32. Note that although that last feature appears in the bottom left corner of Figure 4.4, that’s only because the axes weren’t shown going all the way to zero. If they did, that feature would be located near the center of the graph. The customer value that a product delivers varies with the satisfaction level (the width of the rectangle), but the maximum customer value it can deliver (the area of the widest rectangle) is fundamentally determined by the importance of the need (the height). This reinforces why it’s best to focus on high importance needs, where you can create the greatest value. So another way to express the opportunity is: Opportunity = Importance − Current Value Delivered Customer Value Created by Product Improvements

You can also visualize the actual customer value created by product improvements that you make. If an improvement increases satisfaction from the current level (Satbefore ) to a higher level (Satafter ), then the customer value it creates is the area of the incremental rectangle, given by: Customer Value Created = Importance × (Satafter − Satbefore ) Figure 4.7 illustrates the creation of customer value by making a product improvement that increases satisfaction (sticking with the example product from Figure 4.5). The importance of the need is 70 percent and the satisfaction before making the product improvement was 70 percent. The satisfaction increases to 90 percent after

Identify Underserved Customer Needs (Step 2)

FIGURE 4.7

Creating Customer Value

improving the product. Applying the above formula, the amount of customer value created was 0.14. Customer Value Created = 0.7 × (0.9 − 0.7) = 0.7 × 0.2 = 0.14 As I’ve discussed, a product usually addresses many related customer needs—not just one. So, in addition to increasing satisfaction, another way to create more customer value with your product is to improve it so that it addresses additional, related customer needs—ideally, those with higher importance. THE KANO MODEL

Another excellent framework for understanding customer needs and satisfaction is the Kano model developed by quality management expert Noriaki Kano. I first studied this model in my industrial engineering graduate program. As shown in Figure 4.8, the Kano model also plots a set of two parameters on horizontal and vertical axes: (1) how fully a given customer need is met (horizontal axis),



64

The Lean Product Playbook

FIGURE 4.8

The Kano Model

and (2) the resulting level of customer satisfaction (vertical axis). The horizontal axis ranges from the need not being met at all on the left to the need being fully met on the right. The vertical axis ranges from complete customer dissatisfaction at the bottom to complete satisfaction at the top—consistent with the bipolar scale discussed earlier. I won’t go into the details of how you generate the data by asking customers questions and the six possible results for a given need. The utility of the model is that it breaks customer needs into three relevant categories that you can use: performance needs, must-have needs, and delighters. With performance needs, more is better. As the need is more fully met, the resulting customer satisfaction increases. Say you were shopping for a car and considering two different models. If they were identical in all aspects but Car A had twice the fuel efficiency (e.g., miles per gallon) of Car B, you would have a preference for Car A. Fuel efficiency is a performance benefit for cars. Must-have needs don’t create satisfaction by being met. Instead, the need not being met causes customer dissatisfaction. Must-have

Identify Underserved Customer Needs (Step 2)

features are “table stakes” or “cost of entry”—boxes that must be checked for customers to be satisfied with your product. Sticking with the car example, seat belts would be a must-have feature. If you were interested in a car but realized it had no seat belts, you wouldn’t buy it for fear of getting hurt if you were in an accident. Your must-have need for a reasonable level of safety is not being met. That being said, if Car A had five seat belts and Car B had 100 seat belts, you wouldn’t say that Car B is 20 times better than Car A. Once you have one seat belt per passenger, your must-have need has been met. Delighters provide unexpected benefits that exceed customer expectations, resulting in very high customer satisfaction. The absence of a delighter doesn’t cause any dissatisfaction because customers aren’t expecting it. Returning again to cars, GPS navigation systems were a delighter when the first car models came out with that new technology in the mid-1990s. They meant no longer having to print out directions from your computer and no more getting lost. This feature fundamentally changed how people drove from point A to point B, resulting in customer delight. Going further back in time, cars did not always have built-in cup holders. Chrysler changed that when it introduced the minivan in the early 1980s, which had two functional cup holders sunk into the plastic of the dashboard. They were delighters because drivers no longer had to worry about spilling their beverages as they drove. Of course, cup holders are now ubiquitous in cars—and an increasing percentage of cars come with GPS navigation as a standard feature. That illustrates an important aspect of the Kano model: Needs migrate over time. Yesterday’s delighters become today’s performance features and tomorrow’s must-haves. Growing customer expectations and competition continuously raise the bar over time. This is another way of describing how, in the importance versus satisfaction framework, the upper value on the right side of the satisfaction scale gets redefined over time, moving yesterday’s solutions to the left. The Kano model also exhibits hierarchy, which I discussed earlier. For example, the fact that your product has a delighter doesn’t matter if it’s missing a must-have. A navigation system would be pointless in a car with no seat belts. You have to meet basic needs before you can get credit for performance features. And your product must be competitive on performance features before delighters matter.



66

The Lean Product Playbook

You can think of this as a three-tier pyramid with must-haves on the bottom, performance features just above that, and delighters at the top. You can apply the Kano model to gain clarity about the problem space. Think about the customer benefits that are relevant in your product category and classify them into the three categories of must-haves, performance benefits, and delighters. Evaluating competitive products and reading product reviews can help inform you as you create this framework. I will talk more about using the Kano model in conjunction with your competitive landscape when I discuss how to define your product’s value proposition in Chapter 5. PUTTING THE FRAMEWORKS TO USE

This chapter covers a lot of ground, and hopefully solidifies the problem space concept in your mind while illustrating the depth and richness with which you can explain customer benefits. Once you have identified the customer benefits that you could potentially address, you use the importance versus satisfaction framework to determine which ones allow you to create the most customer value. You want to pursue product opportunities in the upper left quadrant of importance versus satisfaction that have as large an opportunity score as possible. For the opportunity you decide to pursue, you will next break down the related benefits and decide which ones you will address with your product. You want to make sure your product delivers enough customer value and is better than other alternatives, which is the essence of product strategy. In the next chapter, I discuss how to define your value proposition using the Kano model, which you’ll use to specify your MVP candidate.

Chapter 5

Deﬁne Your Value Proposition (Step 3) The next step in the Lean Product Process is to define your product value proposition, which is the next layer in the Product-Market Fit Pyramid. At this point, you have identified several important customer needs that you could potentially address. Now you need to decide which ones your product will address. You want to do so deliberately and resist the temptation to tackle more needs than you should. A good product is designed with focus on the set of needs that are important and that make sense to address together. Swiss Army knives are incredibly useful, providing a set of tools to address a wide range of needs all in one convenient package. But at some point, as you add more and more tools, a Swiss Army knife gets wider, heavier, less usable, and less valuable. Focus is critical when defining a new product. You also don’t want to unnecessarily risk wasting resources with an initial product scope that is too large. You do not have perfect information about all those customer needs. There is quite a bit of uncertainty in both your hypotheses and in what you think you know. That’s why you want to start off by identifying the minimum viable product. Remember, all of your hypotheses about customer needs are hinged on an underlying assumption about your target customer. If you test your MVP and realize that your assumption was wrong, you will have to revisit your hypotheses about the relevant needs to address. Even if user testing verifies that you are heading in the right direction, you will learn new information that causes you to revise and add to your problem space hypotheses. And this will occur each time you iterate. You’ll never have “perfect information.” If you are following a good trajectory as you iterate, there will just be “less imperfect” information that you gather with increasing confidence.



68

The Lean Product Playbook

STRATEGY MEANS SAYING “NO”

This step in the Lean Product Process is about determining your product value proposition, which identifies the specific customer needs your product will address and articulates how it is better and different than the alternatives. When you specify the needs your product will address, you are also deciding the other benefits it won’t address. It can be difficult for some people to say, “No, our product won’t solve that problem”—but that is the essence of strategy. One of the best definitions I’ve heard of strategy is: “deciding what you’re not going to do.” Here’s what Steve Jobs had to say about saying “no”: People think focus means saying yes to the thing you’ve got to focus on. But that’s not what it means at all. It means saying no to the hundred other good ideas that there are. You have to pick carefully. I’m actually as proud of the things we haven’t done as the things I have done. Innovation is saying no to 1,000 things. So you need to start by selecting the customer needs you plan to address. I will show you how to use the Kano model as an organizing framework, with needs classified as must-haves, performance benefits, or delighters. Since you want to make sure your product will be different and better than the alternatives, you should be classifying needs in the context of your relevant competitors. And since your competitors are usually in the same product category that you are, the must-haves will likely be the same and there will probably be significant overlap among the performance benefits. Different products may have different delighters, though. It’s important to list the must-haves, since they are required. However, since all products in the category have to have them, they are not the core part of your value proposition. The core elements are the performance benefits on which you choose to compete and the unique delighters you plan to provide. VALUE PROPOSITIONS FOR SEARCH ENGINES

I’ll illustrate the concept of product value proposition by going back to the early days of Internet search engines. Back then, there were

Deﬁne Your Value Proposition (Step 3)

many search engines, and different products focused on different performance benefits. Some focused on having the largest number of pages in their index, which meant that they would return the largest number of results when a customer conducted a search. Some search engines focused on their index’s “freshness”: how quickly they added new pages and updated existing ones. Others focused on having the highest relevance of results. So there are at least three performance benefits on which early search engines competed: the number, the freshness, and the relevance of results. While early search engines also competed on other benefits, I’ll limit the discussion to these three for the sake of simplicity. At this early stage in the search engine market, the relative importance of each benefit wasn’t clear, and different companies chose different value propositions by focusing on different performance benefits. Over time, most search engines were indexing a large number of pages, so the number of results became less important. While users liked knowing that there were many results, they didn’t usually take the time to look beyond the first few pages. Similarly, most search engines were eventually able to add new pages relatively quickly so that their results were fresh. Therefore, relevance became the most important benefit and the one that offered the biggest opportunity for differentiation. Google was able to achieve much higher relevance than other search engines due to its unique PageRank algorithm. Because they were best at the benefit that mattered most—and had comparable or better performance on the other dimensions—Google won the search engine wars. Table 5.1 shows these three different value propositions. The table shows that Google focused on relevance, while search engine A focused on the number of search results, and search engine B focused on freshness.

TABLE 5.1

Value Propositions for Early Search Engines

Performance Benefit Number of search results Freshness of search results Relevance of search results

Google

Search Engine A

Search Engine B

Acceptable Acceptable Best

Best Acceptable Acceptable

Acceptable Best Acceptable



70

The Lean Product Playbook

What about delighters? Google Suggest, which automatically suggests search query matches, falls into this category. Instead of having to type their entire query—for example, “how many inches are in a yard”—users can start typing the first few letters or words—“how many . . . ”—and then a list of suggested queries appears. The user can then just click to select the query they have in mind from the list of suggestions, which saves them time—and the longer the phrase, the more time saved. Seeing the top related phrases also helps people who aren’t quite certain about their query, which results in reaching more relevant results more quickly. Google Instant Search is another delighter. This feature brings up search results as the user types, before the user hits the “enter” key (or selects an auto-suggested query). This feature also saves the user time. Google observed that people can read results much more quickly than they type, usually taking 300 milliseconds between keystrokes but only 30 milliseconds to scan results. Google has quantified the benefit of Instant Search at two to five seconds saved per search. Table 5.2 shows a more complete description of Google’s value proposition by adding these two delighters to the performance benefits previously discussed. Google Suggest and Google Instant Search are features, not benefits. I listed the feature names in the column for Google, but listed the benefit associated with each delighter in the leftmost benefits column: saving time entering a search query and saving time viewing search results, respectively. TABLE 5.2

Google’s Value Proposition with Delighters Google

Search Engine A

Search Engine B

Performance Benefits Number of results Freshness of results Relevance of results

Acceptable Acceptable Best

Best Acceptable Acceptable

Acceptable Best Acceptable

Delighters Save time entering query Save time viewing results

Yes (Google Suggest) Yes (Google Instant)

No

No

No

No

Deﬁne Your Value Proposition (Step 3)

Google isn’t the only search engine with delighters. When Bing sought to differentiate itself from other search engines, one innovation they came up with was the picture of the day. Each day, when you go to the Bing search page, the background image is a different, stunning photo. The photos are annotated with trivia or hints about the image, and users can try to figure out what the object or location of the photo is. The nice images don’t make searches any faster or improve the relevance of results, but they provide an interesting, pleasant surprise for users each day. NOT SO CUIL

One last search engine to discuss is Cuil (pronounced “cool”), which was launched in 2008. By this time, the search engine market was already in the upper right quadrant of the importance versus satisfaction framework. Search was very important, but users were pretty satisfied with the existing search engines, with Google having the largest market share (over 60 percent at the time). Given this situation, it would be critical for any new product entering the category to have a clear value proposition articulating how it would be better and different than the current solutions. It became clear from their marketing efforts that Cuil was focused on having the largest index. At launch, Cuil claimed an index of 120 billion web pages, which they estimated was three times the size of Google’s. They presented search results to users differently by displaying them in a magazine-like format with more photos. They also tried to differentiate on privacy by promising not to retain users’ search histories. So how did Cuil do? Not so well. Critics complained about slow response times and the low relevance of results. Search expert Danny Sullivan of Search Engine Watch criticized Cuil for focusing on index size rather than relevance. Two years after launching, Cuil shut down. The Cuil team’s hypotheses about what would create a successfully differentiated search engine didn’t pan out. In order to have a shot at beating the incumbent market leader, the value proposition for your new product would have to at least match them on the two important performance benefits of relevance and response time. I’m sure the Cuil team didn’t plan to have lower relevance or response time; that’s just



72

The Lean Product Playbook

what users encountered when they used the product. Even if Cuil had matched Google on those two performance benefits, they would have still needed a valued differentiator to gain significant market share. It’s unclear how valuable their intended differentiators of a larger index and increased privacy really were to customers. Table 5.3 provides a description of Cuil’s intended and actual value proposition compared to Google. Changing customer behavior is always difficult—especially in the upper right quadrant—and you need to create a certain amount of excess value to get customers to switch from a product they routinely use. The notion of needing to have “10×” better performance comes to mind again. TABLE 5.3

Cuil’s Value Proposition versus Google Cuil (intended)

Cuil (actual)

Performance Benefit

Google

Number of search results User privacy How well results are displayed Response time Relevance of search results

Good Best Didn’t matter Okay Best Didn’t matter Good Best Didn’t matter Good Comparable Poor Good Comparable Poor

BUILDING YOUR PRODUCT VALUE PROPOSITION

Now that the search engine examples have illustrated the concept, let’s discuss how you should create your product value proposition. Table 5.4 is a blank template for your value proposition. In the first column, you list the benefits—one per row, grouped by type. You want to include the must-haves, performance benefits, and delighters that are relevant to you and your competitors. You should have a column for each relevant competitor and a column for your product. The blank template lists two competitors. Competitors doesn’t just mean direct competitors: in the unlikely case that you don’t have any direct competitors, there should still be alternative solutions to your product that customers are currently using to meet their needs (remember how pen and paper was an alternative to TurboTax).

Deﬁne Your Value Proposition (Step 3)

TABLE 5.4

Product Value Proposition Template Competitor A Competitor B My Product

Must-Haves Must-have 1 Must-have 2 Must-have 3 Performance Benefits Performance benefit 1 Performance benefit 2 Performance benefit 3 Delighters Delighter 1 Delighter 2 Once you have established the benefits and competitors, you want to go through each row and score each of the competitors and your own product. If you are assessing an existing product, you can score it; if you are building a new product, you can list the scores you plan to achieve. The entries for must-haves should be “Yes.” For performance benefits, you should use whatever scale works best for you: A scale of “High,” “Medium,” and “Low” usually works well. For performance benefits that are amenable to numerical measurement, you can use the values for higher precision. For example, if you had a restaurant reservations application such as OpenTable, the number of restaurants in your system and the time it takes to make a reservation might be two performance benefits for which you could list numerical values. Delighters are typically unique, so just list each delighter on a separate row and then mark “Yes” where applicable. See Table 5.5 for an example of a completed value proposition. I’ve intentionally kept the benefits and competitors generic, so you can more easily envision a similar grid for your product. In this example, there are two existing competitors for the new product you plan to build. All three companies have “yes” for all the must-haves. Competitor A focuses on being the best at performance benefit 1, and Competitor B focuses on being the best at performance benefit 2. You plan to be the best at performance benefit 3. Perhaps you have identified a new customer segment that values performance benefit 3



74

The Lean Product Playbook

TABLE 5.5

Example of Completed Product Value Proposition Template Competitor A Competitor B My Product

Must-Haves Must-have 1 Must-have 2 Must-have 3 Performance Benefits Performance benefit 1 Performance benefit 2 Performance benefit 3 Delighters Delighter 1 Delighter 2

Yes Yes Yes

Yes Yes Yes

Yes Yes Yes

High Medium Low

Low High Medium

Medium Low High

Yes Yes

more than the others; or perhaps you have a new technology that allows you to achieve higher levels of satisfaction with performance benefit 3. Competitor A has delighter 1, and you have your own idea for a different delighter, delighter 2. Each product’s key differentiators are shown in bold. Completing this grid allows you to clearly articulate what benefits you plan to provide and how you’re aiming to be better than your competitors. The column for your product that includes your benefits and intended score for each one is your product value proposition. You have decided on the areas where you plan to play offense and those you are willing to cede as less important. Your key differentiators are the performance benefits where you plan to outperform your competitors as well as your unique delighters. Tying back to last chapter, these differentiators should ideally correspond to underserved benefits that have high importance and low satisfaction, where there are larger opportunities to create customer value. Few product teams ever complete such an exercise to clarify the value proposition for the product they are planning to build. So merely doing so will put you farther along than most companies. A clear value proposition decreases the likelihood that you are just launching a “me too” product, focuses your resources on what’s most important, and increases your chances of success.

Deﬁne Your Value Proposition (Step 3)

SKATING TO WHERE THE PUCK WILL BE

I’ve described the creation of your value proposition as a static snapshot in time. To be strategic, you want to ensure that you are projecting forward in time, anticipating the important trends in your market and what competitors are likely to do. This is especially important in many high-tech markets, which often have a rapid pace of change. As Wayne Gretzky said, “I skate to where the puck is going to be, not where it’s been.” THE FLIP VIDEO CAMERA

A great example related to this is the Flip video camera. Launched by Pure Digital in 2006 as the “Point and Shoot Video Camcorder,” many customers found the device superior to traditional camcorders because it was easier to use, more compact, and more affordable. The success of the Flip video camera led Cisco to acquire Pure Digital for $590 million in 2009. However, two years later, Cisco announced that to align its operations, it would exit aspects of its consumer businesses, including the Flip business. What happened? The Flip video camera achieved product-market fit for several years, but the competitive landscape changed swiftly. In 2009, Apple launched the iPhone 3GS, its first iPhone with built-in video recording. Compared to the Flip, smartphones offered an even more portable solution that avoided the need for a second device. Plus, their wireless connectivity allowed customers to post videos instantly without having to sync to a computer. Cisco corporate strategy aside, it became apparent over time that the smartphone would be the future of easy, portable video recording. PREDICTING THE FUTURE WITH VALUE PROPOSITIONS

Returning to your value proposition template, to predict the future, you can use separate columns for “now” and “later” for each competitor and your product. “Later” would be whatever length of time is the most relevant for your product strategy purposes. Table 5.6 shows an example of how you could do this. Table 5.6 has “now” and “in 1 year” columns for the competitor and your product. Competitor A is the best at performance benefit 1



76

The Lean Product Playbook

TABLE 5.6

Example of Product Value Proposition with Expected Future States Competitor A

Must-Haves Must-have 1 Must-have 2

Now

In 1 Year

Now

In 1 Year

Y Y

Y Y

Y Y

Y Y

High High Medium

Medium Low High

High Low High

Y

Y

Performance Benefits Performance benefit 1 High Performance benefit 2 Medium Performance benefit 3 Low Delighters Delighter 1 Delighter 2 Delighter 3 Delighter 4

My Product

Y

Y Y Y

right now, while your product is the best at performance benefit 3 right now. You anticipate that Competitor A will invest in improving performance benefit 3, but won’t match you. You also anticipate that Competitor A will invest to extend their lead in performance benefit 2. You have decided that performance benefit 2 is less important to your target market. Instead of investing there, you plan to ensure you stay the best at performance benefit 3 and close the gap on performance benefit 1. Turning to delighters, you each currently have your own unique delighter. Looking forward, you expect your competitor to launch delighter 3 and you plan to launch delighter 4. Analyzing your product strategy in this way ensures that you’re not just solving for current market conditions and reduces the risk that the path you’re heading down will end up being suboptimal in the future. Using the tools in this chapter should help you develop a clear understanding of your value proposition. You then need to determine the set of product features you plan to pursue to deliver on your value proposition. The next step in the Lean Product Process is to specify your MVP feature set.

Chapter 6

Specify Your Minimum Viable Product (MVP) Feature Set (Step 4) Now that you have a clear understanding of your value proposition, the next step in the Lean Product Process is to decide on the feature set for your minimum viable product (MVP) candidate. You are not going to start off by designing a new product that delivers on your full value proposition, since that would take too long and be too risky. For your MVP, you want to identify the minimum functionality required to validate that you are heading in the right direction. I call this an MVP candidate instead of an MVP because it is based on your hypotheses. You haven’t yet validated with customers that they agree that it is, in fact, a viable product. For each benefit in your product value proposition, you want to brainstorm as a team to come up with as many feature ideas as you can for how your product could deliver that benefit. You have done all this great thinking in the problem space and are now transitioning to solution space. At this point, brainstorming rules should apply. You should be practicing divergent thinking, which means trying to generate as many ideas as possible without any judgment or evaluation. There will be plenty of time later for convergent thinking, where you evaluate the ideas and decide which ones you think are the most promising. As your team brainstorms, try to build on each other’s suggestions and push each other to come up with even more creative and outlandish ideas. When you are done brainstorming, you want to capture all the ideas that your team generated, then organize them by the benefit that they deliver. Then, for each benefit, you want to review and prioritize the list of feature ideas. You can score each idea on expected customer value to determine a first-pass priority. The goal is to identify the top three to five features for each benefit. There is not much value in looking beyond those top features right now because things will change—a lot—after you show your prototype to customers.



78

The Lean Product Playbook

USER STORIES: FEATURES WITH BENEFITS

User stories (used in Agile development) are a great way to write your feature ideas to make sure that the corresponding customer benefit remains clear. A user story is a brief description of the benefit that the particular functionality should provide, including whom the benefit is for (the target customer), and why the customer wants the benefit. Well-written user stories usually follow the template: As a [type of user], I want to [do something], so that I can [desired benefit]. Here’s an example of a user story that follows this template: As a professional photographer, I want to easily upload pictures from my camera to my website, so that I can quickly show my clients their pictures. This template is a good start, but writing good user stories is an acquired skill. Agile thought leader Bill Wake created a set of guidelines for writing good user stories; to make them easier to remember, he uses the acronym INVEST: Independent: A good story should be independent of other stories. Stories shouldn’t overlap in concept and should be implementable in any order. ● Negotiable: A good story isn’t an explicit contract for features. The details for how a story’s benefit will be delivered should be open to discussion. ● Valuable: A good story needs to be valuable to the customer. ● Estimable: A good story is one whose scope can be reasonably estimated. ● Small: Good stories tend to be small in scope. Larger stories will have greater uncertainty, so you should break them down. ● Testable: A good story provides enough information to make it clear how to test that the story is “done” (called acceptance criteria). ●

Specify Your Minimum Viable Product (MVP) Feature Set (Step 4)

BREAKING FEATURES DOWN

Once you have written high-level user stories for your top features, the next step is to identify ways to break each of them down into smaller pieces of functionality—what I call “chunking.” The goal is to find ways to reduce scope and build only the most valuable pieces of each feature. When someone comes up with a feature idea, there are often creative ways to trim off less important pieces. I deliberately use the term “feature chunk” instead of feature to remind readers that you should not be working with items that are large in scope, but rather breaking such items down into smaller, atomic components. Let’s illustrate the idea of breaking a high-level user story down. Say you are working on a photo sharing application and start out with the user story: “As a user, I want to be able to easily share photos with my friends so that they can enjoy them.” One way to break this story down is by the various channels a customer can use to share photos: Facebook, Twitter, Pinterest, email, text message, and so forth. Each of those would be a distinct feature chunk or smaller scope user story. You may not need to build out all of these sharing channels for your MVP. Even if you decided that you did, it helps to break the story down to be more specific in your product definition, to enable more accurate scoping from development, and to allow you to explicitly prioritize the order in which you build the chunks. You might also limit scope by enabling the user to share only the photo and nothing else for your MVP. You may have ideas for additional functionality down the road such as adding an optional message to each photo or the ability to tag users in photos. Each of those would be a distinct feature chunk. SMALLER BATCH SIZES ARE BETTER

The tactic of breaking features down is consistent with the Lean manufacturing best practice of working in small batch sizes. When a product is being manufactured in a factory line, the batch size is the number of products being worked on together at the same time (at each step of the manufacturing process). The parallel for software development is the size of the features or user stories to be coded. Working in smaller batch sizes increases velocity because they



80

The Lean Product Playbook

enable faster feedback, which reduces risk and waste. If a developer spends a month at her computer developing a feature and then shows it to the product manager and designer, there is a greater chance that there will be a disconnect and that their feedback will require significant changes. If, instead, the developer shows her work to the product manager and designer every day or two, that prevents a large disconnect from occurring. The magnitude of feedback and course corrections will be much smaller and more manageable, resulting in less wasted work and higher productivity. This advice also applies to product managers and designers showing their work product (e.g., user stories and wireframes) to their teammates, too. The benefit of working in small batch sizes applies to customer feedback as well. The longer you work on a product without getting customer feedback, the more you risk a major disconnect that subsequently requires significant rework. SCOPING WITH STORY POINTS

Readers who have experience working with Agile development are probably familiar with the idea of breaking features down into smaller chunks. In many forms of Agile, once you’ve written the user stories, the team discusses each one and the developers estimate the amount of effort required. They often do so by using story points, a type of currency for estimating the relative size of different user stories. For example, a very small user story may take 1 point, while a medium scope user story may take 3 points, and a large scope user story may take 8 points. I discuss story points in more detail in Chapter 12. A good operating principle is that stories that are estimated to require a large number of points—above some maximum threshold value—need to be broken down into a set of smaller stories that are below the threshold value. You can think of a feature chunk as corresponding to a user story that has an acceptably small scope—an estimated number of story points that is below your maximum threshold. USING RETURN ON INVESTMENT TO PRIORITIZE

This is a good time to introduce the concept of return on investment (ROI). So far, you have only prioritized based on how much

Specify Your Minimum Viable Product (MVP) Feature Set (Step 4)

customer value you believe each feature will create. You haven’t yet taken into account the amount of resources required to build each feature. After you have finished chunking your feature ideas, you should perform a second-pass prioritization that accounts for both the value and the effort. A simple way to illustrate ROI is to imagine that I invest $100 in a stock. Several months later, it is worth $200 and I sell it. I have a return—or net profit—of $100, since $200 – $100 = $100. My investment was $100. So my ROI is $100 ÷ $100 = 1, or 100%. The formula for ROI is: ROI =

Final Value − Investment Return = Investment Investment

In the context of investing, both of the numbers you plug into the formula are monetary amounts (e.g., dollars). However, that’s usually not the case for ROI in the context of product development. When you are building a product or feature, the investment is usually the time that your development resources spend working on it, which you generally measure in units such as developer-weeks (one developer working for one week). It’s true that you could probably calculate an equivalent dollar amount, but people use units like developer-weeks because they are simpler and clearer. Similarly, in the context of developing a new product, “return” is often not a dollar amount. Instead, it is usually some relative measure of the amount of customer value you expect a certain feature to create. As long as you use an appropriate number scale to estimate customer value, the ROI calculations will work out fine. You need to use a “ratio scale,” which just means that the scores you use are in proportion to their value. For example, say you use a 0 to 10 scale for customer value and estimate scores for all your feature chunks. Using a ratio scale, if one feature chunk has a score of 10 and a second feature chunk has a score of 5, that should mean that the first feature would create double the amount of customer value as the second. Visualizing ROI

Figure 6.1—which shows the return, or customer value created, on the vertical axis and the investment, or development effort, on the horizontal axis—illustrates the concept of ROI. Let’s start off with



82

The Lean Product Playbook

FIGURE 6.1

Return on Investment

feature ideas A and B, both of which are estimated to create 6 units of customer value. However, idea B requires 4 developer-weeks to implement while idea A requires only 2 developer-weeks. The ROI for idea A is 6 ÷ 2 = 3, while the ROI for idea B is 6 ÷ 4 = 1.5. You should prioritize feature A above feature B. Sometimes two features offer about the same ROI. Look at feature ideas C and D. Idea C offers 4 units of customer value for 4 developer-weeks, for an ROI of 4 ÷ 4 = 1. Idea D offers 8 units of customer value for 8 developer-weeks, for an ROI of 8 ÷ 8 = 1. When you have two feature ideas with the same ROI, it’s best to prioritize the smaller scope idea higher because it takes less time to implement. You will deliver the value to customers more quickly—and by having the feature live sooner, you will get valuable customer feedback on it sooner, too. There are bad ideas out there too—such as idea F, which offers 2 units of customer value for 8 developer weeks, an ROI of 2 ÷ 8 =

Specify Your Minimum Viable Product (MVP) Feature Set (Step 4)

0.25. The large effort of a low-ROI idea is often recognized early as the team works on implementing it; however, they usually don’t realize the low customer value until after launch. Google Buzz and Google Wave are examples of low-ROI projects that each took a larger number of developer-hours to build but were shut down shortly after launching when customer reaction indicated that they had not created enough value. Good product teams strive to come up with ideas like idea G in Figure 6.1—the ones that create high customer value for low effort. Great product teams are able to take ideas like that, break them down into chunks, trim off less valuable pieces, and identify creative ways to deliver the customer value with less effort than initially scoped—indicated in the figure by moving idea G to the left. Some people struggle to create numerical estimates of customer value they feel are accurate. However, that isn’t something to worry about too much, since this isn’t about achieving decimal point precision. Even the effort estimates aren’t likely to be very precise, because you haven’t fully designed the features yet. You can’t expect developers to give you accurate estimates based on just a high-level description of a feature. The accuracy of the estimates should be proportional to the fidelity of the product definition. The main point of these calculations is less about figuring out actual ROI values and more about how they compare to each other. You want to focus on the highest ROI features first and avoid the lower ROI features. You can sort your list of feature chunks by estimated ROI to create a rank-ordered list—which is a good starting point to help decide which feature chunks should be part of the MVP candidate. However, sometimes you can’t just follow the strict rank order to create a “complete” MVP; you might need to skip down to include important features. The return in the ROI calculation can be a measure of value to your business instead of value to the customer. In those cases, you often have an estimated dollar amount that you can use for the return. This will be an expected gain in revenue or an expected decrease in cost. Let’s say, for example, that you have a live product and are trying to improve your conversion rate of free users to paid users. For a given improvement in the conversion rate, you should be able to estimate the expected improvement in revenue. Therefore, you should be able



84

The Lean Product Playbook

to associate an estimated dollar value with each improvement idea you have. Chapters 13 and 14 discuss how to maximize your ROI as you improve your business and product metrics. Approximating ROI

I’ve explained how to think about ROI rigorously, but you can also use this prioritization tool in a less rigorous manner. If you are struggling with creating numerical estimates of customer value or development effort, you can score each feature idea high, medium, or low on customer value and on effort. This will create a three-by-three grid, as shown in Figure 6.2. All of your feature ideas will fall into one of the nine buckets. Even though you won’t be calculating ROIs for each feature numerically, you can rank order the nine buckets based on ROI, as shown in the figure. So all the features in square number 1, which has the highest value and the lowest effort, would be higher priority than the features in square number 2, which would be higher priority than the features in square number 3, and so on. If you find yourself stuck because you’re not sure about the estimates for customer value and effort, just use your best guess to place each feature into one of the nine cells. These are just your

FIGURE 6.2

Approximate ROI

Specify Your Minimum Viable Product (MVP) Feature Set (Step 4)

starting hypotheses; you can—and likely will—change them as you learn and iterate. DECIDING ON YOUR MVP CANDIDATE

Once you are done chunking, scoping, and prioritizing, you can create a simple grid that lists the benefits from your value proposition and that lists, for each benefit, the top feature ideas broken into chunks. See Figure 6.3. In Figure 6.3 I’ve listed the top feature chunks for each benefit in priority order, with higher priority on the left. Rather than naming specific benefits or feature chunks, I’ve intentionally given them generic names so that you can more easily envision replacing them with what would be relevant for your product. “M1A” means feature chunk A for must-have 1. “P2B” means feature chunk B for performance benefit 2, and “D2C” means feature chunk C for delighter benefit 2. In filling out a similar grid for your product, you would instead use the specific labels for your benefits and feature chunks.

FIGURE 6.3

List of Prioritized Feature Chunks for Each Beneﬁt



86

The Lean Product Playbook

Once you have organized your list of feature chunks by benefit and prioritized them, it’s time to start making some tough decisions. You must decide on the minimum set of functionality that will resonate with your target customers. You are going to look down the leftmost column of feature chunks and determine which ones you think need to be in your MVP candidate. While doing so, you should refer to your product value proposition. To start with, your MVP candidate needs to have all the must-haves you’ve identified. After that, you should focus on the main performance benefit you’re planning to use to beat the competition. You should select the set of feature chunks for this benefit that you believe will provide enough for customers to see the difference in your product. Delighters are part of your differentiation, too. You should include your top delighter in your MVP candidate. That may not be necessary if you have a very large advantage on a performance benefit. The goal is to make sure that your MVP candidate includes something that customers find superior to others products and, ideally, unique. The feature chunks that you believe need to be in your MVP candidate will stay in the leftmost column, which you can label “v1,” as you see in Figure 6.4, while the others are pushed out to the right. You can create a preliminary product roadmap by continuing this process and creating columns for each future version with each column containing the feature chunks that you plan to add. Since you plan to be best at performance benefit 3, you are including the highest priority feature chunk, P3A, in your MVP candidate. You also plan to differentiate with differentiator 2, so you are including feature chunk D2A in your MVP candidate. Your MVP candidate also has the two must-haves. Looking out to your next version, v1.1, you plan to invest further in performance benefit 3 and delighter 2 with feature chunks P3B and D2B, respectively. In the version after that, v1.2, you plan to start addressing performance benefit 1 with the highest priority feature chunk P1A. I don’t recommend that you plan more than one or two minor versions ahead at the outset—since a lot of things are apt to change when you show your MVP candidate to customers for the first time. You’ll learn that some of your hypotheses weren’t quite right and will come up with new ones. You may end up changing your mind on which benefit is most important or come up with ideas for new features to

Specify Your Minimum Viable Product (MVP) Feature Set (Step 4)

FIGURE 6.4

Deciding Which Feature Chunks Are in Your MVP Candidate

address the same benefits. So if you’ve made tentative plans beyond your MVP, you must be prepared to throw them out the window and come up with new plans based on what you learn from customers. The way I’ve drawn Figure 6.4, there is, at most, only one feature chunk for a given benefit in your MVP candidate. However, it may be the case that you need two or three feature chunks for a given benefit, depending on your situation and how small your chunks are. The idea is still the same: to pick which feature chunks need to be in that leftmost column, which corresponds to your MVP candidate. Let’s take a step back and reflect. At this point in the Lean Product Process, you have done a fair bit of work. You have: Formed hypotheses about your target customers ● Formed hypotheses about their underserved needs ● Articulated the value proposition you plan to pursue so that your product is better and different ●



88

The Lean Product Playbook

Identified the top feature ideas you believe will address those needs and broken them down into smaller chunks ● Prioritized those feature chunks based on ROI ● Selected a set of those feature chunks for your MVP candidate, which you hypothesize customers will find valuable ●

You have done a lot of rigorous thinking to get this point, but your MVP is still just a candidate, a bundle of interrelated hypotheses. You need to get customer feedback on your MVP candidate to test those hypotheses. But before you can test, you need to create a solution-space representation of your MVP candidate that you can show to customers, which is the next step in the Lean Product Process.

Chapter 7


Once you have specified the feature set for your MVP candidate, you’ll want to test it with customers. In order to do that, you need to create a user experience (UX) that you can show to customers, which is the top layer of the Product-Market Fit Pyramid. The goal is to build a prototype that lets you test your hypotheses. As discussed in Chapter 1, I intentionally use the broad term MVP “prototype” to capture the wide range of items you can test with customers to gain learning. While the first “prototype” you test could be your live MVP, you can gain faster learning with fewer resources by testing your hypotheses before you build your MVP. Also, as discussed in Chapter 1, even though I’m using the term MVP, the Lean Product Process applies even when you are not building an entire product (e.g., adding a new feature or improving an existing feature). The type of prototype you should create depends on the type of test you want to conduct with customers. WHAT IS (AND ISN’T) AN MVP?

There has been spirited debate over what qualifies as an MVP. Some people argue vehemently that a landing page is a valid MVP. Others say it isn’t, insisting that an MVP must be a real, working product or at least an interactive prototype. The way I resolve this dichotomy is to realize that these are all methods to test the hypotheses behind your MVP. By using the term “MVP tests” instead of MVP, the debate goes away. This allows more precise terminology by reserving the use of MVP for actual products. Many people misinterpret the term MVP by placing too much emphasis on the word minimum. They use this as an excuse to build a partial MVP that has too little functionality to be considered viable by a customer. Others use “minimum” to rationalize a shoddy user experience or a buggy product. While it’s true that an MVP is


90

The Lean Product Playbook

FIGURE 7.1

Building an MVP

deliberately limited in scope relative to your entire value proposition, what you release to customers has to be above a certain bar in order to create value for them. The diagram in Figure 7.1 illustrates the difference between this incorrect way of interpreting MVP and the correct interpretation. I’ve adapted this figure from one created by talented UX designer Jussi Pasanen of Volkside, http://volkside.com, who gives his acknowledgements to Aarron Walter, Ben Tollady, and Ben Rowe. Similar to my hierarchy of web user needs (Figure 4.2), this figure separates the distinct aspects of a product. In this case, a pyramid of four hierarchical layers is used to describe a product’s attributes: functional, reliable, usable, and delightful. The pyramid on the left illustrates the misconception that an MVP is just a product with limited functionality, and that reliability, usability, and delight can be ignored. Instead, the pyramid on the right shows that while an MVP has limited functionality, it should be “complete” by addressing those three higher-level attributes. MVP TESTS

Getting back to MVP tests, there are many different kinds you can use, and you may have heard some of their names: “Wizard of Oz,” “smoke test,” and “fake door.” This chapter will explain the different types of MVP tests and help you decide which one will

Create Your MVP Prototype (Step 5)

be most beneficial for your situation. By the way, rather than using the word “test” over and over, I will also use the word “validate.” Some people like “validate” because it implies that there is an underlying hypothesis being tested. Other people cringe when they hear the word “validate” because to them it presumes that your test will be successful. I use “validate” synonymously with “test your hypothesis,” with no presumption about the results. While the term MVP test provides a convenient umbrella for all the various tests, there are key differences among them. There are two main ways to categorize them. Product versus Marketing MVP Tests

The first way you can categorize MVP tests is by whether they are aimed at testing your product or your marketing. A landing page test that measures what percentage of prospective customers click the “sign up” button and leave their email addresses is focused on marketing, because there isn’t any product functionality the customer can actually use. You’re simply describing the functionality to prospective customers to see how compelling they find your description. In contrast, MVP tests used to validate your product will involve showing prospective customers functionality to solicit their feedback on it. You may be showing them a live beta product or just low fidelity wireframes to assess product-market fit. With either a product or a marketing MVP test, you care about how compelling customers find what you’re showing them; but the learning goal of each differs. Marketing tests can provide valuable learning, but they’re not an actual product that creates customer value. At some point, you need to test a prototype of your MVP candidate. If through testing and iteration you reach a point where you feel that you have validated product-market fit with enough confidence, you would then proceed with building an actual MVP. Quantitative versus Qualitative MVP Tests

The second dimension on which MVP tests differ is whether they are qualitative or quantitative. Qualitative means that you are talking with customers directly, usually in small numbers that don’t yield



92

The Lean Product Playbook

statistical significance. Here, you care about the detailed information you learn from each individual test. You may try to discern patterns across the results, but statistical significance isn’t a primary concern. If, for example, you conducted one-on-one feedback sessions with 12 prospective customers to solicit their feedback on a mockup of your landing page, then that would be qualitative research. Quantitative research involves conducting the test at scale with a large number of customers. You don’t care as much about any individual result and are instead interested in the aggregate results. If you launched two versions of your landing page and directed thousands of customers to each one to see which one had the higher conversion rate, then that would be a quantitative test. Quantitative tests are good for learning “what” and “how many”: what actions customers took and how many customers took an action (e.g., clicked on the “sign up” button). But quantitative tests will not tell you why they chose to do so or why the other customers chose not to do so. In contrast, qualitative tests are good for learning “why”: the reasons behind different customers’ decisions to take an action or not. Both kinds of tests are valuable and complement one another. I’ve seen many teams rely on one type of testing too heavily, usually quantitative. You must be mindful of what is most important to learn for your situation and choose the type of test accordingly. In general, when you are first starting to develop your product or marketing materials, it is most beneficial to start with qualitative tests to gain some initial understanding. If you jump straight into quantitative tests without doing any qualitative tests, they usually don’t perform as well—and even if they do, you won’t know why. It is common to see product teams alternate between rounds of qualitative testing and quantitative testing as they learn and iterate. Chapter 9 provides additional advice on how to qualitatively test your MVP with customers. THE MATRIX OF MVP TESTS

I created the two-by-two matrix in Figure 7.2 to list and categorize the different MVP tests based on product versus marketing and qualitative versus quantitative. In each of the four quadrants, I’ve listed the MVP tests that pertain to that combination of attributes. I’ll walk

Create Your MVP Prototype (Step 5)

FIGURE 7.2

MVP Tests Categorized by Type

through each of the four quadrants in the figure and describe the MVP tests in each one. QUALITATIVE MARKETING MVP TESTS

Let’s start off with qualitative marketing tests in the upper left quadrant. Clearly, there are many different ways to market your product. Rather than create a long list of tests for each type, I’ve lumped them all under “marketing materials.” These types of tests involve showing customers your marketing materials and soliciting their feedback. Marketing materials include anything you would want to put in front of a customer: a landing page, a video, an advertisement, an email, and so forth. This test is an attempt to understand how compelling they find this marketing material and why. You are not getting feedback on the product itself, but rather how you talk about and explain the product. These tests can help you understand which benefits resonate with customers and their reactions to different ways of talking about the benefits and describing your product. Conversations like these help



94

The Lean Product Playbook

you see how compelling they find your product value proposition. You can even show customers your competitor’s marketing materials to learn what they’ve explained well and what they haven’t and to test your differentiation. One good way to test your overall messaging is the five-second test. The idea is to show customers your home page or landing page for just five seconds and then ask them to tell you what they remember and what they liked. Because customers make snap judgements about products all the time, this can be a good way to see how well your messaging conveys what your product does and why someone would want to use it. QUANTITATIVE MARKETING MVP TESTS

You can use quantitative marketing tests to validate demand for your product. You can also use them to optimize the acquisition of prospects and the conversion of prospects to customers. Because these tests capture user behavior, they can provide significant learning with large sample sizes. Landing Page/Smoke Test

One of the most popular tests is the landing page or smoke test. In this test, you create a live web page to which you direct traffic. The landing page describes the product you plan to build and asks customers to express some level of interest, which is usually a “sign up” button or a link to a “plans and pricing” page. It’s also called a smoke test because there is no real product for customers to use yet. Instead, there is usually a “coming soon” page that thanks the customer for their interest and asks for their email address or other contact information. The key metric that these tests measure is the conversion rate: the percentage of visitors to your landing page that clicked on the button to convert from a prospect to a customer. For example, if you directed 1,000 prospective customers to a landing page with a “sign up” button, and 250 of them clicked it, then your conversion rate would be 25 percent. The conversion rate will be influenced by which benefits you choose and how well you describe the benefits and your product. Even if you’ve selected benefits that customers find compelling, good

Create Your MVP Prototype (Step 5)

visual design and copywriting are important for landing pages to be successful. Your development team can create and evaluate landing pages using your existing web technology stack and analytics package. However, handy tools like Optimizely and Unbounce make landing page testing and optimization faster and easier with less development effort. Landing Page MVP Example: Buffer

Buffer provides a good example of a landing page MVP test. Buffer is a product that helps you post to Twitter more consistently by letting you specify tweets you want to send and schedule them to be posted at preset times. Buffer’s CEO and cofounder Joel Gascoigne authored a blog post about how he decided to start out with a landing page MVP test (https://blog.bufferapp.com/idea-to-paying-customers-in7-weeks-how-we-did-it). Gascoigne writes how he approached Buffer differently from his previous startup: “I started coding Buffer before I’d tested the viability of the business. As soon as I realized that, I stopped, took a deep breath and told myself: do it the right way this time. It was time to test whether people wanted this product.” Buffer’s first home page described what the product’s value proposition was with a headline and three bullet points. It included a “plans and pricing” button, which was the only thing that visitors could click. Upon doing so, they were taken to a page that said “You caught us before we’re ready.” Then they could enter their email address to be notified when the product launched. As Gascoigne explains, “The aim of this two-page MVP was to check whether people would even consider using the app. I simply tweeted the link and asked people what they thought of the idea. After a few people used it to give me their email and I got some useful feedback via email and Twitter, I considered it validated.” Gascoigne felt that he had confirmed that people wanted the product. The next step was to test if people would be willing to pay for it. So he inserted an additional page (before the email form) that described three different product levels: free, $5 per month, and $20 per month. This allowed him to see how many clicks each plan received in addition to how many people submitted their email



96

The Lean Product Playbook

address. The results were positive. Despite the extra click required, people were still getting to the email form and leaving their email addresses, and some people were clicking on the paid plans. It’s worth noting that, at this point, no one had actually paid any money to use Buffer (because they couldn’t yet). But Gascoigne felt that he had enough validation of product-market fit from these two simple tests to confidently move forward with his product idea. Explainer Video

The explainer video is really just a variant of the landing page test that relies on a video to explain the product. You judge a video’s effectiveness by the conversion rate that it drives, for example, on a sign-up page. This type of test is particularly useful for products that are difficult to explain with just words. Cloud storage service Dropbox conducted one of the most well-known explainer video MVP tests. Founder Drew Houston found that people just didn’t understand why Dropbox was better when he tried to explain how his company’s unique approach to file synchronization made it different from all the other cloud-based file storage products in the market at the time. So he created a video that showed and explained how Dropbox works. The video resonated with customers as solving a real pain point they had managing and sharing their files across multiple devices, driving a large number of sign-ups for Dropbox’s private beta waiting list. Ad Campaign

In order to test a landing page, you need to drive traffic to it somehow—and one way to do so is with advertising campaigns. For example, Google AdWords displays short text ads when customers conduct searches. You can experiment with different search terms and ad copy to try to increase your clickthrough rate, giving you quantitative feedback on what words and phrases customers find most compelling. You can also use display ad campaigns to test different messaging and imagery. Since Facebook ads let you target on demographics, they can offer a good way to test your hypotheses about your target market. Because the ads are usually small, they

Create Your MVP Prototype (Step 5)

often don’t give you enough room to convey your entire value proposition and are instead limited to a tagline. As a result, this type of test is most useful for optimizing your customer acquisition efforts and not validating product-market fit. For example, if you were thinking of building a site for people to find jobs, you might run three ad campaigns, each with a different tagline: “We match you with your perfect dream job,” “We have the most job listings anywhere,” and “We offer the fastest way to scan job listings.” You would then compare the clickthrough rates of the three ad campaigns to see which performed best. Marketing A/B Testing

A/B testing, also called split testing, is a quantitative technique where you test two alternative designs simultaneously to compare how they perform on a key metric, such as conversion rate. You can use A/B testing to try different versions of your marketing materials to see which performs better. For example, you could test two different versions of your landing page—with different messaging, pricing, images, colors, or other design elements—to see which converts better. You can also A/B test most other online marketing materials such as advertising, videos, and emails. Your tests can have more than two alternatives, for example, an A/B/C test. In a true A/B test, the different versions your are testing run in parallel at the same time, for example, 50 percent of traffic to version A and 50 percent of traffic to version B. It’s less desirable to test versions sequentially (100 percent of traffic to version A for a while, followed by 100 percent of traffic to version B for a while). By running alternatives concurrently, you avoid the risk of differences in extraneous factors between the two time periods—such as seasonality or level of promotion—skewing your results. Often after doing several A/B tests to optimize some aspect of your product, you will have identified the best performing option, which is called the champion. From time to time, you may test new alternatives to see if they can beat the champion. An important aspect of this is statistical significance, which is determined by the difference in performance and the sample size. There are formulas and online tools to help you calculate the statistical confidence level for your specific test.



98

The Lean Product Playbook

However, it’s important to know that statistical significance is higher for larger differences in performance and for larger sample sizes. If your sample size is too low, you won’t achieve statistically significant results. If you have two alternatives with very similar performance, it may take a very large sample size to discern any statistically significant difference. Popular A/B testing tools include Optimizely, Unbounce, KISSmetrics, Visual Website Optimizer, and Google Content Experiments (part of Google Analytics). These A/B testing tools let you specify several variations and then randomly distribute traffic among the variations. They track the results for the conversion action you care about and show you how each variation is performing, along with the corresponding statistical confidence level. Multivariate testing is similar to A/B testing, but instead of testing different versions of a page, you test variations of page elements. Each page element that you are changing is a variable. Let’s say you were working on a landing page and had three different ideas for possible headlines and three different ideas for the main image on the page. A multivariate test would try out all nine possible combinations of headline and image to determine which performs best. Crowdfunding

Crowdfunding platforms like Kickstarter and Indiegogo can be a great way to test whether or not people are willing to pay for your product and to quantify demand. These platforms let people who want to make and sell a product promote it and accept money from customers who want to buy the product when it comes out. You can set a fundraising threshold for your product and only commit to build it if you reach that limit. This approach where customers pay you before you start building your product is consistent with Lean principles, in that it can help you eliminate the uncertainty of whether or not anyone will pay for your product. The Pebble Watch is a Kickstarter success story. After founder Eric Migicovsky took the startup through the Y Combinator incubator, he was unable to raise enough funding from venture capital firms. He launched a Kickstarter campaign with an initial funding goal of $100,000. Customers could pay $115 for a Pebble Watch when they

Create Your MVP Prototype (Step 5)

launched, basically preordering at a discount from the full price of $150. The project met its initial goal in two hours and continued to grow. Pebble ended their Kickstarter fundraising a little over a month later, after over 68,000 people had pledged over $10 million. Kickstarter has become an exciting new funding channel for startups. Virtual reality headset startup Oculus Rift started off with an initial $250,000 fundraising goal in August 2012. In one month, they raised just under $2.5 million, almost 10 times their target amount. Facebook acquired Oculus Rift less than two years later for $2 billion. Entrepreneurs who are full-time employees but have a startup idea they want to pursue can use crowdfunding as a way to mitigate risk before taking the plunge. Crowdfunding is an especially good fit for selling consumer products, since it provides a direct-to-consumer e-commerce sales channel. However, because there is no product for people to try out, you have to provide a rich description of both the offering and its benefits. Many campaigns have high-quality videos and extensive FAQs. You should also market to prospective customers in other ways, such as social media, in order to make them aware of your product’s campaign. Crowdfunding sites can be a great way to connect with early adopters for your product. You can readily engage them in discussions about their needs and preferences, and they can be a source of good ideas for improving your product. Successful crowdfunding pages become an active communications hub between the startup and its customers. QUALITATIVE PRODUCT MVP TESTS

I’ve covered the marketing MVP tests that can help ensure your messaging is on point with customers and help quantify the expected values of marketing metrics such as conversion rate. Product tests help ensure that customers see value in your actual product. When developing a new product, a redesigned product, or a new feature, qualitative product tests are the most valuable way to assess and improve your product-market fit. There are two fundamentally distinct times when you can conduct qualitative product tests: before you’ve built your product and after you’ve built it. Both are valuable.



100

The Lean Product Playbook

You can test your product’s design with customers before you build your product. Typical product design deliverables include wireframes, mockups, and interactive prototypes—all of which are representations of what your product will be like without being the actual product itself. You’ll want to validate your design before you start coding your product to reduce waste. It’s usually much faster and less costly to make changes to your designs than your code. I’ll discuss this further, but design artifacts can vary in fidelity—that is, how closely they represent the real product. After you build your product, you can test it with users—which has the advantage that the fidelity of what you’re testing is 100 percent. As a result, you may learn things that you weren’t able to observe in testing your design artifacts. For example, you may get feedback on a mockup of your web page about how well the information is laid out, how clear the copy is, and how compelling the visual design is. However, you wouldn’t catch things like the fact that the web page is very slow to load or that it doesn’t work properly in a certain browser. There are no inherent negatives to live product testing; however, if you wait until your product is live to test it with customers, you are unnecessarily taking a big risk. You want to reduce this risk by showing customers design deliverables earlier in the process to ensure that customers will value the product that your developers are going to spend their valuable time building. You can show customers a variety of design artifacts to solicit feedback. Figure 7.3 classifies these artifacts by their level of fidelity on

FIGURE 7.3

Design Artifacts by Fidelity and Interactivity

Create Your MVP Prototype (Step 5)

the horizontal axis and their level of interactivity on the vertical axis. Again fidelity refers to how closely the artifact looks like the final product, whereas interactivity means the degree to which the customer can interact with the artifact relative to a live, working product. See Figure 7.4 for an illustration of low versus high fidelity. Both of the design deliverables shown in the figure are for the same product, the iOS app Pointedly (https://itunes.apple.com/us/app/pointedlysimple-score-keeper/id933257819). Built by talented UX designer Ben Norris, the app makes it easy to keep score when you’re playing games (like Scrabble), instead of needing pen and paper. On the left side of the figure is a low-fidelity wireframe. It doesn’t use any colors

FIGURE 7.4

Low Fidelity Wireframe versus High Fidelity Mockup



102

The Lean Product Playbook

(it’s grayscale) and only shows the screen elements and their locations without any visual design details. The high-fidelity mockup on the right is meant to look much more like the actual product (although the color image appears in grayscale in the printed version of this book). The app’s user interface elements have been given a visual design using specific colors, fonts, and graphics. Hand Sketches

In the bottom left of Figure 7.3 is the hand sketch, which has the lowest fidelity and the lowest interactivity. Hand sketches are a great way to start visualizing your ideas, especially to share and discuss them with your teammates and other internal stakeholders. Sketching on a whiteboard or paper allows you to iterate your design quickly. I’m a big fan of whiteboarding, especially in the early stage of design. In fact, the whiteboard could very well be the ultimate Lean tool because it enables teams to iterate their ideas so rapidly. As useful as hand sketches are for internal use, they are too low fidelity to show to customers for feedback (which is why they do not appear in the MVP test matrix). Wireframes

The design artifact with the next level up in fidelity is the wireframe, which is a low to medium fidelity representation of a product that gives a sense of the product’s components and how they are arranged. Wireframes are not “pixel perfect”; rather, they show relative size and position. They are usually devoid of any visual design details such as colors, images, and fonts. Instead, they are often grayscale and use placeholders for images to avoid distracting reviewers with visual design elements. Some wireframes may also use a placeholder for text—such as “lorem ipsum”—in place of the final copy, although that is becoming less commonplace. I recommend using real copy from the start, even if it is just a preliminary draft, to identify potential layout issues early. Wireframes can be drawn by hand, but are usually digital artifacts created using a software application—either a general-purpose design application or a special-purpose wireframing application. Illustrator

Create Your MVP Prototype (Step 5)

and Sketch are graphic design applications used mainly by designers. OmniGraffle and Visio are more general-purpose tools. Some nondesigners create wireframes in PowerPoint or Keynote. I recommend using more capable tools specifically designed for wireframing, such as Balsamiq, Axure, and UXPin for web products. Because mobile products—whether native applications or mobile websites—tend to have different user experience elements than traditional web applications, tools that specialize in mobile wireframing have emerged. I recommend the mobile wireframing tools Flinto and Marvel for designers. POP and Dapp are easy to use mobile wireframing tools for nondesigners. There are new, more powerful wireframing tools coming out all the time. If it’s been a while since you’ve tried out the latest ones, I’d encourage you to do so. I’ve seen too many designers and product managers miss out on the next big thing because they keep clinging to an older tool with which they’re comfortable and proficient. Productivity is obviously important, and switching tools does require a time investment. But adopting a new tool can be worthwhile because of the new capabilities it adds and the higher level of productivity it enables once you master it. Some signs that you should probably evaluate a new wireframing tool include: You are drawing basic page elements from scratch using shapes and lines instead of leveraging a library of predefined widgets ● The wireframes you’re creating aren’t clickable or tappable ● It’s hard to share your wireframes with other people ● You’re creating wireframes for mobile screens with a tool that is not optimized for mobile ● It takes you longer than you’d like to create wireframes ● You’re not creating wireframes at all ●

Today’s tools make it so easy and affordable to wireframe that there’s really no excuse for anyone involved with developing a product not to. If you have one or more interaction designers on your team, they may be the ones doing most of the wireframing. But it is still a valuable skill to be able to quickly bang out a visualization of an idea you have to share with others. And if you don’t have a dedicated wireframer on your team, you may have to step up.



104

The Lean Product Playbook

A common feature of modern wireframing tools is a widget library containing most common user interface elements that you would want to use. For example, if you wanted to wireframe an iOS app, you shouldn’t draw the standard iOS controls by hand or use generic controls. Your tool should have a set of the common iOS user interface elements available in its library. Since the wireframe shows a representation of the user interface, product teams often find it useful to add explanatory notes. Such annotations could convey important details, such as the list of options that should be displayed in a dropdown menu, the maximum number of characters a user should be allowed to type into a particular form field, or the wording for a particular error message. Such annotated wireframes can be powerful design tools, containing much of the product specification required for development. Obviously, the version of the wireframes shown to the user should not display these annotations. Until several years ago, wireframes were usually static—that is, not clickable or tappable. But modern wireframing tools have made it very easy to create clickable wireframes that let you connect a set of wireframes for different pages into a logical navigation flow that the user can experience. You usually only make selected user interface controls clickable—the ones that are pertinent to the design and what you want to learn from the specific test. Such a user scenario is called the “happy path”—the one that you intend the user to follow through the user experience you have designed. Clickable and tappable wireframes have essentially replaced static wireframes as the norm. A static wireframe test requires the user to tell you, “I’d click this button,” after which you would show the next static wireframe. Clickable wireframes create a more immersive experience for the user—one where they can independently explore and navigate your product. Plus, because clickable wireframes are usually tested on the device on which the product will be used (e.g., computer, tablet, or phone), the experience feels more realistic. Mockups

As shown in Figure 7.3, the design artifact with the next level up in fidelity is the mockup, which looks much more like the final product than wireframes. Mockups convey visual design details such as

Create Your MVP Prototype (Step 5)

colors, fonts, and images. Some are intended to be “pixel perfect” while others may only represent the approximate size and position of design elements. Mockups are also sometimes referred to as “comps,” and are usually created with a graphic design application such as Illustrator, Photoshop, or Sketch. As with wireframes, mockups can be static or clickable. The output of a graphical design application is usually a static image file like a JPG, GIF, or PNG, which is not inherently clickable. To create a set of clickable mockups, those images are combined using another application that lets you specify “hot spots,” which are click or tap targets that navigate from one mockup to another. The prototyping web application InVision does this well. This tool lets you upload your images and link them together via clickable hot spots. Balsamiq also lets you do this. As with clickable wireframes, clickable mockups create a multiple page or screen user flow that the customer can experience. But instead of seeing low to medium fidelity wireframes, the customer sees high fidelity mockups. Again, the happy path through the user experience is usually enabled while other navigation flows are not. Because clickable mockups can look and feel so close to the real application, they can yield very valuable feedback from users. Some teams that are adept at creating clickable mockups start their user testing of design artifacts there. Those teams often create wireframes before the mockups, but don’t solicit user feedback on the wireframes. Interactive Prototype

The next step up in fidelity and interactivity from clickable mockups is an interactive prototype. The word prototype by itself can be used to describe any clickable design artifact; it simply denotes that it is either not a fully functional product or only a facsimile of a product. Interactive prototypes provide a level of interaction that goes beyond that of just clickable mockups. For example, an interactive prototype might include many types of functioning user interface controls, such as drop-down menus, hover effects, input forms, and audio or video players. Interactive prototypes can be created with a variety of developer tools. Web prototypes are usually built with HTML, CSS, and



106

The Lean Product Playbook

JavaScript. Popular front-end frameworks such as jQuery and Bootstrap are often used for more rapid development. Prototypes can also be built using Ruby on Rails or other rapid development frameworks if you want to have some lightweight server-side functionality, too. Powerful tools such as Axure—which lets you export your prototype to HTML, CSS, and JavaScript—enable you to create interactive prototypes without any coding. Mobile prototypes can be built in HTML or in native code, for example, iOS or Android. Wizard of Oz and Concierge MVPs

None of the qualitative product tests discussed so far have been a live, working product or service. The Wizard of Oz MVP and concierge MVP allow you to actually test your live product or service; but instead of the final version, you are using manual workarounds. I call MVPs like this “manual hack” MVPs, since they are inefficient and not meant for the long run. The idea behind a concierge MVP is to be very involved with a small number of early customers to really understand your target market, their needs and preferences, and how to tailor your product to best meet these. Doing so helps you validate what your product or service should do before you actually build it. Concierge MVPs work best with services, especially those with processes that require a fair amount of interaction with and input from the customer. Concierge MVP Example: Airbnb

For example, the lodging rental site Airbnb used a concierge MVP to grow their service. In a talk at South By Southwest (SXSW), Director of Product Joe Zadeh described how the Airbnb team hypothesized that property listings with professional photos would get more business. So they manually recruited hosts to offer them professional photography, and recruited photographers to match up with the hosts. After scheduling the photo shoot and taking the pictures, the photographers would upload their pictures to Dropbox, and Airbnb employees would upload them to the associated property listing. Airbnb would then pay the photographers. The Airbnb team saw

Create Your MVP Prototype (Step 5)

that their hypothesis was true: listings with professional photographs had two to three times more bookings than the market average. After proving their hypothesis, Airbnb replaced most of their manual process steps with automated steps. So, instead of being done by a human, the Airbnb system invites hosts to take advantage of professional photography, assigns photographers to hosts, and updates each listing with the photos. Airbnb mitigated risk and potential waste by validating their hypothesis before investing the resources required to build an automated solution. The Wizard of Oz MVP is similar to the concierge MVP in that you perform certain steps manually in the short term. However, the difference is that it’s not obvious to the customer that you are performing these steps manually; like in the movie The Wizard of Oz, they are hidden behind a curtain. The Wizard of Oz MVP appears to users as the real live product. The goal is to validate the manual steps that are required before making the investment to build out an automated solution. Live Product

You can also test live product with customers. Ideally, before building your MVP, you will have validated its design by testing increasingly higher fidelity artifacts with customers. When you iterate to a point where you feel that you have validated product-market fit with enough confidence, you would proceed with building a real MVP. Chapter 12 discusses how to build your product using Agile development. Even if you’ve tested design artifacts along the way, it’s a good idea to test your actual MVP once it’s built. Changes often occur between the design and development phases. Because live product is the highest fidelity possible, you may learn new things from customers that you didn’t uncover during lower fidelity tests; for example, how a web product looks and behaves on different screen sizes and browsers. You can test your live product in a moderated or unmoderated fashion. In moderated testing, you are present with the customer as they use your product, whereas the customer is alone with unmoderated testing (and the test is recorded so you can watch it later).



108

The Lean Product Playbook

Moderated tests can be conducted either in-person or remotely using screen sharing software such as Skype, WebEx, or join.me. I discuss this further in Chapter 9.

QUANTITATIVE PRODUCT MVP TESTS

Once you have a live product with a meaningful amount of usage, you can conduct quantitative product tests. Unlike qualitative product tests where you are asking smaller numbers of customers for their opinions, quantitative product tests measure the customers who are actually using your product (usually with large sample sizes).

Fake Door/404 Page

The fake door or 404 page test is a good way to validate demand for a new feature that you are considering building. The idea is to include a link or button for the new feature and see what percentage of customers click on it. This lets you gauge whether customers actually want the feature before you spend the resources to build it. Since you haven’t built the feature yet, the customers usually see a page thanking them for their interest and explaining that the feature is not built yet when they click on the link or button. You can also add a form asking the customer to share why they would find this feature valuable. The extreme case of this type of test is to not even bother building the destination page, since you can technically track the clicks without it. In that case, clicking on the link or button goes to the website’s generic 404 page (404 is the HTTP error code for “page not found”). The gaming company Zynga, which has a strong reputation of quantitative product testing, uses fake buttons often. In a talk he gave at the Stanford Technology Ventures Program, Zynga cofounder Mark Pincus described how his team would test new game ideas by coming up with a five-word pitch for each one. They would then publish the pitch as a promotional link in their live games for a short period of time to see how much interest it generated from customers. Of course, you want to be mindful of how long and how often you run fake door tests to avoid making your customers unhappy. Rather than leaving fake door tests live for an extended period of time, it’s

Create Your MVP Prototype (Step 5)

best to run them only for the amount of time required to achieve the sample size you need and then take them down. Product Analytics and A/B Tests

Product analytics aren’t a test per se, but they can give you insights into how your customers are actually using your product. For example, you can see which features they use the most and where they spend most of their time. When you roll out product changes, you can look for changes in key product metrics to test your hypotheses. Product analytics also form the foundation for A/B testing, because they are used to calculate the results of the tests. Leading product analytics solutions include Google Analytics, KISSmetrics, Mixpanel, and Flurry. Product A/B tests or split tests are used to compare the performance of two alternative user experiences (A and B) in your product. For example, say you developed a new registration flow for your web application that you think will have a higher completion rate than your current registration flow. Rather than just replacing the old flow with the new flow, you could A/B test the flows: randomly direct 50 percent of traffic each to the old flow (A) and the new flow (B) and compare the completion rates. There are several popular product A/B testing tools available such as Optimizely, KISSmetrics, Visual Website Optimizer, and Google Content Experiments (part of Google Analytics). Most companies use third-party tools for marketing A/B testing. However, when it comes to product A/B testing, many eventually choose to build their own testing infrastructure to achieve more flexibility through tighter integration with their code. One of the main reasons for this is because most A/B testing tools use a JavaScript solution, which works fine for testing front-end (client-side) product variations. However, these tools don’t offer as much help when complex back-end (server-side) variations need to be tested. That being said, leading A/B testing tools such as KISSmetrics offer ways to integrate with your internal server-side A/B platform. You don’t have to split the traffic evenly between your A and your B variations. As long as the variation with fewer users has enough data points to calculate statistical significance, it will work out fine. Many companies are constantly experimenting, subjecting small subsets of



110

The Lean Product Playbook

their users to dozens or hundreds of alternative features or designs. You may notice a new button, feature, or design every once in a while on Google, Amazon, or other large websites. If you compare what you’re seeing to another user and it’s different, one of you is most likely an A/B test subject. Analytics and A/B testing are powerful empirical tools that help you understand your customers’ behavior and optimize your product. Competency with these tools enables rapid iteration and separates great product teams from others. Given the importance of analytics and A/B testing, Chapters 13 and 14 discuss these topics in more detail. This chapter has covered a variety of MVP tests and clarified the difference between qualitative versus quantitative, and product versus marketing tests. Once you’ve selected the MVP test that you want to conduct, you must design the page, screen, or other user experience artifact to test. Good user experience design is important and helps all the MVP tests discussed in this chapter be more successful. So before jumping into how to test your MVP, I want to discuss the principles of great UX design, so that you can apply them as you design the artifacts for your MVP tests.

Chapter 8

Apply the Principles of Great UX Design At this point in the Lean Product Process, you are clear on the feature set you believe should be in your MVP. User experience (UX)—the top layer in the Product-Market Fit Pyramid—brings your product’s features and benefits to life for the customer. Even if you have made good decisions on the other four layers, you will not achieve product-market fit without a good UX. Unlike the other chapters in Part II of the book, this chapter does not represent a separate step in the Lean Product Process. Instead, the advice in this chapter applies to Step 5, “create your MVP prototype,” covered in Chapter 7. This entire chapter is devoted to UX design because it is so critical to achieving product-market fit. You should use the guidance in this chapter when you are designing your MVP prototype. This chapter won’t transform you into a world-class designer, but it will give you an overview of UX design and an understanding of key concepts. This information is especially beneficial for product managers, developers, and others who work closely with designers. This knowledge should equip you to have richer discussions with designers, to contribute more to the design process, and to ultimately create better products. WHAT MAKES A GREAT UX?

We have all experienced products with either a fantastic or a poor user experience. The latter feel unintuitive and hard to use. You can’t find what you’re looking for, and you’re not clear what to do next. You may navigate to a dead end or receive a cryptic error message. It may be hard to read the text, or the design may not be aesthetically pleasing. All of those problems are symptoms of bad UX design.


112

The Lean Product Playbook

In contrast, a product with a great UX feels easy to use. It’s effortless to find what you’re looking for and to figure out what to do next. You don’t even notice the user interface and are able to focus on accomplishing the task at hand. The product may even be fun to use and convey emotional benefits such as confidence in your abilities or peace of mind. A great design may lead you to what psychologists call a state of “flow,” where you are completely immersed in using the product. In this state, everything else falls away, and you experience full involvement and enjoyment of the task at hand. An incredible UX can be a strong product differentiator. So what leads to an exceptional UX? No matter how easy to use or beautiful a product is, it can’t deliver a great user experience if the customer doesn’t value the benefits the product provides. In the Product-Market Fit Pyramid, the customer benefits that a product aims to deliver live in the value proposition layer, which is two levels below the UX layer. These benefits are addressed in the solution space by the feature set the product team has chosen. So, one way to evaluate UX is to consider how much it helps or hinders the functionality in conveying the desired customer benefits. Poor UX gets in the way, preventing the user from realizing the benefits. Great UX makes it easy for the user to realize the benefits that the product’s functionality offers. In addition to addressing benefits that customers find valuable, a great UX also achieves a high degree of usability and delight. Usability

The first key attribute of a great UX is usability, which indicates how easy it is for customers to use the product. Usability focuses on the users’ goals and the tasks they need to perform to achieve those goals. What percentage of users are able to successfully complete each task? What percentage are able to do so, but encounter problems along the way? You obtain answers to these questions through usability testing, where you ask users to complete key tasks and observe what they do. Beyond the successful completion of tasks, usability also includes efficiency. If customers are able to figure out how to use a feature, but it requires too many steps or takes too long, that results in poor

Apply the Principles of Great UX Design

usability. Efficiency is easy to assess by measuring effort. You can simply count the number of clicks, taps, keystrokes, or other user actions required to complete a task in a certain UX. Likewise, you can measure how much time it takes users to complete each task. You should compare these efficiency metrics as you evaluate different designs or try to improve a given design. In addition to actual physical effort such as clicks and keystrokes, perceived effort is also important. You should be mindful of the cognitive load that your UX places on the user. You can mentally overwhelm users by showing them too much information or giving them too many choices. You can also tax their knowledge or memory. A great UX avoids requiring users to exert much physical or mental effort. The likelihood of a user successfully completing a task is directly related to the amount of effort it takes. After observing usability tests and analyzing usage metrics for many products, I came to a general realization I call “Olsen’s Law of Usability”: The more user effort required to take an action, the lower the percentage of users who will take that action. The less user effort required, the higher the percentage of users who will take that action. I have seen this law ring true time and again when evaluating user experiences. It is valuable to keep this principle in mind not only for achieving better usability, but also to improve the conversion rate for user actions that are important to the success of your business. Examples of such actions include submitting a registration form or completing a payment flow. In addition to the objective behavioral measures I mentioned, your users’ perception of your product’s ease of use is important. For example, you can ask users, “How easy or difficult is the product to use?” and allow ratings on a seven-point bipolar scale: 1. Very difficult to use 2. Difficult to use 3. Somewhat difficult to use 4. Neither easy nor difficult to use



114

The Lean Product Playbook

5. Somewhat easy to use 6. Easy to use 7. Very easy to use At the end of a usability test, you can also ask customers other questions such as, “How well did the product meet your needs?” or, “How satisfied are you with the product?” They will not base their responses on what your intended value proposition or feature set is—which they likely don’t even know about. Instead, their answers will depend on their actual experience using your product. You won’t get any credit for having a valuable feature if users can’t find it or can’t figure out how to use it. You determine a product’s usability with respect to a particular user profile. Your target customers will have a certain level of knowledge or skill. Different target customers can vary in how tech savvy they are and how much relevant domain knowledge they have. Usability expectations would be quite different for a product intended for highly trained power users as opposed to one for a mainstream consumer market. Your personas, discussed in Chapter 3 and later in this chapter, should help inform this. Ease of learning is also an important usability attribute. How much time and effort does it take a user to progress from having no knowledge of how your product works, to working knowledge, to mastery? This is especially important for new users. They will decide very quickly whether your product is right for them or not, and ease of learning is critical in that assessment. Many products that deliver a great user experience address the need for “user onboarding” with helpful tutorials and guides for first-time users. These guides often disappear later or can be dismissed by the user when no longer needed. Delight

The second key attribute of a great UX is delight. Strong usability helps avoid a poor UX, but it is not enough to deliver a great UX. Usability answers the question, “Can customers use your product?” Delight answers the question, “Do customers enjoy using your product?” Delight, which goes beyond simply avoiding user frustration, means evoking positive emotions. Products that delight users are enjoyable and fun to use.

Apply the Principles of Great UX Design

One aspect of delight is aesthetics—ensuring that the product looks appealing. Customers see your product before they start using it, and visual appeal helps create a positive first impression. A pleasant design can convey a sense of high quality, make a product seem more credible, and make users feel at ease. The positive emotions that aesthetics help evoke can lead to higher customer enjoyment when they use your product. Simplicity helps some user experiences deliver delight. Less is often more in UX design—eliminating visual clutter reduces cognitive load and helps a user focus on what’s important. Think of the minimalistic design of the Google home page with its search box and not much else. User experiences that seem to read the user’s mind can help create delight. By selecting smart default choices on the user’s behalf or proactively addressing top-of-mind questions, a product can make users feel like it understands them and is empathetic. Google Suggest, described in Chapter 5, is a good example of such a feature. Products can convey personality to evoke emotion from users. This is typically done through the tone of the language used. Humor is another good way to create delight, both with text and with funny images. Delight often involves a dynamic response by the product based on user action. A great example is the “rubber band” effect that occurs in iOS when a user attempts to scroll past the end of a displayed document or webpage. It’s such an amusing effect that many people—whether consciously or not—can’t help but fidget with it. Animations and sound effects can also contribute to delight. I know that many Quicken users love hearing the cash register “ka-ching” sound after they enter a transaction. The Apple Mac startup chime is another sound that evokes a positive emotion for many users. Surprise is an important component of delight. Your product can amuse users by doing unexpected things. In Twitter’s early days, the social media platform experienced service outages. When that happened, instead of seeing a typical, unremarkable error page, users were treated to Twitter’s infamous “fail whale” graphic. I recall a pleasant UX surprise I encountered while using email marketing service provider MailChimp. After you finish composing a marketing email you plan to send, MailChimp lets you preview how the email will look at different screen widths. As you vary the



116

The Lean Product Playbook

preview width, the user interface shows the MailChimp mascot pointing to the number of pixels on a ruler with his outstretched arm. The instructions warn that the width of your email should not exceed a certain number of pixels. When you increase the width past a certain point, the expression on the chimp’s face changes from happy to pained, and text warnings such as, “Too big!” and, “Stop it!” appear. If you keep increasing the width past the recommended maximum, the chimp’s arm actually detaches, showing a “POP!” graphic. I found this delightful UX so clever, funny, and surprising that I still vividly remember it years later. THE UX DESIGN ICEBERG

So how do you create a user experience that customers find usable and delightful? Designing a great UX requires skill in several different areas, collectively known as UX design. My framework for UX design is the iceberg shown in Figure 8.1. Like an actual iceberg, only a small

FIGURE 8.1

The UX Design Iceberg

Apply the Principles of Great UX Design

portion of UX design is visible and immediately apparent—but there is much more beneath the surface. Starting at the bottom, the four layers of the iceberg are conceptual design, information architecture, interaction design, and visual design. I will describe each layer in detail, but here’s a quick overview: The conceptual design, the iceberg’s bottom layer, is the underlying concept that forms the essence of the user experience. The next layer is information architecture, which determines how you structure your product’s information and functionality. The next layer is interaction design, which defines how the user and your product interact with one another. The top layer that sticks above the water—the portion of the iceberg that users see—is visual design: how your product looks. We are visual creatures; visual processing is the main way our brains ingest information. But just looking good on the surface is not enough to create a great user experience. Excellent UX design requires attention and skill at all four layers of the iceberg, as well as a good design workflow that progresses from the bottom layer to the top. CONCEPTUAL DESIGN

The iceberg’s bottom layer, conceptual design, has to do with the core concept you are using to design your product. Ideally, your conceptual model should resonate with how your target customers think. A product based on a good conceptual design feels intuitive and easy to use. This layer of the UX iceberg can often contribute greatly to product innovation. You can envision the UX iceberg as sitting at the top of the Product-Market Fit Pyramid where the UX layer sits. Doing so highlights the fact that the conceptual design layer is just above the feature set layer. Conceptual design is what breathes life into the features and gives them form. By that, I don’t mean the details of how the features look and feel, but rather the essence of how they function to create value for the user. Examples are helpful to explain the idea of a conceptual design. Recall from Chapter 1 how the Quicken team achieved significantly greater ease of use than 46 competing products. The product’s success was largely due to its conceptual design of using the checkbook as a metaphor, which customers found very intuitive.



118

The Lean Product Playbook

Uber’s Conceptual Design

Let’s discuss the example of Uber, the popular service that lets you easily book car rides. What conceptual design did the company use for its mobile application? The Uber app uses a map-centric design. While map-centric designs are relatively common, the innovation of Uber’s conceptual design was to show users the location of nearby cars in real time: those available for hire before you book, as well as the car that you end up booking. It’s worth pointing out that implementing this design required technical innovation in order to track the position of Uber drivers in real time. Uber knew that taxi customers are frustrated when they are victims of late pick-ups or no-shows. As part of its value proposition, the company decided that it was critical to give users transparency into when their car would arrive. Uber’s innovative design shows you the estimated wait time, along with the car’s location as the driver comes to pick you up. The conceptual design that Uber chose for its UX makes the app’s value proposition immediately obvious to the first-time user and instantly conveys how the service is fundamentally different than trying to hail or schedule a taxi. User Research

Coming up with a good conceptual design is easier when you have a deep understanding of your target customers and their needs. An important but often overlooked part of UX is the “U”: the user. Recall that the Product-Market Fit Pyramid starts with the target customer; this is the person for whom you are designing the experience. You gain the understanding of your customer through user research, which is a specialized field within UX design. User researchers utilize a range of techniques to learn about customers, such as discovery interviews, usability tests, and surveys. Chapter 4 shared advice on customer discovery interviews. Chapter 9 describes how to conduct customer interviews to solicit valuable feedback on your designs. Chapter 13 provides a framework for UX research methods and describes the other techniques (besides interviews). User research informs all levels of the UX iceberg. When customers provide feedback on your UX, it’s helpful to parse their feedback and map it to the relevant layer of the UX design iceberg.

Apply the Principles of Great UX Design

It’s critical when you conduct user research to ensure that the UX researcher isn’t the only person who gains most of the learning. Product team members should observe as much user research as they can. Experiencing user research firsthand is much more impactful than just reading a research report. It’s like the difference between watching a sports event live from a front row seat versus reading an article about it the next day. Team debriefs, where individuals share and discuss their observations, help maximize learning and should be held promptly after the research occurs. Documenting the summary of results and key takeaways is also important to solidify the learning and capture it for others. One common and useful deliverable from user research is the persona. Personas

You need to understand your users in order to design a great user experience for them. The UX design tool used for this is the persona, which I first discussed in Chapter 3 in the context of defining your target customer (refer to Figure 3.1 to review). As a quick recap, a persona is an archetype of actual users. Good personas convey your target user’s goals along with any relevant psychographic, behavioral, and demographic attributes. Personas help inform your decisions as you design a product that delivers the customer benefits in your value proposition. In addition to the user’s goals, several other aspects of the persona are relevant in the context of UX design. The first is how tech savvy the user is. Users who are less comfortable with technology will need very simple interfaces that focus on the most important tasks along with clear instructions and a good help system. However, a technically advanced user would care less about those things and instead prefer more powerful tools that offer greater flexibility and productivity. There is often a tension in UX design when you need to address both types of users in a single product. Another aspect of personas that can help inform good UX design is the context in which customers will be using the product. If the user is rushed for time, then you need to make key information and frequently used functionality readily available without much effort. If, instead, the product context is more of a “lean back” experience, the user interface controls should probably be less visible and fade



120

The Lean Product Playbook

into the background so that the content can be front and center. If the product is used in a loud environment with lots of ambient noise, then you probably shouldn’t rely on voice commands. Teams use personas in the UX design process to remind themselves for whom they are designing the product. That’s why naming your persona is so critical. As the team wrestles with a decision over different design alternatives, they can ask themselves: “Which would best meet Nancy’s needs?” Many teams even create cardboard cutout photos of their personas that they place in the team’s workspace to help ensure they keep their user in mind as they work on the design. Once you have a conceptual design that you believe your persona will find intuitive and valuable, the next step is to define the high-level components of your product and how they should be organized. INFORMATION ARCHITECTURE

The second layer from the bottom in the UX design iceberg is information architecture, or IA for short. IA is the design discipline responsible for defining how the information and functionality of a software product should be structured. Products typically consist of multiple pages or screens, and there are numerous ways to organize them. The customer accesses the various parts of the product by using the product’s navigation system. When looking for a certain page, the customer relies on the navigation labels to infer how the product is organized and guess where the desired page is located. Card sorting is a research technique used to learn how customers think about the different parts of the product and how they are related in order to identify their preferred organization scheme. Good IA organizes a product in a manner that users find intuitive, with labels that are easy to understand, resulting in good usability and findability. Findability refers to how easy it is for users to find what they’re looking for in the product. To measure this attribute, you could ask a group of test users to try to find a certain page or screen in your product and see what percentage are successful. You can also assess findability by looking at navigation patterns from an analytics tool. Are your users taking the shortest path to get to each page? Or are they getting lost and taking longer paths or hitting the browser’s back button?

Apply the Principles of Great UX Design

Note that IA deals with the product’s feature set at a high level: what the features are, how they should be organized, and how they should be labeled. At this point, you are not yet thinking about user flows, page layout, or look and feel. Information architecture is a foundational layer that contributes to a great UX by making the product’s structure feel intuitive to users. The main IA deliverable used to do that is the sitemap. Sitemaps

A sitemap is used to define a product’s structure. Even though the term “sitemap” comes from “site” (as in “website”), it’s the name of the design deliverable used to specify structure for any software product, including mobile apps. A sitemap shows all of the pages or screens, how they are organized into sections, and the high-level navigation patterns provided. A sitemap also specifies page titles and the words used to label sections of the product. You should test your sitemap with users to ensure that your labels convey the intended meaning and that, when asked to find pages or screens pertaining to different features, they are able to easily do so with your proposed structure. Figure 8.2 is an example sitemap for a web application that enables video advertising campaigns. Each box in this sitemap represents a page. Lines connect the boxes to show navigation paths between pages. In a few cases, the navigation is one-way, indicated with an arrow (refer to the legend on the left side). The global navigation items—pages to which the user can navigate from any page—are clearly identified. The global navigation shows the major sections of the product, which correspond to the main links you would see at the top of a website. The sitemap indicates which page users navigate to when they click on each of those top-level links. For each section of the website, its subpages are shown in a clear hierarchical format. The sitemap also shows which pages behave differently from normal—by either opening in a popup or opening in a new browser tab. Once you have created a sitemap that defines your product pages and how they are organized, the next step is to identify how the user experience flows across those pages—that is, how the user will interact with the product.



FIGURE 8.2



Sitemap Example

Apply the Principles of Great UX Design

INTERACTION DESIGN

The next layer up in the iceberg is interaction design, which determines how your product and the user interact with one another. Interaction design specifies user flows: That is, what actions can the user take at each step, and how will the product respond? It also governs how users enter information, such as in a form. Any user interface control or link with which the user can interact (click, hover, drag, type, tap, swipe, etc.) falls under the umbrella of interaction design. For example, let’s say that your product has a registration form. Decisions about what information you ask your user to provide, how you design the fields in the form, and what you consider valid versus invalid input are all part of interaction design. So is deciding what happens after the user clicks the “register” button. Any user task that consists of multiple pages or steps requires interaction design. Navigation—when the user goes from one page or screen to another—is a common user interaction, which is also affected by your IA. If your product offers the user any “operating modes” aside from normal operation—such as an edit mode or preview mode—those are part of your interaction design, too. Similarly, if your design involves different states that impact what the user can and can’t do, they are also part of your interaction design. For example, a product for sale on an e-commerce site can have various states: in stock or out of stock, in your shopping cart, or ordered. There is an allowed flow through those states that your interaction design will enforce: The product must be in stock to add to your cart and must be in your cart to be ordered. A good design would make these states clear to the user throughout the flow. States that are important for interaction design can also often be closely tied to your conceptual design (the bottom layer of the iceberg). In the example above, the conceptual design is based on a virtual shopping cart, meant to be analogous to a real-world shopping cart. The Uber app also has a fundamental connection between its conceptual design and the state of the user’s trip. Before you book a car, the app shows you all nearby cars; but once you book a car, it only shows you that car. Another important part of interaction design is the feedback the product gives the user: how the system responds when the user takes



124

The Lean Product Playbook

a certain action. Error messages fall into this category. For example, if you fill out a registration form on a website and enter an invalid email address (e.g., due to a typo), the website should show you an error message asking you to please check to ensure the email address is valid. Error messages should be well written and clearly explain to the user why the error occurred and what they can do to fix it. Response time is another aspect of product feedback. If users click a button and there is no resulting indication that something is happening, they will assume their click didn’t register or that your product isn’t working. Slow performance creates poor interaction design. Users need confirmation that the system is receiving their actions. Even if the system’s final response to the action cannot be accomplished quickly, users should receive some feedback to acknowledge their click or tap if they will experience a perceptible delay. Animated spinners are a common solution for this, but you could also display a message explaining what the product is doing (e.g., “searching thousands of flights to find the best matches for your request”). If the product is going to take a while to complete the requested task, it is important to give users a sense of progress and how much time remains. The progress bar is a good tool for doing that, like the ones you see when you are downloading or uploading large files. For long user flows—such as a multipage wizard—giving the user a high-level sense of the steps involved and a progress indicator showing “you are here” as he or she completes the steps is helpful. TurboTax’s Interaction Design

Let’s discuss a product that has taken a complex process with many steps and made it easy for users to accomplish through great interaction design. There are few things as complex as preparing one’s income taxes. There can be countless questions across many forms that the user must answer to prepare their annual tax return. Yet the product team at TurboTax has managed to create a user experience that guides the novice through that process step by step, which is no small feat. The product offers two modes, the first of which is “EasyStep.” The conceptual design of EasyStep is a structured interview that guides the user through the process by asking them to answer one

Apply the Principles of Great UX Design

or two questions at a time. The product dynamically determines what question to show the user next based on the user’s previous answers. This design enables the user to focus on one thing at a time while hiding all the other questions and the complex dependencies between questions. The main user interaction is answering the question at hand— presented in a wizard interface—and then clicking the “continue” button. The product meticulously guides users step by step through the long process until their tax return is completed. Through good interaction design, TurboTax makes this formidable task easy for users. EasyStep’s interaction design also allows knowledgeable power users to skip steps, jump around in the interview, and fill out information as they please. EasyStep is the product’s main, default mode. The second mode is “forms” mode, where you see the actual tax forms and worksheets with their values populated (from the EasyStep interview) and can edit them directly if desired. TurboTax users can finish their taxes without ever going to the forms mode, and they can easily toggle between the two modes. Many users never even see the forms that are behind the scenes, which are mainly used by power users. By utilizing these two distinct modes, TurboTax’s interactive design effectively addresses the needs of both novice and expert users with the same product. Flowcharts

So how do you take a complex process or task—like preparing a tax return—and make it simple? Flowcharts are the primary design artifact for doing that. They specify the possible flows for key tasks in your user experience. They show the actions that can be taken and the decisions that can be made by both the user and your product. Actions are represented with rectangular blocks, and decisions points are represented with diamond-shaped blocks (also called conditionals). The blocks are connected with arrows to show the allowed flows. Figure 8.3 shows an example flowchart for the CarMax mobile application designed by talented UX designer Christine Liu http://christineliu.info. The app’s goal is to help the user find a car they like and then connect them with a CarMax dealership. The app uses



FIGURE 8.3



Flowchart Example

Apply the Principles of Great UX Design

customers’ Facebook data to suggest cars they might like. Users can browse suggested cars and can view detailed information on each car until they find one they like. At that point, they can contact a CarMax dealer via email, chat, or phone to book an appointment to see the car. The flowchart starts with the user downloading and opening the app, then shows the paths the user can take through the various screens to the end of the user experience. Notice that the flowchart defines the UX at a high level without addressing the design details of any screen (e.g., layout or visual design). Aside from their role in UX design, flowcharts can help ensure that everyone on the product team understands the end-to-end user experience the product should deliver. This can be especially valuable for the team members who will be implementing and testing the product. For simple flows, many teams skip the formal creation of flowcharts and jump straight to clickable wireframes. Because they are more visual and interactive than flowcharts, clickable wireframes are often more effective at communicating the desired interaction design. They do a good job of illustrating the flow and interactions for the “happy path”—that is, when users do what you expect them to do. You can supplement a wireframe with annotations to address error states and call out other important notes. However, if there are many different branches in the user flow and many possible states that need to be specified, then a flowchart helps capture and communicate that complexity. Wireframes

You typically create wireframes, first discussed in Chapter 7, after you are clear on the desired user flows for your product. Wireframes represent an important threshold in the Lean Product Process. Up to this point, you have spent a lot of time thinking about customers, their needs, your value proposition, the feature set, the IA, and the user flows. Now, for the first time, you are actually specifying what customers will see when they use your product—what each page or screen will show. At this point in the UX design process, you are not worried about pixel-level precision or visual design aspects such as color. You are trying to determine the layout of each page or screen: which components



128

The Lean Product Playbook

should be there and how they should be arranged. You are asking questions like, “Should this page have one or two columns?” and, “For this page, should I put the image on the left and the form on the right or vice versa?” Historically, wireframes were static, focusing only on layout. Clickable and tappable wireframes that bring the user flows to life are increasingly common. Modern design applications make it easy to create such wireframes relatively quickly. You won’t be able to finish designing the wireframes for any nontrivial product in one sitting; it requires iteration, and there is no “right answer.” You know what your problem space objectives are, and you are trying to identify different solution space alternatives to meet those objectives. Rather than fixating on the first design direction you come up with, it is beneficial to deliberately apply divergent thinking. Such an approach encourages you to come up with as many different possible design directions as you can. It’s like brainstorming: You are focused on generating, not evaluating ideas at this point. There will be plenty of time down the road to shoot down the bad ideas. At a later point in the design process, you will evaluate the top design approaches and use convergent thinking to narrow your focus. However, if you narrow down too early, you run the risk of exploring only a small portion of the solution space and ending up at a local maximum when there is actually a better solution yet to be discovered. Instead of designing the wireframe for each page or screen from scratch, good designers look across a product and identify groups of pages or screens that should be similar. Each group will share a distinct template that defines its layout. For example, you may create a template that all two-column pages share. Using a standard set of page or screen templates across your entire product helps ensure a consistent design. You should also standardize components that appear on multiple pages. Once you have created your initial set of wireframes, you now have—for the first time—a solution space artifact that you can and should test with customers. Chapters 9 and 10 describe how to solicit user feedback on your wireframes and use it to iteratively improve your designs. It’s okay to start out with static wireframes if that’s your preference, but you should create and test wireframes that are clickable or tappable. These enable you to test your user flows in addition

Apply the Principles of Great UX Design

to your layout, and give customers an interactive experience that feels more realistic. Once your target customers validate that your wireframes are easy to use and deliver on your value proposition, it’s time to move to the next layer of the UX iceberg: visual design. VISUAL DESIGN

Visual design is the tip of the iceberg—the part that is most immediately obvious to anyone looking at your product. It is also called graphic design, look and feel, or chrome. And as with the chrome on a car, it doesn’t determine what the product does or how you use it, but it does impact how it looks. Aside from creating an aesthetically pleasing product, good visual design helps reinforce the visual hierarchy (discussed later in this chapter) and contributes to ease of use. It can also convey your brand personality, create user delight, and differentiate your product from others. Three major components of visual design are color, typography, and graphics. Color

Color is an important aspect of a product’s visual design. You can create design artifacts for all the other layers of the iceberg in black and white and they will work just fine. But when you create high-fidelity mockups to specify the visual design, you need to decide on colors. Color contributes to aesthetics and is used to make certain elements on the page stand out more than others. You can also use color to convey certain attributes or emotions. Warm colors such as red, orange, and yellow are typically more energizing and passionate, whereas the more subdued cool colors such as green, blue, and purple are more calming and reserved. Many applications and websites use a blue color scheme because it conveys trustworthiness and calm. Green is associated with nature, growth, and money. Purple suggests luxury and creativity. Red is associated with aggression, passion, power, and danger. Orange is energetic and vibrant. Yellow conveys happiness and sunshine. Brown is associated with warmth and the earth. Black can suggest sophistication, elegance, and mystery. White is associated with purity, cleanliness, and simplicity. These are common generalizations, and the meaning of colors can vary around



130

The Lean Product Playbook

the world. So if your target customers are of a particular culture, you should be mindful in your design about how they perceive color. Instead of using colors willy-nilly, a good visual design has a deliberate color palette—the set of colors used in the product. The idea is to pick a combination of colors that look good together and to use only those colors in your design. Using color with consistency helps create a cohesive visual design. A color palette will usually have one or two main colors and one or two background colors. It can also have one or two accent colors, which are used to complement the main color. The full palette will also include additional colors created from the main color—usually lighter or less saturated versions of the same hue. Your use of color can help or hinder readability and usability. To ensure readability of text, there must be a strong contrast between the colors used for text and for the background. Black text on a white background is high contrast, but light blue text on a light gray background is low contrast and barely visible. For better usability, the use of color for key controls such as buttons and primary navigation should make them stand out clearly. An orange button on a black background will pop, but a blue button on a slightly darker blue background will not. Typography

Typography—defining the arrangement and appearance of text—is another important element of visual design. In the earlier days of the web, browsers only supported a small number of typefaces, such as Arial, Georgia, and Verdana. But the widespread adoption of CSS3 web fonts has provided a multitude of choices. Different typefaces convey different attributes: formal versus informal, classic versus modern, light versus dramatic. You should select fonts that reinforce the tone you want to set with your product. A key distinction among typefaces is serif versus sans serif. Serifs are the small decorative flourishes that extend from the edges of letters; sans serif fonts do not have these. Traditional design advice has been that serif fonts work better for print materials, which have a very high resolution (dots per inch), whereas sans serif fonts work better for the web, which has lower resolution. However, you see an increased use of serif fonts online with the proliferation of web fonts. At small

Apply the Principles of Great UX Design

sizes, serif fonts can be hard to read on a screen; however, they are often used in headings and other large text elements. Even more so than with color, you want to limit the number of different fonts you use in your product. A common approach is to select two fonts: one for body text and one for large text, such as headings. It’s common to see websites and mobile applications use a sans serif font for body text and a complementary serif font for headings. Font size is an important part of your typography. Your body text, which usually has the smallest size, should of course be large enough to be readable. Titles and headings will have a larger size. As with color and typefaces, you want to avoid using too many different text sizes and be consistent throughout your product. Fonts can also have different weights and styles such as bold, italic, or underlined. Typography usually plays an important role in establishing your design’s visual hierarchy (discussed later). You should deliberately design the color and relative size, weight, and position of the text elements on your pages to create the desired visual hierarchy. Graphics

Images, both photographs and illustrations, are often used in visual design. For certain product categories, such as ecommerce, using images well is critical. Take Airbnb: In order for customers to feel comfortable renting a place to stay, they need to see pictures of it. Chapter 7 discussed how Airbnb more than doubled their conversion rate by using higher-quality photos. Images are often used on landing and other marketing pages. The use of hero images is common—where a large, prominent photo shows your product, a typical customer, or some other artistic or inspirational object or scene. For example, Netflix often uses large photos of customers watching and enjoying a show. Illustrations are often used to explain how your product works. You can also use other graphical elements such as lines, shapes, textures, gradients, and shadows in your visual design. These smaller touches can help you achieve the look and feel you want for your product by adding structure, depth, and pizzazz. Icons are small symbols used to represent objects or concepts. They are most commonly used for buttons or other user interface



132

The Lean Product Playbook

controls, especially when space is at a premium. In many applications, icons are the main way to use the product. Browsers use icons for their back, forward, and refresh buttons. Adobe Photoshop utilizes a toolbar packed with icons as the main way for customers to access its functionality. Similarly, Microsoft Office products use a “ribbon” full of icons. Aside from user interface controls, icons are also used on marketing pages to support and complement the text. Iconography is a specialty within visual design; iconographers have to tweak individual pixels by hand to create their tiny masterpieces. The importance of good icon design has grown with the increased usage of mobile devices. Their small screen size puts space at a premium; so many controls in mobile applications are icons. Customers need to be able to look at an icon and understand what it means. However, it can be challenging to convey an icon’s intended meaning because it is so small and is just a symbol without any text. If a standardized symbol already exists for an icon in your design, I strongly recommend you use it instead of trying to invent a new symbol. Let’s say you’re creating an app that plays audio and are designing icons for the play and pause buttons. It would be silly to create your own symbols when everyone is familiar with the triangle pointing to the right that means “play” and the two vertical lines that mean “pause.” Most applications use a set of multiple icons, in which case it is important to have design consistency across the icons. Each icon needs to have a unique symbol, of course, but the overall shape, color, and style should be consistent with the set. Consistency in your product’s visual design is important to create a good UX. Two useful tools for achieving consistent visual design are style guides and layout grids. Style Guides

A style guide is a visual design deliverable that is used to achieve a consistent look and feel. They are especially important for products with many pages or screens. A style guide specifies the visual design details—such as color, size measurements, fonts, and graphics—for commonly used elements. A style guide helps maintain consistency, especially if multiple designers are working on the product, and also reduces work for your UI developer.

Apply the Principles of Great UX Design

Layout Grids

The layout grid is a design tool that helps you ensure consistent alignment of the design elements on each page or screen. Grids have long been used in print design, and their use in web and mobile design helps deliver a better UX. A grid consists of a specific number of columns of the same size separated by a “gutter” or margin. You select the size of your grid to match your situation. The sample grid in Figure 8.4 consists of 12 columns, each 94 pixels wide, separated by 18-pixel wide gutters. The total width of 1,326 pixels is optimized for screens that are 1,366 pixels wide, so that users don’t have to scroll horizontally. This width allows up to 40 pixels for a vertical scrollbar and any other visual elements from the browser or operating system. The idea is to align all page or screen elements to the grid as you lay them out. Examples of a page or screen element would be a block of text, an image, or a button. Elements can span more than one column. The key is that the left and right horizontal edges of elements should

FIGURE 8.4

Layout Grid Example



134

The Lean Product Playbook

begin and end on the grid. The grid I’ve shown has 12 columns, which is evenly divisible by 2, 3, 4, and 6, and allows a wide range of possible element widths. See Figure 8.5 for an example of a wireframe that utilizes a grid to arrange page elements. The grid shown in these two figures only divides the space horizontally (into columns). Grids used in print design often specify vertical divisions (rows) as well. Grid lines for vertical positioning have been less useful on the web due to the large variation in screen heights. In addition, it can be hard to control the exact vertical position of elements because browsers dynamically render content based on the width of the screen. As a result, digital grids tend to focus only on the horizontal divisions. With the advent of responsive design, discussed later, designers now have a greater ability to control the vertical position of elements in their web designs. Recall that at the wireframe stage, your layout usually describes only the approximate position and relative size of elements. By enabling precise, pixel-perfect layouts, grids help you make the transition from lower fidelity wireframes to high fidelity mockups.

FIGURE 8.5

Wireframe Using a Layout Grid

Apply the Principles of Great UX Design

Mockups

Mockups, discussed in Chapter 7, are higher-fidelity design deliverables that capture your visual design. They build on your wireframes, using color, typography and graphics to create the look and feel of your product. Mockups are typically created by a visual designer in a tool such as Adobe Illustrator or Sketch and then exported as an image file (PNG, GIF, or JPG). You can solicit feedback from users on static mockups such as these, but it is more valuable to show users a set of clickable or tappable mockups. These give users a better sense of your product and how it works. Tools such as InVision let you take a set of static mockups and string them together into a user flow. These tools let you identify a clickable area on a mockup (e.g., a button or a link) and specify to which other mockup it should navigate. Chapters 9 and 10 describe how to solicit user feedback on your mockups and use it to iteratively improve your designs. Once you have a set of clickable or tappable mockups that your target customers agree is easy to use and delivers your value proposition, then you have finished your UX design. The next step would be to implement your UX by building your product. Chapter 12 discusses how to do that using Agile development. I’ve explained the UX design iceberg and how to bring your feature ideas to life by progressing through its four layers. I’ve also described the key design deliverables along the way, including the ones you should test with customers to assess your UX and product-market fit. In this next section, I share several important design principles that will help you create a great UX. DESIGN PRINCIPLES

Design is a magical part of the Lean Product Process. It’s where intangible ideas about benefits and features get transformed into an actual user experience. In many ways, design is more art than science, but there are several design principles that can help you create a better user experience. Gestalt Principles

The Gestalt principles are a set of useful theories that describe how humans visually perceive objects. The word gestalt means “an



136

The Lean Product Playbook

organized whole that is perceived as more than the sum of its parts.” That’s what our visual processing system attempts to do with what we see. There are several Gestalt principles, but I will focus on the principles of proximity and similarity. According to the Gestalt principle of proximity, the brain perceives objects that are closer together as more related than objects that are farther apart. Therefore, you should put related objects close to one another in your designs. You should apply this principle when you are determining the layout in your wireframes. This applies to both arranging content as well as user interface controls. For example, you can see in Figure 8.5 that all of the primary navigation links are together. If your user interface gives the user three choices on how to proceed, then the three buttons or links should be shown together. You should avoid putting unrelated items close to each other or else the user may infer they are related. For example, placing a cancel button for one feature too close to another feature on the same page may lead to user confusion about which feature the cancel button affects. The Gestalt principle of similarity maintains that the brain perceives objects that share similar characteristics as more related than objects that don’t share those characteristics. Therefore, in your designs, objects that are similar or related should look similar by having the same shape, size, or color. You should avoid making unrelated objects look alike. You should apply the principle of similarity when you are determining your visual design. For example, you could require that all your hyperlinks be blue and underlined or that all your action buttons have the same rounded rectangle shape. Because they describe how our visual perception works, the Gestalt principles lead to the next design principle: visual hierarchy. Visual Hierarchy

Visual hierarchy is an important design principle that determines which elements of your design the user considers most important. This importance drives the user’s attention, influencing the order in which they look at the various elements. The size and color of elements are two of the main attributes that create a visual hierarchy. The brain assumes that larger objects are more important and smaller objects are less important. It also assumes that elements with high contrast—for example, a color that

Apply the Principles of Great UX Design

makes them stand out or “pop”—are more important. Images that stand out can have the same effect. Our eyes are naturally drawn to pictures of people, especially faces. The location of elements also affects visual hierarchy, because users start reading at the top of the page. In English and other left-to-right languages, users start reading on the left side of the page. Therefore, all other things being equal, people will look at elements near the top left corner of the screen first. A nice hack to quickly determine the visual hierarchy of a page or screen is to squint your eyes. You will not be able to read the text or see details, but you will notice the location, size, and color of major design elements. You can also take a screenshot and blur it (using a graphic design application) to create the same effect. When you try this test on a product with strong visual hierarchy, you will be able to identify the most important design elements. Designers should use the principles of how human visual processing works to reinforce the desired hierarchy of information. To create a visual hierarchy, you should first identify the relative importance of the different components that should be on the page. Then design the location, size, and color of your components to reinforce that prioritization. The design of a page with good visual hierarchy will attract the user’s eyes to the most important element. The design then guides the user’s eyes from one element to the next in priority order, usually moving along intuitive top-to-bottom and left-to-right paths. This helps the user find what they are looking for and successfully complete tasks. It also leads to higher conversion rates for key user actions. A good visual hierarchy is a critical component of a good UX. Principles of Composition

In addition to the Gestalt principles and visual hierarchy, you should also consider these principles of composition when creating and evaluating your designs: Unity: Does the page or screen feel like a unified whole or a bunch of disparate elements? ● Contrast: Is there enough variation in color, size, arrangement, and so forth to create visual interest? ●



138

The Lean Product Playbook

Balance: Have you equally distributed the visual weight (position, size, color, etc.) of elements in your design? ● Use of space: How cluttered or sparse does your design feel? Ensuring your design has enough white space—the space you don’t use on the page or screen—is important to avoid designs that feel crowded to the user. ●

Responsive Design

When you specify the arrangement of design elements on a page or screen, you have to make assumptions (either explicit or implicit) about the size of your “canvas.” In print design, you know the exact width and height of the paper on which your design is going to be printed. Unfortunately, the digital world is not so simple. Your customers will be using your product on devices with a variety of screen sizes, so the size of your canvas is not so straightforward. Clearly, smartphones have much smaller screens than laptops and desktop monitors. The original iPhone was 360 by 480 pixels, for example. There is now a wider range of smartphone sizes than ever. Tablets emerged to populate the gap in screen size between smartphones and computer monitors. Phablets have filled the gap between smartphones and tablets. At the high end of the screen resolution spectrum, desktop monitors with large resolutions have become popular. The range of different screen resolutions has grown even further with the advent of wearable devices, such as Apple Watch, and their tiny screens. How should your product team deal with this large and highly fragmented variation in screen resolutions? You can use responsive design for web-based products. Rather than trying to accommodate all users with a single design, responsive design allows users on different size screens to see different versions of your user interface. The design responds to the user’s screen size, usually the width. You start by determining the screen width “breakpoints” you want to use and then apply the desired differences in styling to each width. It’s common to have a large width version for computer screens and a small width version for phone screens. Many products also use an intermediate width breakpoint for tablets.

Apply the Principles of Great UX Design

With responsive design, as the screen width shrinks from wide to narrow, some page elements start “wrapping”—that is, getting pushed to the next line. Some elements become smaller in size or just disappear at smaller screen widths. Responsive design enables these types of dynamic UX changes without requiring a lot of additional coding effort or complexity. Designing for Multiple Screen Sizes

The need to accommodate multiple screen sizes is a reality of modern-day software design. For web-based products, responsive design is a great tool for doing so elegantly and without too much additional effort. Native mobile applications share the same problem, and mobile software development kits (SDKs) include tools that enable an app to have different layouts optimized for different screen sizes. But should you begin your UX design process with the larger or smaller screen size? If you’ve initially designed your product for a larger screen size, modifying it for a smaller screen size can be challenging. The same amount of content just won’t fit, so it can be hard to choose what to remove. You will probably have to change your navigation. You will need to rethink and replace content that is just too wide for the smaller screen. Often such teams end up creating a second, separate product with a different code base—a situation that isn’t ideal. For one thing, the need to make changes and additions in two separate pieces of code leads to inefficiency and an increased chance of errors. And because the mobile and non-mobile products were not designed together, they often look and feel very different from one another—which results in an inconsistent user experience that can confuse customers. It is harder to design for a smaller screen due to the space constraints, which require more tradeoffs. As a result, many teams embrace a “mobile first” approach—designing for the smallest screen first since this forces them to prioritize what is most important. After the mobile design is far enough along, they design the larger sizes, which can often easily accommodate additional content and functionality. Note: The intent is not for the two designs to be



140

The Lean Product Playbook

designed sequentially or separately. They should be done in parallel; it’s just that the mobile design leads the process. Often, rather than just being a smaller version of the full-size product, the mobile version of the product will play a complementary role in relation to the web version. It may have unique functionality that the web product doesn’t have (e.g., taking advantage of geolocation or other sensors). Or it may offer a more focused subset of the full functionality of the web product. Designing the two in parallel helps ensure that they work together to deliver a user experience that achieves product-market fit. COPY IS ALSO PART OF UX DESIGN

Before concluding this chapter, let’s touch on an often-overlooked component of the user experience: copy. This is the text that your customers see, whether it’s on your marketing pages or in your product. The quality of the copy on marketing pages can result in major differences in your conversion rate. But the copy you use in your product—labels, instructions, descriptions, and error messages—can really affect usability. Users often have very little text to guide them, so labels on buttons and links need to be clear and easy to understand. It is a major usability problem if a user wants to perform an important action but isn’t sure which button to use. Descriptions of features and instructions should be written in simple text using words that users understand—not internal or industry jargon. Error messages should be helpful and explanatory instead of cryptic. The good news is that it is relatively easy to identify and fix problematic copy; you just need to conduct usability tests of your product. In your tests, if users encounter difficulty with a particular word or phrase, you should ask them what they would call it, since they often have great suggestions. THE A-TEAM

As you can see from the topics covered in this chapter, UX design is a discipline that spans several different skills. Many companies have a “design gap”—a situation where all of the skills required to create a great UX design just aren’t present. Many teams don’t have a designer. Even if you have a designer, that person probably isn’t

Apply the Principles of Great UX Design

strong in all the UX design skills. It is possible for a single designer to be strong in several of the different UX design skills, but it is more common for a designer to be stronger in either visual design (how the product looks) or interaction design (how the product works). To create a great UX, your team needs to be talented in both of these areas. You also need a front-end developer who can skillfully implement the design, as well as a strong product manager. Aside from each person individually possessing the requisite skills, it’s crucial for these team members to work together effectively in order to deliver a great UX. I like to call a team who has this set of four essential skills—product management, interaction design, visual design, and front-end development—the “A-Team” (like the popular 1980s television show). Other roles or skills are obviously important to deliver a great product: back-end developers, quality assurance (QA), DevOps, and so forth. But when it comes to creating a great UX, having an A-Team is critical. UX IS IN THE EYE OF THE BEHOLDER

At the end of the day, your customer is the ultimate judge of how good your user experience is, which impacts your product-market fit. Recall from the technology adoption life cycle in Chapter 3 that innovators may be willing to tolerate a substandard UX for a breakthrough product that provides cutting edge benefits. But as you try to advance through the technology adoption lifecycle to penetrate additional segments, they will not be as tolerant, and UX becomes more important to product-market fit. Even though great design takes a lot of skill and work, there’s really no excuse for having a bad user experience. As I’ve discussed, you should be showing your designs to customers to identify and resolve any issues. In fact, that’s what the next chapter is about: how to test your MVP prototype with customers.



Chapter 9

Test Your MVP with Customers (Step 6) Once you have applied the principles of great UX design to create the prototype of your MVP candidate, the next step in the Lean Product Process is to test it with users. This is where the rubber meets the road. You’ll recall that Chapter 7 discussed two fundamentally different types of test you can run: quantitative and qualitative. That is, you’ll either pay attention to the details of what you’re hearing from a small number of customers (qualitative) or to the aggregated results for a large number of customers (quantitative). Quantitative tests, such as A/B tests and landing page tests, are relatively straightforward to conduct and analyze. Since they don’t involve talking to users, they’re just about the data. You track the conversion rate (or other metric) for your MVP test and see how it compares to the target value that represents a successful outcome (or to the value for other alternatives). You need to be mindful of your sample size, which will affect the level of confidence of your results. This chapter focuses on how to conduct qualitative user testing of your MVP. User feedback is incredibly valuable because it identifies what you don’t know. When you are so close to your product, it is difficult—often impossible—to perceive it as a new customer does. You have become more familiar with your product than any new user could ever be. As a result, you have “product blindness”: blind spots for the issues that a new user will readily encounter within minutes of using your product. User testing is the antidote for product blindness. User testing validates or invalidates your hypotheses, whether you made them explicitly or they are implicit assumptions. Because of product blindness, the first time you test with users often leads to the most surprising learning. I recall when I conducted the first user feedback sessions for cloud collaboration startup Box—it was quite an eye-opening


144

The Lean Product Playbook

experience for the team. They learned so many new things that they instantly saw the value of user testing and wanted to do more. Qualitative user tests require that you show customers your product or design deliverables—wireframes, mockups, or prototypes—to solicit their feedback. It takes skill to design and run these tests successfully. I’ll be sharing lots of advice—both what to do and what not to do—to help you get the most value out of your qualitative user testing. HOW MANY CUSTOMERS SHOULD I TEST WITH?

I recommend conducting user tests with one customer at a time for the best results. You can speak with more than one customer at a time, but you usually get suboptimal results due to group dynamics. You especially see this negative affect in focus groups, which involve talking to anywhere from 6 to 12 people at once. Some participants may not speak their mind openly for fear of being judged or criticized. One or two outspoken people often dominate the discussion, drowning out other voices. Participants also often experience groupthink, where all or most of the group artificially converges on the same opinions, which leads to inaccurate data. By speaking with one customer at a time, you don’t experience any of those negative group dynamics, and you’re able to have a richer, more in-depth conversation. The customer is much more likely to speak up and share his or her true feelings, especially if the moderator is the only other person present. In my experience, the more observers you have, the more worried about being judged some customers can be. Many moderators like to have a note-taker present so they can focus on conducting the user test, which is fine. I personally prefer taking my own notes—that way, I’m certain that my insights get captured, and it’s truly a one-on-one interview. If you want observers to be able to watch testing sessions live, then using a webcam that projects the video feed to a monitor in another room is a good alternative. If the user test is remote, then the observers can join the screen sharing session. I have conducted user testing with two and three customers at a time. It worked out fine because I was getting feedback on printed mockups; we were all seated at a table and could see and point to the papers on the table. It probably wouldn’t have worked so well

Test Your MVP with Customers (Step 6)

if each customer was looking at the designs on a laptop. I took this approach instead of one-on-one sessions because my client wanted to obtain the results of the research very quickly, and this allowed me to speak to more customers per day. Sometimes research subjects don’t make their appointment; no-shows are just a reality of user research. So another benefit of having two or three people scheduled for each session was that I wasn’t left twiddling my thumbs if one person was a no-show. Product teams often ask, “How many customers should I test with?” If you talk to too few, you run the risk of not catching all the issues you need to address. And you might discover opinions that aren’t really representative but not realize that’s the case. On the other side of the spectrum, talking to too many people takes additional time and resources. You can go past the point of diminishing returns where you just keep hearing the same feedback and aren’t learning anything new. I’ve found that testing in waves of five to eight customers at a time strikes a good balance. That number of tests is enough to uncover major issues and identify patterns across users. After each wave, you will be revising your product or design artifact based on what you learned and then testing it with a new wave of customers until you’ve validated that you’ve achieved product market fit. You should plan for the fact that some customers will not show up for their user test. The typical no-show rate is usually around 10 percent. From a practical standpoint, I would just schedule one more test than my desired sample size. If I knew I wanted to speak to seven people, I would schedule eight to hedge for a no-show. IN-PERSON, REMOTE, AND UNMODERATED USER TESTING

You can conduct user testing research either in-person or remotely. In-person is straightforward: the moderator and the customer are in the same room. Remote testing is possible using screen sharing or video recording technology. With remote testing, you can have either moderated or unmoderated tests. Moderated means that the researcher is present and conducting the test with the customer. Unmoderated implies that no moderator is present; instead, customers are provided with the artifact or product to test and guidance on what to do. These sessions are recorded for the product team to watch later. Most tools capture the customer’s screen (i.e., so you



146

The Lean Product Playbook

can see where they were clicking in your product), and many also record audio so you can hear the user’s thoughts. Some tools also capture video of the customer’s face (i.e., using a webcam). Of the three qualitative testing methods—in-person, moderated remote, and unmoderated remote—I would recommend in-person if possible. You can gather much richer data sitting next to a user versus sharing a screen. You can see the user’s screen and face when you’re in his or her presence, and can pick up little things like sighs, facial expressions, and other subtle cues. You can see where the user’s eyes are looking. You are also likely to build a better rapport in-person, which usually leads to better data because the customer feels more comfortable talking to you. Of course, sometimes it can be difficult to find target customers nearby. If this is the case, then remote moderated testing is a good way to reach them where they are. While not quite as good as in-person testing, you can still get valuable information. To see the customer’s screen, you will use a screen sharing application such as GoToMeeting, WebEx, Skype, Screenleap, or join.me. As with any situation like this, you should be prepared to encounter technical difficulties. When you’re ready to start a remote session, it’s common to find that customers have not installed the software required to share their screen or need help getting it running properly. Additionally, the screen-sharing program can get in the way of the test, for example, by causing user confusion or shrinking the size of the design artifact you’re showing. There is often some lag between the customer’s actions and when you see them on your screen, and firewalls can cause problems as well. However, when you don’t run into technical difficulties, remote-moderated testing can yield a lot of valuable information from users. The third type of testing is unmoderated remote testing, which you accomplish using a service such as UserTesting or Validately. Such services provide access to your design artifacts, facilitate users through the session, and capture what they do. Many of these services also offer a panel of users to test your product. One advantage of this approach is that you can get results more quickly. You usually don’t have to spend any time on recruiting or scheduling users, and multiple users can perform the user testing at the same time versus being constrained by moderator availability. However, you

Test Your MVP with Customers (Step 6)

are not present to guide the user through the experience. The user follows written instructions, so you have to put more thought into structuring the flow of the test and the directions you give the user. It’s best to pilot the test with one or two people before recruiting many users. Additionally, the fact that you are not present means that you cannot ask questions as they arise, such as, “Why did you click that button?” You must provide all the questions you’d like the user to answer in advance—so you need to give more thought and attention to detail to how you word the questions compared to moderated testing. Most unmoderated remote testing tools focus on recording what users are doing on their screens, capturing their mouse movements and clicks. While seeing the user’s screen is helpful, hearing audio from the user adds even more value. Some tools even include video of the user’s face. Other tools don’t record the user or the screen and just capture clicks and calculate clickthrough percentages. That type of quantitative information will be useful once you have launched at scale, and you can get it from your analytics package. But when you are trying to test product-market fit with unmoderated tests, it’s preferable to have both screen recordings and user audio, which most of the leading tools provide. One advantage of unmoderated testing over moderated testing is that there is no risk of the moderator influencing the results. In reality, customers are going to be evaluating, signing up for, and using your product on their own, without anyone by their side. This makes unmoderated testing more prototypical of the user’s real world situation. Most customers who sit with a moderator pay more attention and try harder than they would if they were on their own. So how should you select which method to use? When you are early in defining and validating your MVP, moderated testing is the way to go to ensure you can ask questions and get rich customer feedback. As I’ve emphasized, in-person is ideal—unless it’s a challenge to find target users and remote testing is more feasible. When you are farther down the road and feeling more confident about your MVP, unmoderated testing can be a useful tool to compliment moderated testing since it takes less time and is less expensive. That’s why unmoderated is also a good option if you just don’t have the resources to conduct moderated testing.



148

The Lean Product Playbook

HOW TO RECRUIT CUSTOMERS IN YOUR TARGET MARKET

Of course, you want to make sure that the customers with whom you are testing are in your target market. Otherwise, their feedback could send you iterating in the wrong direction. You can ensure a good fit by using a screener—a set of questions, like a survey, that you ask prospective participants. For example, if you were targeting younger males, you would ask questions about age and gender. You create multiple-choice answers for each question and decide which answers qualify versus disqualify respondents from your target market. Chapter 3 discussed the different types of customer attributes you can use to specify your target customer. In addition to demographic attributes, behavioral attributes are typically very useful. If, for example, you were targeting hardcore videogamers, you would probably ask, “Do you play videogames?”, and filter out people who replied “no.” You might then ask people who replied “yes,” “In a typical week, how many hours per week do you play videogames?” The respondents would select from a list of possible responses such as “less than 5 hours per week,” “5 to 10 hours per week,” “10 to 20 hours per week,” “20 to 30 hours per week,” and “over 30 hours per week.” You could decide that gamers need to play 20 or more hours per week to be in your target market, and therefore only accept people who selected the last two choices. Psychographic attributes—users’ opinions and feelings—can also be useful for screening. Sticking with the same target customer, one possible psychographic question could be, “Do you consider yourself a hardcore gamer?” You could also ask, “How much do you enjoy playing videogames?” and provide a scale for responses. You should refer to the personas you created for your target market as you develop your screener questions. As with everything else, the screener questions serve as hypotheses for you to test and iterate. If you notice while running your initial user tests that your first set of screener questions didn’t get you the right kind of customer, then you should change them for the next round of tests. You’ll often discover additional criteria to add to your screener after your first tests. For example, let’s say we had a portfolio management application targeted at investors. Our first screener might ask questions about trading frequency and portfolio value. We discover after our first set of user tests that there are two distinct types

Test Your MVP with Customers (Step 6)

of investors: those that like to make their own investing decisions, and those that prefer to delegate decisions to a professional advisor. Our value proposition resonates with the first group but is not appealing at all to the second group. For subsequent user tests, we would add a question to our screener to target the do-it-yourself investors and filter out the delegators. We should update our persona accordingly, too. Once your screener reflects the customers from whom you’d like to get feedback, the next step is to recruit them. This can be the most challenging step for many people who are excited about Lean user testing. If you are trying to improve an existing product, you can often talk with your existing customers. If not, you have to figure out how to find your target customers. You might be lucky enough to have a list of prospective customers you can contact. Otherwise, you’ll have to hunt for them. One approach is to try to recruit local participants by posting online to Craigslist, TaskRabbit, and similar websites. A best practice is to include in your posting a link to an online survey hosted at SurveyMonkey, Google Forms, or another survey site with your screener questions. The volume of responses you receive can vary quite a bit depending on where you post, what you say, and the size of the incentive you offer. If you experience a low response rate in your recruiting efforts, using remote testing lets you expand beyond your local market to anyone online. Some companies use Amazon’s Mechanical Turk (MTurk) as an affordable recruiting source for remote testing, and several services have been built on top of MTurk to make this easier to do. Many remote testing services, such as UserTesting, have a panel of customers available for testing. The amount of control these services give you over screener questions can differ. Some limit you to prespecified attributes such as gender, age, employment status, and so forth, while others let you ask your own questions. When selecting a remote testing service, ensure that you have the required level of control over screener questions. Getting feedback from customers that aren’t in your target market is a waste of time and money that can lead you in the wrong direction. It can be harder to reach your target customers if they are not consumers—for example, if you’re aiming for marketing executives or doctors. One creative way is to target conferences, meetups, or other events where they congregate and conduct some guerrilla



150

The Lean Product Playbook

on-the-ground testing. One of my clients had an idea for a product related to purchasing carbon offsets. He was originally planning to build a web application, which would have taken a lot of time and money. I explained why I thought a Kickstarter MVP would make more sense, since it would allow him to validate his value proposition before spending any money on coding—and he agreed. He targeted a local conference on alternative energy and brought his iPad along. As he spoke with attendees, he figured out which ones were in his target market and showed them his Kickstarter page. He received tons of valuable feedback in a short amount of time. Events like that can be a good way to get concentrated feedback. Unfortunately, relevant events probably aren’t taking place often enough near you to support rapid testing and iteration. A remote testing service with a panel can be a good option for recruiting users frequently with relatively short notice. Customer research companies are another option for conducting in-person tests. Many research companies have a local panel of participants from which they can recruit. Such companies often offer an end-to-end service that includes testing facilities and a moderator, but you can usually just pay them to recruit for you. The price per recruit can vary but is often between $75 and $150. If your target customers are relatively scarce and place a high value on their time—say heart surgeons or CEOs—it can cost a lot more or simply not be feasible to recruit them. In my experience, research companies are a great way to recruit local participants for in-person testing. The main disadvantage is the cost. But you frequently get more than your money’s worth back in valuable feedback, especially if you’ve done a good job on your screener and conduct a good test. How to Avoid the Scheduling Trap

I see a lot of companies who want to conduct user tests struggle with the logistics of scheduling the sessions. Product teams spend much of their time heads-down, working on defining their value prop, writing user stories, and designing wireframes. When they’re ready to test their wireframes with users, they pop their heads up and scramble to recruit users quickly so they don’t lose time. It’s very hard to recruit users at the drop of a hat like that. At that point, most

Test Your MVP with Customers (Step 6)

teams haven’t thought about their screener or test script. If the team doesn’t have any resources to help with recruiting users, it often falls on the product manager or designer—both of whom already have a full plate. It can take a week or two (or longer) until the first user test is scheduled. By that time, the team has probably received a lot of pressure to move forward and succumbed, proceeding with high fidelity design or even coding. By the time they’re able to digest the feedback from the wave of user tests, it’s too late for it to impact the product. They complete an iteration or two of development and then this frustrating cycle repeats itself, leading many teams to reduce their frequency of user testing or stop altogether. What’s a Lean product team to do? The best way out of this trap is to just blindly schedule users on a routine basis. For example, you might schedule three users to come in every Tuesday afternoon or five users every other Wednesday. I use the term “blindly” because when you schedule the users, you probably won’t know exactly what you’ll be testing with them. Instead of waiting until your product or artifact is ready to schedule users, teams can just count on users being available at the designated recurring time. This breaks the dependency between having your test ready to go and scheduling users, and enables a much higher frequency of user testing with a lot less work. I also recommend that teams get a resource to help recruit and schedule users for tests. Junior employees or interns can be good options, as well as part-time contractors. They mainly just need a well-written screener to do the job. Starbucks User Testing

If you’re up for guerrilla tactics, another option is what I call Starbucks user testing, where you spend time at a cafe and test with people you recruit on the spot. The main benefits of this method are its low cost and immediacy. The main drawback is that you’re not able to closely control the type of customers with whom you speak. If you have a mainstream consumer product like Google or Facebook, it can be feasible to find people who are in your target market. However, this approach probably won’t work if you have a very specific target customer. You can try to visually screen people and make inferences from their appearance (e.g., gender, age, how they’re dressed, etc.).



152

The Lean Product Playbook

Be prepared for a fair amount of rejection; many people don’t like being approached by strangers or are too busy. Personally, I’ve found the shopping mall to be a good alternative to the cafe, since people there seem to be less busy and more open. Your opening line is critical to your success rate. Make it a point to quickly inform people about what you’re asking of them and what you’re offering in exchange for their time. For example, you could say, “Hi, sir, do you have 10 minutes to share your feedback on a new website in exchange for a $25 Starbucks card?” Compensating Customers

Speaking of cost, how much should you compensate testers for their time? The typical range is $75 to $125 per hour, but it depends on your target customer and how much their time is worth. Talking with a heart surgeon probably would cost much more than that—while talking to a high school student would be much less. Many resourceful startups are able to recruit testers without compensation by finding people who have sufficient interest in their product category. I’ve used admission to an exclusive private beta as a carrot, as have other companies. If your company has nice swag (e.g., a T-shirt, hoodie, or track jacket), that can work, too. There are several options for payment. Gift cards are convenient for both parties since they are easy to buy and easy to use. A generalpurpose gift card such as Visa or MasterCard has more appeal than a specialty gift card. If you’re doing Starbucks user testing, a Starbucks card works well. Cash works but can be a pain to obtain from a company account. Some companies prefer to issue checks for accounting purposes. Cashier’s checks are a good option because the respondent doesn’t have to worry whether they will bounce or not. If you are testing with current customers, then an alternative to giving them money is giving them credit toward your service or future purchases. USER TESTING AT INTUIT

I was first introduced to user testing at Intuit, a pioneer in the field. After the launch of each new version of Quicken, product managers would conduct “follow me homes.” They would wait in the store

Test Your MVP with Customers (Step 6)

aisle where Quicken was being sold. When they saw a customer who was going to purchase Quicken, they would ask if they could follow the person home, where they would observe the customer install and use the product. The ability to watch customers use our product in their real world setting gave us lots of valuable insights. You may have heard of “contextual inquiry” or “ethnographic research”; these are UX research methods that also focus on observing customers in their real-life setting. It’s not always feasible or economical to go to where your users are; sometimes it makes more sense to have them come to you. Intuit also created a state-of-the-art usability lab with several rooms for conducting tests. We would invite customers to the lab to test software as we developed it. The moderator would be the only person in the room with the customer, but a one-way mirror enabled additional people to watch the test live. The lab’s cameras captured the computer screen as well as the customer’s face and relayed the video to monitors in the back room. RAMEN USER TESTING

Intuit’s usability lab was very impressive and fun to use as a product manager. But the reality is that you don’t need such an elaborate setup to conduct user testing. Since I left Intuit, I’ve worked at many startups, which usually have to be scrappier with their limited resources. I’ve helped them conduct what I call Ramen user testing, a technique that eliminates everything but the essential parts of user testing. Instead of using a dedicated facility, you just use a conference room at your office. Instead of hiring a dedicated moderator, someone on the team (usually the product manager or designer) runs the session. If you’ve never run a user test, I recommend you give it a try. I’ve found that many people who are initially intimidated by the notion of running a user test just need a little encouragement. It isn’t rocket science—like most things in life, it just takes practice to get better at it. But because it can be challenging to try to moderate and take notes at the same time, I recommend having a dedicated note-taker. Have customers bring their laptop or device for the test if possible, since this tends to work out much better than making the customers use a device with which they’re not familiar. I’ve seen differences in



154

The Lean Product Playbook

operating system (Windows or Mac), keyboard, mouse, or browser throw users for a loop and interfere with a test. It’s good to observe customers using your product on the actual devices they use at home or work. You often learn something new that didn’t come up in your team’s internal discussions and tests. Once in the room, I like to seat the customer at the table with their laptop or mobile device and have the moderator and note-taker sit next to the customer, one on either side. This allows you to face the same direction as the customer and see the screen. Plus, sitting next to them allows you to notice facial expressions and other subtle cues. I encourage others on the team to watch the user tests, too. It is very powerful when multiple people observe the same customer feedback at the same time. The problem and solution spaces can be a bit murky as you are seeking product-market fit, which can cause team members to have different hypotheses and opinions. Watching user tests together helps team members achieve a shared understanding. That being said, you don’t want to overwhelm the customer with lots of people in the room. A maximum of three people in a conference room with the customer is enough. It’s also important for anyone but the moderator to remain quiet and not disrupt the test. If more people want to watch, then you can set up a webcam that transmits to a screen in a separate viewing room. I’ve also used a setup in a larger room where I attached a projector to the customer’s laptop for others to watch. I arranged the projector and observer chairs behind the customer’s field of view but close enough for the observers to hear the customer. Some people on your team might be tempted to record in-person user tests instead of watching them live with the idea of watching them later. I’ve been involved with a large number of tests, and I’ve never seen anyone actually go back and watch the recordings. Chances are that if someone on your team isn’t motivated enough to attend the test, they aren’t going to be motivated enough to watch the recording. Plus, many customers don’t like the idea of being recorded. Now, if your team really sees value in it and is really going to watch the recordings, then go for it—as long as the customer doesn’t object. Otherwise, skip the recordings and focus on watching the live sessions.

Test Your MVP with Customers (Step 6)

HOW TO STRUCTURE THE USER TEST

So you’ve successfully recruited a handful of target customers and have the first one in the room with the moderator. Now, how do you run the test? First off, it’s helpful to prepare a test script that lists what you plan to show and ask the user. This helps you plan ahead to make sure that you cover what you want, that the flow of the test makes sense, and that you manage your time effectively. The test script should identify exactly which design artifacts or parts of the product you plan to show the user, what tasks you plan to ask the user to attempt to accomplish, and what questions you plan to ask the user, all in the desired order. It can help to conduct a pilot test with a team member first to work out any kinks and become comfortable with the flow, especially if you’re nervous about running the test. It can also be helpful to print out the test script (or a shorter outline of it) to have by your side and refer to as you run your tests. User tests typically run about an hour plus or minus 15 minutes, maybe longer if the user is excited about your product and giving you lots of feedback. I recommend spending the first 10 to 15 minutes of the session warming the user up and conducting discovery about his or her needs and current solution. Then I like to spend about 40 to 45 minutes getting feedback from the user on the product or design artifacts. I close with 5 to 10 minutes of wrap-up, where I answer any questions from the user and ask any closing questions that I have. It’s important to start the user test off on the right foot. It’s a good idea to try to spend a minute or two chit-chatting to get to know the person a bit. Building a rapport and making them feel comfortable usually results in the user being more honest with you and giving you more feedback during the test. It’s also important to set some expectations. Most people are nice and don’t want to say critical things, especially right to your face. They know you are probably on the team that designed or built the product, and they don’t want to hurt your feelings. Compensating them for their time can cause a positive bias, too. To help counter all these natural tendencies, it’s important to explicitly tell users up front that you want their honest feedback,



156

The Lean Product Playbook

even if it’s negative. Let them know that they won’t hurt anyone’s feelings. I like to point out that their critical feedback will help make the product better—which is the whole reason for conducting the user test. During the test, it’s important that the user verbalize his or her thoughts so you can hear them. Some people have a natural tendency to do this while others are naturally more quiet and reserved. To help ensure you receive enough feedback, it’s a good idea to explicitly encourage the user to share thoughts out loud as they occur (i.e., stream of consciousness) throughout the user test. This is called the think aloud protocol. If you find a user who is still quiet after you’ve given them this guidance, you can try to remind them again. These tips for the beginning of the user test will help improve your odds of receiving valuable feedback from users, but there’s no guarantee. No matter what, some people will not say anything critical about your product, and others just won’t say that much. You might find that around 10 percent of users who show up are “duds.” If you’re not getting good feedback from a significant percentage of your user tests, then you should reevaluate your screener, test script, or moderator—since this is a sign that one or more of these could probably use some improvement. HOW TO ASK GOOD QUESTIONS

Discovery questions are great for exploring the problem space and your value proposition with customers. You can start by asking them about their current behavior and feelings about the key benefit you plan to provide. For example, if you were Uber, you could start by asking people how frequently they take taxis, what types of trips they take with taxis, and how they find a taxi when they need one. You could also ask them to walk you through the end-to-end details of a recent taxi experience. Then you could ask them what they like and don’t like about their experiences with taxis and their overall level of satisfaction. Notice that you haven’t even mentioned Uber at this point in the interview. You’re just trying to understand the customers’ needs, their current solution, what they like and don’t like about it, and how satisfied they are. You’re trying to discover qualitative information you can use to validate your hypotheses about your target

Test Your MVP with Customers (Step 6)

customer and your value proposition. Discovery questions also help warm the user up to the context of your product before you show it to them. After discovery, you transition to the product feedback portion of the user test. The moderator’s job is to solicit the user’s feedback on the product in an effective manner without perturbing the results. The top way that moderators perturb the results is by asking leading questions, such as “That form was easy to fill out, wasn’t it?” or “So, do you think you would want to click the ‘buy’ button?” Moderators who ask rhetorical and leading questions like this care more about confirming that the product is good than they do about getting actual, authentic feedback. The point of user testing is not to make ourselves feel good; the point is to get objective feedback from real customers. It’s up to the moderator to ensure objectivity. It’s understandable that it can be hard to disassociate yourself and remain impartial when you’re testing a product you’ve worked so hard on—but that’s what you must strive to do. The best moderators engage the user with the product with as little intervention as possible. They refrain from any commentary, and mainly observe and ask questions. If a user takes an action on a prototype but doesn’t verbalize that they did or why they did, a good moderator might say, “I see you just clicked on that button. Could you tell me why?” You’ll notice that instead of just asking the user “why,” the moderator started by stating what he observed. Such “echoing back” is a powerful technique to ensure you understand the user and to probe deeper. For example, if the user answered, “I clicked on the button because I was looking for [_____],” the moderator might ask, “Why were you looking for [_____]?” This is reminiscent of the “five whys” technique. Asking a customer “why” too many times can make them feel defensive, so it’s a good idea to mix it up with other phrases such as “Could you please tell me more about that?,” “Could you please help me understand [_____]?,” or “What thoughts were going through your head when you did [_____]?” It’s common for users to ask the moderator questions during the test. For example, a user might ask a moderator, “So, should I click here to log in?” Rather than replying yes or no, a good moderator might ask, “What would you expect to happen if you clicked there?,” or might say, “Do whatever you would do if you were by yourself.”



158

The Lean Product Playbook

Good moderators often use the judo move of answering a question with a question. ASK OPEN VERSUS CLOSED QUESTIONS

There is a big difference between open and closed questions. Open questions give the customer plenty of latitude in answering. They usually begin with “why,” “how,” and “what.” In contrast, closed questions limit the customer’s possible responses (e.g., to yes or no). For example, asking the closed question “Do you select which flight to book based on price?” is not as good as asking “How do you select which flight to book?” Closed questions often start with “do,” “did,” “is,” “are,” or “would.” Asking open versus closed questions is less a matter of moderator bias and has more to do with the moderator’s skill level. In normal conversation, when you’re not moderating a user test, closed questions are perfectly fine. But as moderator, you have to be mindful of this. Writing your intended questions in advance in the test script can help. But you also have to be able to focus on asking open-ended questions on the fly as well (e.g., in response to something the user did or said). A helpful technique is to get in the habit of saying your next question in your mind before you verbalize it. That way, if it is a closed question, you can change it to an open-ended question before you pose it to the customer. Another error to avoid is embedding a preferred or possible answer in your question. This turns what starts off as an open-ended question into a closed question. For example, I could ask a user, “How would you like the application to sort your transactions? By date?” Sometimes, inexperienced moderators can’t help but eagerly provide what they think is a likely response. Even if the suggestion wasn’t the top reply the user would have told you, he or she may now say yes because you just suggested it. Sometimes this occurs because the moderator doesn’t feel comfortable and is attempting to make things “easier” for the user. Long pauses are going to happen; users need time to process what you are showing them and formulate their thoughts. While such periods of silence would feel awkward in a normal conversation, they’re totally fine during a user test. You should avoid suggesting an answer and just stop talking after you ask a question to keep it open-ended and give users latitude in how they can answer.

Test Your MVP with Customers (Step 6)

They will often surprise you with things you didn’t already know. It’s fine (and can be fun) to try to predict how the customer will reply; but keep your predictions in your head and don’t verbalize them. Again, you want to intervene in the test as little as possible. You will have to start by showing the user the particular part of your product or artifact on which you’d like feedback; but once you do this, you should recede into the background. They may look at the first page and then ask you, “So, what should I do?” I like to reply, “Pretend I’m not here. Just do whatever you would do if a friend told you to check out this product and you were by yourself on your computer at home.” If the person isn’t verbalizing his or her thoughts, you should ask for feedback—for example, “What are your impressions of this page?” I like to let the user interact with the parts of the product that he or she discovers naturally on his own (again, without moderator intervention). But if you want feedback on a certain part of the product that the user hasn’t discovered, then you can ask him or her to navigate there. After doing that, I would again recede into the background. As the user interacts with the product and makes comments, you should ask probing questions as necessary. For example, if the user comments after filling out a form, “That was complicated,” you should follow up by asking, “Could you please tell me why you felt that was complicated?” or saying, “Tell me more about that.” I FEEL YOUR PAIN

If users have difficulty understanding or using your product, it’s important not to help them, as painful as that may feel. Your goal is to keep the test as real as possible; you’re not going to be able to hold every customer’s hand after your product launches, so it’s important for the product to stand on its own. You should simply act as though you were a fly on the wall and not break character during the feedback portion. If users complain or ask questions, you should refrain from explaining confusing text or UI to them, telling them what to click on, or grabbing their mouse and doing it for them (yes, I’ve seen moderators do that). You can let users know that you will address their questions at the end of the test. If the quality or



160

The Lean Product Playbook

UX of your product or artifact is so poor that it prevents users from effectively interacting with it on their own, then you should stop doing user tests and solve those problems. Though it’s not very common, I have seen moderators respond to user criticism by getting defensive and trying to argue with the user’s opinion—or blaming the user for a test that didn’t go well. Such behavior is unproductive and unprofessional. If the user couldn’t understand your product, it’s clearly your company’s fault—not the user’s. WRAPPING UP THE USER TEST

The wrap-up section starts after the feedback portion is over. This is a good time to ask users to reflect on everything they’ve seen and provide overall impressions and feedback. You may want to ask the user to provide some ratings. For example, you could ask, “On a scale of 0 to 10, with 10 being best, how valuable did you find the product?,” or “Based on what you saw today, how likely would you be to use the product?” You could also ask, “How easy to use was the product?” You can ask verbally or you can give the customer a short form to fill out, which may lead to less biased results. I call this “semi-quant” because although you’re asking for numerical ratings, the data will be limited to a small sample size. As you iterate and improve your MVP candidate, you should see ratings improve from one wave to the next. The wrap-up section is also the time to answer any questions that came up during the test or that the customer has at the end. If the user had trouble using the product due to known bugs or issues, you can explain that. This is also when you should give users any compensation for their time and thank them. I usually ask users to sign a form acknowledging receipt of payment. On that form, I will often include prompts for the users to write their email and phone number if they want. I also like to include two yes-no questions: “Would you be willing to participate in future research?” and “Would you like to be notified when this product is available?” These are both meant to be a more honest measure of interest. If a user has nothing but positive feedback during the test and gives your product high ratings but doesn’t circle “yes” for those two questions, they were just being nice.

Test Your MVP with Customers (Step 6)

I ran one test where I didn’t give users any form at the end. After giving them their checks and thanking them, a high percentage asked me when the product was launching, gave me their contact information, and asked me to please notify them when it launched so that they could buy it. The product had tested well, but this additional evidence of product-market fit was a welcome surprise. HOW TO CAPTURE AND SYNTHESIZE USER FEEDBACK

As the user goes through the test, you’re trying to uncover data that supports or refutes the hypotheses you have made to get to your MVP candidate: your target customers, their underserved needs, the differentiators in your value proposition, and so forth. There are three distinct elements of your product that users will give you feedback on: functionality, UX, and messaging. Feedback on functionality has to do with whether your MVP addresses the right benefits or not. Users may complain that a key feature is missing or tell you that a feature you’ve included is not important to them. It’s important to tie such feedback back to benefits and your value proposition. You may have the right feature set that’s addressing the right benefits, but have a poor UX that prevents users from taking full advantage of your feature set. Finally, you may have the right features and UX, but the way you talk about your features, benefits, and differentiators—your messaging—may not resonate with customers. When you receive critical or positive feedback from customers, it can be very helpful to map it to those three high-level categories of functionality, UX, and messaging. Documenting feedback this way after a test allows you to develop a clearer picture of what is and isn’t working well. Let’s discuss an illustrative example where we capture user feedback. Table 9.1 shows a summary of feedback from a wave of five user tests. You can see that I’ve captured the results for each user in a column. I’ve organized the feedback into separate sections for feature set, UX, and messaging. I’ve also captured quantitative ratings for value and ease of use that I asked for at the end of each test. Both positive and critical feedback is included, one item per row. I indicate which users gave each item of feedback with a “Y” for yes. This makes it easy to eyeball patterns across users. In the right column, I’ve calculated overall results for all five users (percentages and median



162 Y

Y


Y




Messaging + Liked the hero figure on our home page − Didn’t understand our tagline

How valuable? (1−10)

How easy to use? (1−10)



Y Y

UX Design − Didn’t see “sign up” link − Had difficulty with registration + Thought the design looked professional

Y Y

Sofia D.

Y Y

Vanessa O.

Tracking Key Results from User Tests

Feature Set + Thought feature X was valuable and unique − Complained that feature Y was missing

TABLE 9.1





Y

Y

Y

Xavier G.





Y

Y Y Y

Y

John G.





Y

Y

Y Y

Rich S.


(median)

(median)

60% 40%

60% 60% 40%

80% 80%

Overall

Test Your MVP with Customers (Step 6)

ratings). In the interest of simplicity, I’m not including in this table any feedback that fewer than 40 percent of users mentioned. You can see that in Wave 1, we received positive feedback from customers on feature X, our professional-looking design, and the hero image on our home page. Wave 1 also revealed four issues: 1. 80 percent of users complained that feature Y was missing. 2. 60 percent of users didn’t see the “sign up” link. 3. 60 percent of users had difficulty with the registration flow. 4. 40 percent of users didn’t understand our tagline. After we act on this feedback to improve our product and conduct a second wave of user testing, we would expect and hope to see progress toward greater product-market fit in three ways. First, we should hear more positive feedback items from a higher percentage of users, especially those related to our value proposition. Secondly, we should no longer hear the negative feedback that we heard in earlier waves. Remember, the users in your new wave never saw the earlier version of your product. So no new user is going to tell you, “Nice job fixing issue [_____].” Instead, you measure such progress by silence—the absence of hearing complaints you heard in prior waves. The third measure of progress is in your key ratings. You should see user ratings for value and ease of use (and any other key metrics) rise between waves. USABILITY VERSUS PRODUCT-MARKET FIT

It’s crucial as you conduct your user tests to differentiate between feedback on usability versus product-market fit. Feedback on usability has to do with how easy it is for customers to understand and use your product, whereas feedback on product-market fit has to do with how valuable they find your product. You’ll notice at the bottom of Table 9.1 that I included a rating question devoted to each of those two attributes. You may get a lot of feedback from customers early in the design of your MVP that your UX needs improvement. In that situation, poor usability often prevents users from seeing the full value your product provides. You may discover that you have bugs that get in the way, too. Messaging that doesn’t resonate with customers can also be a stumbling block.



164

The Lean Product Playbook

As you eliminate those dissatisfiers, the value of your product can better shine through, and you can get a more accurate read of product-market fit. After making many improvements, you may get to the point where users get through your tests easily, without running into any usability issues. However, you should not infer from those results that you have product-market fit. You need to explicitly assess product-market fit by asking how much they value your product. I experienced this firsthand working on a product that provided users with real-time news tailored to their interests. In the first wave of tests on our rough live product MVP, users provided feedback on lots of usability issues. We also discovered a few bugs and some unclear messaging in the tests, too. After we fixed those issues, we heard a smaller number of issues in the second wave, which we also fixed. In the third wave, the user tests starting going much better. Most users sailed through the test with no problems, which made me excited about our progress. I started asking users at the end of each test how likely they would be to use our product. Even though the tests went well, around 20 percent of users said they wouldn’t use it. This result surprised me, mainly because the tests had gone well and hadn’t garnered much negative feedback. Also, most of the customers with whom we tested expressed a certain amount of interest in a personalized news product, so I felt that they fit our target customer profile well enough. I then asked the 20 percent why they wouldn’t use our product, and I learned that a segment of users have a strong preference for getting their news a certain way. This was a great, unexpected insight, which led me to start asking people how they preferred to get their news during the discovery questions at the beginning of my interviews. I discovered that there were three very different ways that people preferred to get their news—and our product approach had been designed to most resonate with one of those ways. Learning this helped us make more sense of the market. Online news is a mainstream consumer product with a large audience, so it seems natural that the market would contain different segments with distinct preferences. The team agreed that a design that tried to address all three different ways would be schizophrenic and not make any of the three types of users happy. So we used what we learned to refine our target customer definition.

Test Your MVP with Customers (Step 6)

This example shows how usability issues can prevent you from assessing product-market fit and how great usability does not mean you have strong product-market fit. It also shows how user testing can help you validate and refine your hypotheses (in this case, who your target customer is). User testing is a powerful tool in the Lean toolkit. Done well, you can get very valuable feedback on your hypotheses at multiple levels of the Product-Market Fit Pyramid: underserved needs, value proposition, MVP feature set, and UX. However, it’s important to note that user testing is inherently based on the assumption that you are talking with the right type of customer. It’s very important that you ensure that the customers with whom you are talking are in your target market. You can do a great job defining your value proposition and MVP feature set, design an amazing set of clickable wireframes, and run your tests perfectly. However, if you are talking to the wrong type of customer, you will not get the data you need. In fact, you may get bad data that is very different from what your target customers would have told you. Iterating your MVP based on data from the wrong type of customer can send you in the wrong direction—heading off a cliff instead of toward the Promised Land of product-market fit. If your user tests are showing that you don’t have product-market fit once you get past any major usability issues, you might need to revisit your hypotheses about your value proposition, MVP, or UX design. But it could be the case that you need to revisit your hypotheses about your target customer, so keep an open mind to that possibility. In the next chapter, I discuss how to use the data you capture from user testing to improve your MVP candidate and how to use rapid iteration to achieve product-market fit.



Chapter 10

Iterate and Pivot to Improve Product-Market Fit I explained in the previous chapter how to conduct a wave of user tests to assess your MVP’s product-market fit. This chapter is about what to do after you complete each round of testing. Lean is about learning and iterating quickly. This means that you want to use what you have learned after you receive a round of feedback to modify your hypotheses and your MVP so that you can test them with customers again. You want to iterate quickly from one round of user testing to the next with the goal of improving product-market fit each time. This chapter will walk you through how to do that. THE BUILD-MEASURE-LEARN LOOP

Eric Ries discusses the above concept of iterative learning in his book The Lean Startup (The Lean Startup is a registered trademark of Eric Ries). His “build-measure-learn” loop has helped many people understand the importance of iteration and validated learning. But based on my observations of how some people talk about and try to apply the loop, there are some nuances worth discussing. It’s important to clarify that “build” doesn’t mean that you have to actually build a product. Creating a set of clickable wireframes that you test with users is perfectly acceptable. “Build” simply means having something that you can test with customers, which could be a live product or design artifacts, such as wireframes or mockups. “Design something to test” is a broader, more accurate description, so I prefer the label “design” for this step. The goal is to identify and create what will let you test your hypotheses while consuming the least resources. “Measure” implies numerical data—but keep in mind that “measure” doesn’t have to be as quantitative as it sounds. Many


168

The Lean Product Playbook

people have a bias for trying to prove things with quantitative data. I agree that’s nice when you can do it, but A/B testing is not the only way to test hypotheses or gain learning. All information you gain by observing customers falls under “measure.” Even though they aren’t statistically significant, the results of qualitative testing fit into “measure,” too. The key point is that you are testing your hypotheses with customers. Therefore, “test” would be a broader and more accurate label for this step. The “learn” step is interesting. There are actually two things going on in this step. First, you are learning new things from the results of each test. Second, you use what you learn to modify the hypotheses that led to the test you just ran. It makes things clearer to split “learn” into two distinct steps: “learn” and “hypothesize.” In fact, if you think about it, this whole process doesn’t start with “build”—it starts with some initial hypotheses. How else would you have a basis for deciding what to build? THE HYPOTHESIZE-DESIGN-TEST-LEARN LOOP

For the above reasons, I use a modified version of the build-measurelearn loop that I call the hypothesize-design-test-learn loop—shown in Figure 10.1. As you go through this loop, you transition from problem space to solution space and back again. You start with the “hypothesize” step, where you formulate your problem space hypotheses. In the “design” step, you identify the best way to test your hypotheses. Creating a design artifact or product based on your hypotheses takes us from the problem space to the solution space. In the “test” step, you expose your product or artifact to customers and make observations, which lead to validated learning (the “learn” step). You complete the loop by using this validated learning to revise and improve your hypotheses. These revised hypotheses will inform your next iteration through the loop. To summarize: you test and improve your problem space thinking by showing customers a product or design artifact in the solution space and soliciting their feedback on it. The more quickly you can learn, the more quickly you can deliver additional customer value and improve your product-market fit. But learning is just one of the steps in the process. In order to gain additional learning, you have to go around the entire loop again.

Iterate and Pivot to Improve Product-Market Fit

FIGURE 10.1

The Hypothesize-Design-Test-Learn Loop

If you think of the loop as the Monopoly game board, “learn” is the “Go” square that you pass each time around. In the game, you earn $200 for passing “Go”; in the Lean Product Process, you earn validated learning. The “learn” and “hypothesize” steps tend to be fairly quick, so your speed through the loop is usually governed by how quickly you can design and test. As you validate and invalidate your hypotheses and form new ones, you should refer to the Product-Market Fit Pyramid, shown again in Figure 10.2. For each hypothesis, you should identify to which layer of the pyramid it corresponds. Each layer builds on top of the layer of hypotheses below it. It’s easier to make changes near the top of the pyramid, but changing hypotheses near the bottom can have significant ramifications for higher layers. For example, having to change a page’s UX design to make it more usable is relatively minor. Let’s say instead that your value proposition presumed that a certain customer benefit wasn’t important, but you learn from users that it actually is. You will now need to modify your value proposition, which will



170

The Lean Product Playbook

FIGURE 10.2

The Product-Market Fit Pyramid

impact your MVP feature set, and your UX design. This is why you want to focus on addressing the issues at the lowest level first as you process what you learn in your user testing. Once you validate that you have eliminated those issues, you can then focus on addressing issues at the next higher level. ITERATIVE USER TESTING

As I discussed in Chapter 9, each user test is going to give you valuable information about your MVP. It is helpful to debrief each test with the product team soon afterward to share observations and synthesize the learning. I recommend using a table like Table 9.1 to capture key observations from each wave of user testing. At the end of each testing wave, you want to look across all the users to see how many gave the same feedback, either positive or negative, which you can express as a percentage. Those percentages should help you prioritize the changes you make to your MVP. If all or most of the users in the wave raised an issue, then addressing it should be higher priority. If only one or two users mentioned an item, it can be lower priority. You should identify which items you plan to address before the next testing round.

Iterate and Pivot to Improve Product-Market Fit

Wave 1

Let’s continue with the example that was introduced in Chapter 9. First, we’re going to summarize the test results from those five users in Wave 1 into a single column. See the Wave 1 column in Table 10.1. You’ll notice that I removed the positive feedback, in order to keep the example simple. We discovered four issues in Wave 1: 1. 80 percent of users complained that feature Y was missing. 2. 60 percent of users didn’t see the “sign up” link. 3. 60 percent of users had difficulty with the registration flow. 4. 40 percent of users didn’t understand our tagline. Let’s go through each of the issues from Wave 1. The fact that feature Y is missing is an MVP feature-set issue. In this case, it turns out that we had considered feature Y, thought it was valuable, and

TABLE 10.1

Tracking Results across Multiple Waves of User Testing Wave 1 Wave 2 Wave 3 Wave 4

Feature Set − Complained that feature Y was missing − Said features X and Y should work together

80%

0%

0%

0%

N/A

80%

0%

0%

60% 60%

0% 40%

0% 0%

0% 0%

N/A

80%

40%

0%

Messaging − Didn’t understand our tagline

40%

0%

0%

0%

How valuable? (1−10, median) How easy to use? (1−10, median)













UX Design − Didn’t see “sign up” link − Had difficulty with registration flow + Thought that feature Y was hard to use



172

The Lean Product Playbook

planned to build it later, but didn’t think it was critical to include in our MVP. Now that we have learned from customers that they need it, we decide to add it to our MVP. Our designer comes up with a design for feature Y and we add it to our design artifacts. The fact that users cannot see the “sign up” link is a visual design issue. To address this, our visual designer puts the link in a more prominent position, makes it bigger, and renders it as a button using a color that pops on the screen. She updates our design artifacts accordingly. Difficulty with the registration flow is an interaction design issue. Our interaction designer addresses the problems that users experienced by coming up with a revised registration flow and updating our design artifacts. The fact that a large percentage of customers didn’t understand our tagline is a messaging issue. It turns out that the specific wording we used didn’t convey the meaning we had intended. As a result, the tagline didn’t effectively communicate what we view as our differentiating customer benefit. We brainstorm alternative taglines, identify our new favorite, and update our design artifacts accordingly. Wave 2

Now that we have addressed the four issues we saw in Wave 1, we are ready to test again with a new wave of users. We test our new wireframes with five more users and see the results shown in the Wave 2 column of Table 10.1. After adding feature Y, we see that none of the new users complained about it missing, so that represents progress. We also see that all five users in Wave 2 saw the “sign up” link now, which is a big improvement. However, 40 percent of users are still complaining that the registration flow was difficult, even though we redesigned it. This happens. The first time you revise your product based on customer feedback, you don’t always get it perfect. In the Wave 2 tests, we saw that users no longer encountered some of the specific UX problems that were experienced in Wave 1. But our fix for one of the previous UX issues didn’t work as well as we thought it would. Plus, we saw minor issues with some of the new design elements. Given these results, we decide to have a cross-functional team meeting to share the issues we are still

Iterate and Pivot to Improve Product-Market Fit

seeing with the registration flow, brainstorm possible solutions, and identify the best ones. We come up with a new version of the flow that we think should be much easier and incorporate it into a new version of our design artifacts. We also see that our new tagline didn’t have the issues that we saw in Wave 1, so we are excited by that result. It’s great to see an issue go from a high percentage in one wave to zero percent in the next wave after making a fix based on what we learned. That usually means you have adequately addressed that issue and can focus on others. After you eliminate an issue, you may discover new issues with your updated MVP. Case in point, our Wave 2 customers were happy that our MVP had feature Y, but 80 percent of them felt that this feature was hard to use. We discuss the detailed usability problems we saw users experience, come up with an improved design for feature Y, and update our design artifacts accordingly. We were also surprised to learn from 80 percent of customers that they want feature Y to work with feature X—since we had added feature Y as a new, standalone feature. In hindsight, what we learned from users about how the two features should work together makes a lot of sense and makes both features more useful. We revise the designs of the two features and update our design artifacts. We should see the magnitude and number of issues decrease as we iterate. In this case when going from Wave 1 to Wave 2 we successfully addressed three issues (feature Y, “sign up” link, and tagline). We tried but were unsuccessful in solving the registration flow issue. And we discovered two new issues: Feature Y is hard to use, and it should work with feature X. We should also see our overall ratings improve as we iterate. In this case, we can see that our value rating increased from 7 to 8—most likely due to the addition of feature Y. And our ease-of-use rating increased from 5 to 6, likely as a result of the improved “sign up” link and our partial improvement to the registration flow. Wave 3

Given what we learned from Wave 2 and our updated design artifacts, we’re ready for Wave 3. We conduct tests with another five customers and get the results shown in the Wave 3 column in Table 10.1.



174

The Lean Product Playbook

We didn’t get any complaints about features X and Y not working together, so we accomplished our mission on that front. We see that after our second attempt to redesign the registration flow, all five users got through it without complaining about it being difficult—which shows we made significant progress. Even though we redesigned feature Y, 40 percent of users still thought it was hard to use. That’s down from 80 percent, which is good; but we still have some work to do since it is such an important feature. As a result of our product improvements, our value rating increased to 9, and our ease of use rating improved to 7. We’ve made good progress since our initial MVP. We’re no longer getting major feedback on missing functionality. Our messaging seems solid. We’re mainly getting feedback on the need to improve our UX design, which is common early in the life of a product. I’ve kept my example simple by using only a small number of major feedback items. When we test with users, we will also receive a large number of minor feedback items. We can and should incorporate improvements to address those as well. We should see our product-market fit improving as we iterate through the hypothesize-design-test-learn loop. Wave 4

We decide at this point to further improve the design of feature Y to make it easier to use and conduct a fourth wave of testing. We see in the Wave 4 column of Table 10.1 that no one complained about feature Y being hard to use in that wave. And we didn’t discover any new major issues. Our value rating stayed at 9 and our ease-of-use rating improved to 9. At this point, we should feel good enough about our MVP design to proceed to the next step in our product process. If the artifacts we tested with users were high fidelity (e.g., clickable mockups on InVision), we would proceed to building our MVP. If the artifacts we tested were low fidelity (e.g., clickable wireframes), we could proceed to clickable mockups. In certain cases, we might choose to skip high fidelity design and go straight to coding if we felt really confident about our design and didn’t think there would be much risk from skipping user testing after visual design. This could be the case if we

Iterate and Pivot to Improve Product-Market Fit

were adding new functionality to an existing product and already had a visual style guide that we could easily apply. There is no hard-and-fast rule to determine when you’ve validated your MVP “enough.” There certainly is a risk of continuing to test past the point where it is of much value. You want to avoid analysis paralysis. Conversely, you can launch before you’ve validated enough, which can result in the need for painful rework on the design and coding fronts. So you want to try to strike the right balance. At some point, though, your baby bird needs to leave the nest; that is, you need to stop testing design artifacts and build your MVP. This is an exciting transition. It puts you that much closer to delivering real customer value with a live product, and also enables the next level of testing your product with customers. Testing with design artifacts is valuable to validate your assumptions and ensure that you’re achieving product-market fit. Testing with a live product is even better. When you test artifacts, customers are telling you what they would do if the product were live. But what customers say they will do and what they actually do can be quite different—and actual customer behavior trumps customer opinions any day. In addition, your live product is the highest fidelity possible. Lower fidelity artifacts may lack some of the details that your final product contains. Or deviations from the design artifacts may have been introduced in the process of building your product. Once you build your live product, it’s best to conduct another wave of tests to see where you stand. Hopefully you measure the same or a higher level of product-market fit from your last wave of tests with design artifacts. If not, you should iterate through the hypothesizedesign-test-learn loop with your live product until you do. Many companies use a private beta for this phase, so that only a limited number of customers can see the product until it is ready for prime time. PERSEVERE OR PIVOT?

I’ve painted a pretty rosy picture. Sure, it takes several waves of iteration and hard work, but you’ll get to product-market fit eventually, right? Unfortunately, many teams don’t have that experience. When they test with users, they don’t get glowing feedback. They try to



176

The Lean Product Playbook

iterate, but don’t make progress with customers. They feel like they’ve hit a brick wall. Several things can go wrong along the path I’ve described. One or more of your hypotheses may be incorrect. Or even if your hypotheses are correct, your execution in designing, building, or marketing your product may fall short. If you find that you are not making progress as you try to iterate, I recommend you pause and take a step back. Brainstorm with your team about what all the possible problems could be. Map each problem back to the corresponding layer of the Product-Market Fit Pyramid in Figure 10.2. You may find that you are iterating at a higher level than where the true problem lies. For example, if your hypothesis about your target customer is wrong, iterating your UX design won’t make much difference. You want to start at the bottom of the pyramid and work your way up until you identify which of your hypotheses are incorrect. When you change one of your main hypotheses, it’s called a pivot. A pivot is larger in magnitude than the change you normally see as you iterate along the path you have chosen; it means a significant change in direction. For example, switching to a completely different target customer would be a pivot. Deciding to change the differentiators in your value proposition would be a pivot. Making tweaks to your UX design is not a pivot. There are many examples of successful pivots. Photo-sharing site Flickr began as “Game Neverending,” a web-based massively multiplayer online role-playing game focused on social interaction. After the company added a tool that made it easy to share photos on web pages, they saw how much customers loved using it. The company pivoted and launched photo application Flickr in February 2004, which experienced incredible growth and was acquired by Yahoo! in March 2005. Photo-sharing app Instagram began as “Burbn,” an HTML 5 social app that combined elements from check-in app Foursquare and the game Mafia Wars. After reimplementing Burbn as a native iPhone app, the cofounders felt it was cluttered with too many features. They decided to build a new app from scratch, cutting everything except for the photo, comment, and like capabilities. They launched Instagram in October 2010, which experienced tremendous growth and was acquired by Facebook for approximately $1 billion in April 2012.

Iterate and Pivot to Improve Product-Market Fit

One of the hardest parts of the Lean Product Process can be deciding whether to persevere with the opportunity you are pursuing, pivot to a new opportunity, or stop altogether. Let’s get that last one out of the way first. You don’t have all the time in the world to achieve product-market fit—resource constraints usually limit how much time you have. In a startup setting, you have to rely on external funding from investors before you are profitable. If you don’t achieve product-market fit or make significant progress toward that goal, it can be challenging to raise the next round of investment. Even new product efforts within a successful company have fixed budgets as well as timeframe expectations for making progress. I’ve seen some startups that always seem to be pivoting. You shouldn’t change direction every time you hit a rough patch, nor should you drop what you’re doing to chase each cool new idea you come up with, also known as shiny object syndrome. I like to joke that if you’ve pivoted three times, you’re heading in the opposite direction from where you started. At the other extreme are people who stubbornly keep banging their head against the wall and don’t take a step back to reevaluate. So how do you decide whether to persevere or pivot if you still have cash in the bank and time on the clock? You should consider pivoting if you just don’t seem to be achieving gains in product-market fit after several rounds of trying to iterate. If, despite your best efforts, your target customers are only lukewarm on your MVP, you should consider a pivot. Said another way, if you haven’t yet identified a customer archetype that is very excited about your MVP, then you should consider pivoting. Sometimes the best way to pivot becomes relatively clear from your tests. For example, you might find that a less central part of your value proposition is what most resonates with customers. In this case, you should trim the rest and focus your efforts on that part. Or you may discover your target market consists of distinct submarkets and learn that one of those submarkets really loves some aspect of your value proposition. Figure 10.3 uses a mountain climbing analogy to explain productmarket fit and pivoting. You start out at the bottom of the first mountain, which represents the market opportunity you are pursuing based on your target market and value proposition hypotheses. The higher



178

The Lean Product Playbook

FIGURE 10.3

Pivoting to Achieve Higher Product-Market Fit

you climb, the stronger your product-market fit. After your first wave of testing, you learn a lot and improve your product. You see in your second wave that you have improved product-market fit; but you get much smaller gains in your next wave. Your product is better than when you started, but you haven’t managed to reach a high enough level of product-market fit. You try different things in your next two waves but can’t seem to make any progress. In the process of user testing, you discover an adjacent market opportunity represented by the second mountain. This second mountain is taller than the first because the amount of market value that can potentially be created is greater. You decide to revise your hypotheses and pivot to pursue this new market opportunity. You iterate through the hypothesize-design-test-learn loop and find that you are able to improve your product-market fit, reaching much higher heights. This analogy serves as a reminder to pay attention to how high up the mountain you are climbing (your level of product-market fit) as you iterate. Try to measure your rate of ascent (improvement in product-market fit) after each wave of user testing. If it feels like you are not making much progress, try to find other paths up

Iterate and Pivot to Improve Product-Market Fit

the mountain (revisit and revise your hypotheses). If, after doing that, you’re still not making decent progress, stop to reconsider the mountain you’re on (your hypotheses about your target customer and value proposition) and look around for other mountains that might be easier to climb (other market opportunities). Pick your new mountain (pivot) and try to climb up that one (iterate to greater product-market fit). Before concluding Part II of this book, I want to walk you through the details of a real-world example where I applied the Lean Product Process, which I share in the next chapter. In that example, I pivoted from one mountain to another after the first wave of user testing. I will walk you through my decision to pivot and show how pivoting resulted in much higher product-market fit.



Chapter 11

An End-to-End Lean Product Case Study Now that I’ve described each of the six steps of the Lean Product Process in detail, I want to walk through a real-world case study to further solidify the concepts I’ve covered. I’ve shared this example in talks and workshops that I’ve given, and many participants have told me how helpful it is to see the application of the Lean Product Process with an end-to-end example. MARKETINGREPORT.COM

One of my clients asked me to help define and evaluate a new product called MarketingReport.com. This client’s company had a successful consumer web service and was contemplating a new web service to pursue a potential market opportunity. I worked closely with two company executives and a UX designer on this project. The new service idea centered on a widespread customer problem associated with direct mail—namely, that many people who receive direct mail do not find it valuable and consider it a nuisance. The executives had some insight into the direct mail industry and knew that the mailings were targeted based on marketing databases that profiled customers. For example, you might receive an unsolicited coupon for cat litter from a certain pet store chain because a marketing database somewhere indicates that you have (or are likely to have) a cat in your household. The idea was to solve this problem by providing a product that gave customers transparency into the profile that marketers had built of them and empower them to make that profile more accurate. So, if I don’t own a cat but own a dog, I could correct the marketing databases so that I receive coupons for dog food instead of cat litter. The executives saw parallels with the credit industry.


182

The Lean Product Playbook

Every day, thousands of credit-related decisions are made based on people’s credit scores. Before the advent of credit reporting services, consumers didn’t have much visibility into why they had the score they did; their credit worthiness was based on “behind the scenes” data about their credit history. They might therefore get declined for a loan and not know exactly why. Inaccurate data about their past payments—such as a loan payment reported as unpaid that actually wasn’t—could negatively impact them. By providing transparency, credit-reporting services enable customers to see the data behind their credit rating and correct any inaccuracies. MarketingReport.com would do for personal marketing data what credit reports had done for personal credit data. The initial idea was to provide the service for free and to monetize the marketing data that the service generated. By giving customers access to the data, and the ability to correct inaccuracies and provide additional information, we planned to build a collection of rich and accurate profiles. Therefore, it was critical to define a service with which customers would want to engage. STEP 1: DETERMINE YOUR TARGET CUSTOMERS

You’ll recall that Step 1 of the Lean Product Process is to identify your target customer. We agreed at this early point that this would be a mainstream consumer offering. Steps 1 (target customers) and 2 (customer needs) are closely related, so we didn’t narrow our target market hypothesis any further than mainstream consumers at this point. We knew we would refine our target customer hypothesis as we gained additional clarity about the customer benefits we could deliver. STEP 2: IDENTIFY UNDERSERVED NEEDS

We then started working on Step 2: identifying underserved customer needs. Both executives agreed that the service’s core benefit was empowering customers to find out what “they” (the direct marketing databases) know about the customer. However, there were a lot of different ideas for what the service would do beyond that core benefit. So we brainstormed a long list of different potential

An End-to-End Lean Product Case Study

customer benefits that our service could deliver, which included these five ideas: 1. Discover money-saving offers of interest to me 2. Reduce the amount of irrelevant junk mail I receive 3. Gain insights into my spending behavior 4. Meet and interact with other people with similar shopping preferences 5. Earn money by giving permission to sell my marketing-related data To identify which customer benefits we wanted to pursue, we came up with a set of evaluation criteria, some of which were positive and others negative. We evaluated each benefit on the following criteria: Strength of user demand (+) Value of marketing data obtained (+) ● Degree of competition (–) ● Effort to build the v1 product (–) ● Effort to scale the concept (–) ● Fit with the company’s brand (+) ● Amount of reliance on partners that would be required (–) ● ●

We scored each customer benefit on these criteria based on our estimates, which allowed us to weed out less appealing ideas. Customer benefits 1 through 4 from the above list were considered worth further consideration. STEP 3: DEFINE YOUR VALUE PROPOSITION

At this point, I wanted to nail down which benefits were in scope versus out of scope for our envisioned product so we could solidify our value proposition. So I led the executives in an exercise to map out the problem space for our product, shown in Figure 11.1. You can see that I grouped related benefits together. We had our core benefit of finding out what “they” know about me (in the middle). A second cluster of benefits (at the top) included reducing junk mail and saving trees (being friendly to the environment). A third cluster of benefits (at the bottom) included saving money on



184

The Lean Product Playbook

FIGURE 11.1

Initial Value Proposition for MarketingReport.com

purchases, gaining insights into my spending, and interacting with similar shoppers. I felt that the three clusters on this problem space map were too much to bite off in a single product. Plus, it wouldn’t feel coherent if we tried to build one service that addressed all these benefits; the top cluster and bottom cluster were very different. Additionally, while one executive liked the top cluster of benefits more, the other preferred the bottom cluster. I thought they were all good ideas, so I recommended that we pursue two distinct product concepts, each with its own value proposition. The first concept, dubbed “Marketing Shield,” would consist of the top two clusters. The second concept, dubbed “Marketing Saver,” would consist of the bottom two clusters. By using this approach, each concept included the core benefit of “find out what ‘they’ know about me” but wasn’t too broad in scope. The executives agreed. In Chapter 5, I recommend that you articulate your product value proposition using the Kano model to classify each benefit as a must-have, performance benefit, or delighter, while taking your

An End-to-End Lean Product Case Study

competition into account. We viewed the core benefit of “find out what ‘they’ know about you” as a delighter because this type of service didn’t exist. We knew that there were other products in the market that gave customers money-saving offers; so we viewed that as a performance benefit. Similarly, although social networking products existed, they weren’t necessarily focused on shopping; so we viewed that as a performance benefit as well. We weren’t aware of other products that let you compare yourself financially to others, so we viewed that as a delighter. On the Shield front, we viewed reducing junk mail as a delighter. There were plenty of other ways to be environmentally friendly, so we viewed “save trees” as a performance benefit. STEP 4: SPECIFY YOUR MVP FEATURE SET

Now that we had the value proposition for each of our two concepts, we started talking about the solution space and brainstormed features that would deliver those benefits. See Figure 11.2 for the features that we settled on for each product concept. The main feature for the core benefit of “find out what ‘they’ know about me” was a marketing report containing a collection of marketing-related information about the user built over time. The report originates from data about a customer’s purchases and their responses to surveys, mailings, and phone calls. The idea was to provide customers with transparency into the data that the marketing databases contained about them. Two key components we envisioned for the marketing report were the marketing profile and the marketing score. The profile was based on a set of consumer segmentation clusters used by direct marketing firms. Each cluster has a catchy, descriptive name—like “young digerati,” “soccer and SUVs,” or “rural retirees”—and is based on key demographic data such as age, marital status, home ownership, children, and zip code. Marketers use these profiles to target relevant offers to people. We were inventing the idea of a marketing score from scratch. It was intended to be analogous to a credit score—a single number that represents your overall credit worthiness. In the same way, the marketing score was a single number that represented your overall



186

The Lean Product Playbook

FIGURE 11.2

Features for Marketing Shield and Marketing Saver

attractiveness to marketers. A higher credit score garners you a better interest rate; a higher marketing score would garner more and better money-saving offers. We identified several factors that would go into determining a customer’s marketing score. Turning to the bottom cluster, the main benefit was money-saving offers. The idea was that the customer could identify what types of products and services interested them and would then receive relevant money-saving offers. We would basically be playing matchmaker between consumers and companies who wanted to promote their products. The feature would consist of a user interface where the customer could specify their preferences, a marketplace of vendors, matchmaking logic, and delivery of the offers via the website and email. The second feature in the bottom cluster was comparing yourself to others. The idea was that customers could compare their spending

An End-to-End Lean Product Case Study

patterns with similar customers to see if they are spending more or less on certain areas such as dining, clothing, entertainment, and so forth. Gaining this insight would allow them to modify their spending behavior as they saw fit. The third feature in the bottom cluster was the ability to interact with similar shoppers. The idea was that customers might discover new products or learn about great deals from similar shoppers. Social networking was relatively hot at the time and we wanted to experiment with some social functionality related to online shopping. The feature in the top cluster was a service that would block junk mail. This addressed both the “suppress junk mail” and “save trees” benefits. The idea was that we would start out with a “Wizard of Oz” MVP by manually filling out and submitting “do not mail” requests on behalf of customers. We would eventually transition to a more automated solution if warranted. STEP 5: CREATE YOUR MVP PROTOTYPE

With our feature set defined, it was time to move on to Step 5 to bring these features to life with some design artifacts. We knew that we wanted to test design artifacts with customers in person before we did any coding, so we decided to go with medium fidelity mockups. The mockups had enough visual design—coloring, fonts, graphics, and styling—to effectively represent the product to users, but we didn’t worry about making them pixel perfect. We started by thinking through and defining the product’s structure (information architecture) and the flow of the customer experience (interaction design). The customer would start by receiving an email describing the service. The email’s call to action was “see your marketing report” with a link to a landing page, which further described the service and had a key conversion button labeled “see report.” Each customer was assigned a unique code that was included in the email. After clicking the “see report” button, the customers were taken to the “verify information” page. This page contained a form listing their name, address, marital status, household income range, and other key demographic information. This page showed customers



188

The Lean Product Playbook

the information contained in marketing databases and allowed them to correct any inaccurate information. After clicking the “continue” button on this page, they were taken to the Marketing Report page. The Marketing Report page was the main page of the product after going through the onboarding flow. It was a dashboard of modules that enabled us to use the same general design for the two product concepts by just swapping out different modules for the “Marketing Shield” versus the “Marketing Saver.” For both product concepts, the Marketing Report dashboard included modules for the Marketing Profile and the Marketing Score, since these were the core feature ideas. The remaining modules would vary by concept. The Marketing Shield version had a “block junk mail” module. The Marketing Saver version had modules that covered money-saving offers, comparing yourself to others, and social networking. The Marketing Report page was the hub from which customers would navigate. They could click on each module to drill down to a more specific page dedicated to that topic. For example, the page dedicated to money-saving offers allowed the customer to select which types of products and services were of interest to them, such as vacations, electronics, and so forth. The page dedicated to “block junk mail” displayed a list of different categories of direct mail and allowed users to select which ones they no longer wanted to receive. That page also included an upsell offer to “Marketing Shield Premium,” a paid service that would further reduce the amount of junk mail you received and provide greater privacy for your marketing profile data. With the exception of this upgraded offering, the “Marketing Saver” and “Marketing Shield” services were intended to be free to customers. STEP 6: TEST YOUR MVP WITH CUSTOMERS

With our mockups done, it was time to proceed to Step 6: testing with customers. At this point, we needed to revisit and refine our target customer definition before we started recruiting. Since we had two distinct MVPs, each with its own value proposition, we had to define the target customer for each. The target customers for the Marketing Shield remained mainstream consumers, but we refined our definition to be people who highly valued their privacy. The target customers

An End-to-End Lean Product Case Study

for the Marketing Saver also remained mainstream consumers, but we refined our definition to be people who place a lot of value on saving money on their purchases and getting good deals. Recruiting Customers in our Target Market

We decided to use in-person moderated testing, and I moderated the sessions. In the interest of collecting data more quickly, I spoke with two or three customers at a time instead of one-on-one. I selected a local research firm to recruit customers for us, and they used a screener that I created to qualify customers. Let’s walk through the screener questions I used and the rationale behind them. First off, I wanted the research subjects to be employed full-time (at least 30 hours per week) to ensure that they were in our target market. Many unemployed or retired people participate in market research because they have ample free time, but they wouldn’t necessarily be representative of our target market. I required at least a high school diploma and recruited a balance of education levels, consistent with our mainstream audience. I also recruited for a mix of household incomes with a minimum of $40,000. They also had to have a computer in their household and use the Internet a minimum number of hours per week (since our service was going to be delivered via the web). We also wanted to make sure they had recently purchased a product on the Internet (so that they would be comfortable paying online for our service). I viewed all those requirements as ensuring the person was a mainstream, working adult in the target market for web-based services. Next, I had to decide how to ensure the person was in the target market for the particular concept (Saver or Shield). I decided to use past behavior as an indicator for fit with our two target markets. Since the distinct benefit of Marketing Saver was saving money, for that group I asked about several different money-saving behaviors: Have you used three or more coupons in the past three months? Are you a Costco member? ● Have you made a purchase on eBay in the past six months? ● When making purchases, do you usually or always spend time researching to make sure you’ve found the lowest price? ● ●



190

The Lean Product Playbook

The respondent earned one Saver point for each “yes” response. We considered anyone with two or more Saver points to be in the Saver target market. Similarly, since Marketing Shield was about privacy and security, I asked that group about several behaviors related to those topics: Have you ever asked to be put on the “do not call” list? Do you have caller ID blocking? ● Do you own a paper shredder at home? ● Have you paid for antivirus software in the past six months? ● ●

Respondents earned one Shield point for each “yes” response—and we considered anyone with two or more Shield points to be in the Shield target market. We did not select respondents that failed to qualify for either of the two segments. A small number of respondents qualified for both segments, since the criteria were not mutually exclusive (i.e., it is possible to care about saving money and about privacy). Once respondents qualified for the research, we also asked them for their address, age, marital status, number of children, home ownership, household income, education level, occupation, and ethnicity. We used this information to create a personalized version of the “verify information” page mockup for each customer. We also used this information to tailor each customer’s Marketing Profile to the matching segmentation cluster. This personalization of the mockups allowed us to develop a much more realistic experience for customers in our tests than we’d be able to do using a generic page and asking them to use their imagination. In fact, most customers expressed surprise when they first saw this page: They asked, “How did you get all this information?” We successfully provoked the realization in customers that “they” (the marketing databases) really do know a lot about you, which was the core value proposition for Marketing Report.com. We worked out the days and times when we would hold research sessions. Since we were talking to people who worked full-time, we selected times later in the day after working hours (6 and 8 p.m.) to better accommodate their schedules. One common mistake companies make is to force research sessions during their working hours

An End-to-End Lean Product Case Study

for their convenience. If it’s not a problem for your target customers to meet at that time, that’s fine. But meeting during the workday is inconvenient for most working adults. As a result, you can skew the type of customers who show up to your research to the point of not being truly representative of your target customers. To make scheduling more efficient, we asked respondents when we screened them to let us know all of the session times that they would be able to attend. Once we had all this info for all the respondents, scheduling them later to fill all our time slots was easy. The research firm successfully recruited customers for all our slots. User Testing Script

While the recruiting was taking place, I created the script I planned to use in moderating the user testing. Each session was 90 minutes long. Here is the high-level outline of my user testing script showing the time allocation: 1. Introductions and warm-up (5 minutes) 2. General discovery questions (15 minutes) a. Direct marketing mail b. The data about you that companies have c. Comparing yourself to others financially 3. Concept-specific questions (45 minutes total) a. Discovery questions related to concept’s main theme (10 minutes) b. Feedback on concept mockups (35 minutes) 4. Review: What did you most like/dislike about what you saw? (5 minutes) 5. Brainstorm: What would make the product more useful/ valuable? (10 minutes) 6. Feedback on possible product names (10 minutes) 7. Thanks and goodbye To test out my script and our mockups, I ran a pilot user test first with someone from the company before the first session with real customers. Based on the pilot test, we made some tweaks to my questions and to the mockups. Then we were ready to go!



192

The Lean Product Playbook

On each of three evenings, I moderated two sessions with three customers scheduled for each. Half the sessions were for Marketing Saver and half were for Marketing Shield, so nine customers were scheduled for each concept. We ended up having two no-shows, so we spoke with eight customers for each concept. As I mentioned, we personalized the data in the mockups for each customer. Because this was before the days of clickable mockups, I printed out each mockup—one per page. I put them in a stack in front of each customer in the order that they would be seen. I followed my script, and as the customers navigated through the mockups, I flipped the pages. What We Learned from Customers

The sessions went well. The customers were engaged and articulate. We received a lot of great feedback—in fact, so much that I typed eight pages of notes to capture everything we learned. The bottom line is that neither concept was appealing enough to customers. However, there were a few rays of sunshine that managed to poke through the clouds. The core part of the value proposition in both concepts—“find out what ‘they’ know about me”—only had limited appeal. Customers found the Marketing Report and Marketing Profile somewhat interesting, but not compelling. Most of them found the Marketing Score confusing and it had low appeal. The features for comparing oneself to others and social networking in the Saver concept had low appeal with customers. However, customers did like money-saving offers (one of our rays of sunshine). This was the most appealing part of the Saver concept. The idea of reducing junk mail had strong appeal in the Shield concept, as did the idea of saving trees as a secondary benefit. I should clarify that “strong appeal” was still far from a slam dunk. Customers had plenty of questions and concerns about what we showed them. However, I was confident we could use what we learned to avoid those concerns and make the next revision even stronger. I could tell there was enough latent interest in those benefits. After the research, I took the map of our problem and solution spaces—originally shown in Figure 11.2—and colored each box

An End-to-End Lean Product Case Study

green, yellow, or red for strong, some, or low appeal. When I looked at the results, I saw two separate islands of green: the Saver target customers liked money-saving offers and the Shield target customers liked blocking junk mail. The two options for moving forward were clear: we just had to pick which direction we wanted to pivot. ITERATE AND PIVOT TO IMPROVE PRODUCT-MARKET FIT

While these two distinct concepts had strong potential appeal, the appeal for Shield was stronger. Because a lot of websites already provided money-saving offers (such as coupons.com) for Savers, it wasn’t clear to customers how our offering was differentiated. Also, customers were less willing to pay for this service and said that they would only be willing to pay a price that was less than the actual savings it achieved for them. In addition, it would take a lot of effort to sign deals with the companies that would make the offers, and this service wasn’t a great fit with the company’s brand. In contrast, we detected a stronger potential product-market fit for Shield. Customers seemed more willing to pay for a service that reduced their junk mail. We had introduced the concept of paying for the service with the “Marketing Shield Premium” upgrade option. Some customers told us that if the service really worked as expected during an initial free trial period, they would be willing to pay afterwards. When asked how much they would be willing to pay for a service like this, some people indicated that it would be a small amount—but we didn’t get a strong response. This service was also a better fit with the company’s brand. The Pivot

Based on what we learned, we decided to abandon our previous core value proposition of “finding out what ‘they’ know about me” and pivot to a service that only dealt with blocking junk mail. We tentatively named it JunkmailFreeze. At this point, we identified three options for how to proceed. The first was to create a new set of mockups, test it with customers, and then build the product. The second option was to code a higher fidelity prototype in HTML and CSS to test with customers and then build the product. The third option



194

The Lean Product Playbook

was to design and build the product without bothering to test with customers again beforehand. We decided go with the first option. JunkmailFreeze was quite a pivot away from our core concept, and we had learned a lot about how to improve the junk mail blocking service. Plus, it wouldn’t take much time to generate a new set of mockups and recruit another batch of customers. We decided to speak with fewer customers this time in the interest of saving some time and money. Iterating Based on What We Learned

We tossed out our old mockups and started fresh to design a new product focused on reducing junk mail, with a secondary benefit of saving trees. We came up with a pretty straightforward user experience for our new MVP prototype. It started with an email from a friend recommending JunkmailFreeze with a link to the home page that explained the benefits and had a big “get started” conversion button. It also had a “learn more” link. Clicking the “get started” button led to a simple sign-up page where the user entered their name, address, email address, and password. After clicking the “register” button on that page, the user was taken to the “my account” page where they could specify which types of junk mail they no longer wished to receive. The other pages up to that point explained the benefits of using JunkmailFreeze; but this was the key page where the user interacted with the product to achieve those benefits. Before we had conducted our first round of user testing, our view of the relevant benefit was simply to reduce the amount of junk mail a customer received. We learned so much more about the problem space after talking with users. We learned that there were certain types of junk mail that almost all customers hated the most: preapproved credit card offers and cash advance checks. Most customers do not have a secure (locked) mailbox. So they were worried that someone could steal a preapproved credit card offer from their mailbox and open a credit card in their name, or steal a cash advance check and use it. We used that knowledge to craft relevant messages in our new designs. In general, finance-related junk mail was the top area of concern— including the types just mentioned, as well as loans and insurance. Customers had concerns about these types of junk mail increasing

An End-to-End Lean Product Case Study

the risk of identity theft. It seemed that every customer we spoke with knew someone who had been a victim of identify theft. So we added the benefit of reducing the risk of identity theft to our messaging. We learned that many privacy-conscious customers spend a lot of time shredding their junk mail. Several told us that when they get home from work, they take their mail out of their mailbox and stand next to their paper shredder as they read through it, shredding as they go. This nightly routine takes five minutes for some people, which adds up over time. We realized that there was also a “save time” benefit associated with reducing junk mail and added that to our messaging. We also learned that customers considered catalogs to be a pain because they are so big and bulky. People discard many unwanted catalogs and consider it a hassle and quite a waste of paper. However, we also learned that people still wanted to receive certain catalogs, and that different people had different preferences for the types of catalogs they wanted to receive. We also learned that many customers consider local advertising a nuisance. One form was the pack of local coupons, which many people tossed out without opening. Another form was circulars and flyers from local business such as supermarkets. People also complained about being sent free local newspapers to which they hadn’t subscribed. This is a great example of how talking to customers helps you gain such a deeper understanding of the problem space. We learned so much that our new “My Account” page let the user block up to 31 different types of junk mail across seven categories. After users selected which types of junk mail they wanted to block, they clicked the “continue” button to complete their registration. The “registration complete” page told users what to expect next: that in the next couple of months, they should see a dramatic reduction in the amount of junk mail they receive in the categories that they selected. For our first round of testing, we learned that customers expected that the service would take a while to “kick in.” We also knew that operationally it would take a while for the service to go into effect for each customer. The page also explained that users could return to JunkmailFreeze at any time to change the types of junk mail they wish to freeze. We had learned from our first round of research that it was



196

The Lean Product Playbook

important to customers that they be able to change their settings. Several expressed concern that by blocking their junk mail, they may inadvertently not receive some type of mailing that they would want to receive. Because the messaging on this page matched customer expectations, most nodded or said, “That sounds good,” when they read it. We provided a “learn more” path for customers who weren’t ready to sign up right away. We explained on the “learn more” page how JunkmailFreeze contacts direct mailing companies on your behalf to get off their mailing lists. We explained that you would still be able to receive direct mail items that were important to you. Because identity theft was such a large concern related to junk mail, we also provided a page that explained how the two were connected. It included a photo of a row of vulnerable mailboxes and explained how you could reduce your risk of identity theft by using our service. Because customers had a lot of questions about who was providing this service during the first round of user testing, we also added an “About us” page. They wanted to know about the company and its background. Wave 2

Now that our JunkmailFreeze mockups were done, we recruited another group of customers using the same Shield screener, since it had worked so well. For this second wave, we scheduled three groups of two customers each. I updated the research script to focus on junk mail. The sessions were 90 minutes long starting at 6 and 8 p.m. (same as last time). Because we had done a fair amount of discovery in the first wave and wanted to focus on getting the product details right in this wave, we spent more time on the mockups (45 minutes instead of 35). I moderated the user testing, and our recruits were again engaged and articulate. Climbing the Product-Market Fit Mountain

The second wave of user testing was one of the coolest things I’ve seen—and the results were very different from the first. None of the customers had any major concerns or questions with the product. Instead, there was a lot of head nodding as they went through the

An End-to-End Lean Product Case Study

mockups, and unsolicited comments like “Oh, this is great.” They did have minor comments, questions, and suggestions. But because we had learned what the major issues were in the first round and had adequately addressed them in our second wave mockups, everyone really liked our product. That being said, the mockups we showed weren’t “done.” We gained an even deeper understanding about what customers wanted in a junk mail blocking service. For example, we learned that category-level controls for blocking weren’t adequate for junk mail related to credit cards and catalogs. For those items, customers wanted to the ability to specify their preferences at the individual company level (e.g., Chase or Wells Fargo for credit cards, Nordstrom or L.L. Bean for catalogs). We also received feedback on our messaging and UX that would further improve the product. This time when we asked customers how much they would be willing to pay for a service like this, we saw a stronger willingness to pay and a willingness to pay a higher amount compared to the first wave. You always have to be a bit skeptical when you discuss pricing with customers. Again, what they say they would do and what they would actually do can be different. You don’t really know what they will be willing to pay until you have a real product and they have to vote with their wallet. But there was clearly much stronger interest in our Wave 2 product. I felt confident that we had achieved an adequate level of product-market fit with our mockups to move forward. There was one more reason I felt confident. After each test was over, I thanked the customers and gave them their compensation checks. After receiving his or her check, every customer asked me if this service was live now and if they could sign up for it. When I explained that it hadn’t been built yet, they all asked if I would please take their email address and notify them when it was available. None of the customers in the first wave had exhibited any behavior like this. Because this was genuine, positive customer interest outside the scope of our user test, I took it as further proof of product-market fit. REFLECTIONS

Before this particular project, I had conducted various types of customer research to solicit user feedback on a product or product concept, and I had been on teams that practiced user-centered design.



198

The Lean Product Playbook

But this project was the first time that I created and tested a product idea in such a Lean way. Focusing on mockups and not coding anything allowed us to iterate rapidly. By being rigorous about our target customer with our screener we were able to recruit customers that gave us great feedback. The whole project took less than two months and used resources very efficiently. I was excited that in so little time—and with just one round of iteration—our small team was able to improve our product idea so much and achieve a high level of product-market fit. I also like to share this example with others because we didn’t really do anything special or unique. We just followed the Lean Product Process. There’s no reason anyone else couldn’t replicate the results we achieved. By following the process I describe in this book, any team should be able to achieve similar results with their product idea. Of course, the details of how it works out in your case will vary. It may take you more waves. You may not have to pivot, or you may have to pivot more than once. And there’s no guarantee you will achieve product-market fit for every product idea you pursue. But you should be able to test your hypotheses and assess your level of product-market fit with confidence. As previously discussed, you can conduct user testing with design artifacts or a live, working product. To minimize risk, make faster progress, and avoid waste, I strongly recommend getting feedback on design artifacts before you start coding. This will allow you to be more confident in your hypotheses before you invest in coding. Once you have validated your mockups or wireframes with customers, it is time to start building your product. All the learning you gain in the Lean Product Process will help you better define the product to build. In the next chapter, I offer advice on how to go about building your product.

Part III

Building and Optimizing Your Product

Chapter 12

Build Your Product Using Agile Development At this point, you have validated your target customer, their underserved needs, your value proposition, your MVP feature set, and your UX. As a result, you should feel confident about the blueprint you’ve developed. Validating product-market fit with prototypes is incredibly valuable, but now it’s time to turn your blueprint into an actual working product that customers can use. Building the product you’ve defined is obviously a critical step, and solid execution really matters here. There are many risks that could impede you while trying to build your blueprint. You may run into issues with technical feasibility, where what you’ve designed is impossible or too challenging to build, either in general, or with the resources you have available. Your product may be feasible but have such a large scope relative to your resources that it will just take too long to build. Good market opportunities only exist for so long before competition moves them to the upper right quadrant of the importance vs. satisfaction framework. An important part of product-market fit is having the right product at the right time (recall the product strategy discussion in Chapter 5). Even if you have an appropriate scope, poor execution can result in your actual product falling quite short of the promise of your prototype. You clearly want to minimize these types of risks—and the product development process you use can have a big impact on that. This chapter shares best practices in product development to help you deliver great products more quickly with less risk. AGILE DEVELOPMENT

Just as you took an iterative approach to arrive at this point, you want to do the same in building your product. “Agile development”


202

The Lean Product Playbook

is the broad term used to describe a variety of iterative and incremental product development methodologies. Before the adoption of Agile development, most software products were built using the “waterfall” approach—one that proceeds sequentially through a series of steps. The team first defines all of the requirements, and then designs the product. They then implement the product, followed by testing to verify it works as intended. The key characteristic of waterfall is that the team does not progress to the next step until the previous step is 100 percent complete. In other words, no design happens until all of the requirements are defined, and no coding happens until the entire product is designed. Waterfall is also referred to as a “big design up front” (BDUF) approach. In contrast, teams using Agile methodologies break the product down into smaller pieces that undergo shorter cycles of requirements definition, design, and coding. There are several benefits of Agile. First, because you are planning in smaller increments, you can react to changes in the market or other new information more quickly. Second, your product reaches customers earlier—which means that you start hearing feedback from customers on your actual product sooner, which helps guide your subsequent product development efforts. Third, teams can reduce their margin of error in estimating scope by working in smaller batch sizes. I discussed the Lean concept of small batch sizes in Chapter 6, but let’s explore why they are so beneficial in software development (or any development under conditions of high uncertainty). When developers estimate the amount of time it will take them to implement new functionality, there is a degree of uncertainty in their estimated values. This uncertainty results in estimation errors where the actual duration differs from the estimated duration. A good way to compare the actual duration and the estimated duration is to take the ratio of the former to the latter. If a project took twice as long as expected, the ratio would be 2×; if it took half as long as expected, the ratio would be 0.5×. Steve McConnell created a diagram called the “cone of uncertainty” that characterizes the range of expected estimation error over the life of a software project. In McConnell’s chart, the upper and lower bounds of the estimation error are symmetric curves, starting at 4× and 0.25×, respectively, at the outset of a project and decreasing throughout the project to converge at zero at

Build Your Product Using Agile Development

the end. It makes intuitive sense and jibes with experience that the estimation error early in a project is larger than the estimation error near the end of a project. However, in practice, I have not experienced estimation errors to be symmetric. In other words, I have not seen that developers are just as likely to finish tasks early as they are to finish them late. Most of the time, software development tasks take longer than estimated. And while it’s true that some tasks do get completed early, the magnitude of positive surprises tends to be much smaller than the magnitude of negative surprises. Why is that? To help explain the asymmetric nature of software estimation errors, I’ll quote epistemologist and former Secretary of Defense Donald Rumsfeld: There are known knowns. There are things we know we know. We also know there are known unknowns. That is to say, we know there are some things we do not know. But there are also unknown unknowns. The ones we don’t know we don’t know. When developers are asked to estimate the effort for a task, they take into account the “known knowns.” Skilled estimators will also account for the known unknowns in their estimates. It’s true that some estimation error can come from an inaccurate understanding of the known knowns or the known unknowns. But I believe the biggest wild cards in estimate after estimate are the unknown unknowns, and that they are what make the distribution of estimation errors asymmetric. Let’s say I estimate that task A will take me five minutes and task B will take me five months. Both tasks could have unknown unknowns. But the uncertainty is nonlinear with increasing scope, as the top curve of the cone of uncertainty suggests. The chances that the five-minute task will spiral out of control are pretty low. The five-month task is over 30,000 times larger in scope, which is a lot more room for unknown unknowns to hide. When developers go through the thinking and investigation required to break a large task into smaller ones, they reduce the unknown unknowns by converting them into known unknowns. You can’t completely escape unknown unknowns, but by using smaller batch sizes, you can rein them in to be more manageable and ship product more predictably. In contrast, waterfall projects, which are



204

The Lean Product Playbook

typically large in scope, are notorious for taking much longer than original estimates. Aside from these delays, some Agile zealots like to bash waterfall because they strongly object to the notion of a process having sequential steps that rely on prior steps. They act like Agile makes it okay to just jump in and start coding things. However, that perspective goes too far. Even in Agile, you should design before you code; you’re just doing so in much smaller increments. It’s worth pointing out that waterfall is a better approach for some projects. For example, we wouldn’t want to send humans into space with a minimally viable spaceship. I began my career designing nuclear-powered submarines. We definitely checked our requirements and reviewed our designs multiple times before starting construction. The risk of failure is just too high in these situations; that is, people would likely die. Also, unlike the code for a website—which can be quickly changed at will—it’s much harder to make changes to a spaceship or submarine after it’s built. When the risk of failure or the cost of making changes is too high, it’s better to spend more time gaining a higher level of confidence before starting implementation. Agile development’s core principles were laid out in the Agile Manifesto, which was written in 2001 (you can view the manifesto and the principles at http://agilemanifesto.org.) Agile encourages early and continuous delivery of working software with a mindset focused on creating value for customers. A key part of Agile is defining your product in a customer-centric way with user stories. As Chapter 6 discusses, a user story is a brief description of the benefit that the particular functionality should provide, including whom the benefit is for, and why the user wants the benefit. Well-written user stories usually follow the template: As a [type of user], I want to [do something], so that I can [desired benefit]. Agile also promotes strong cross-functional communication and collaboration, with business people and developers working together daily, ideally face-to-face. Instead of encouraging adherence to a rigid

Build Your Product Using Agile Development

plan, Agile emphasizes flexibility to quickly respond to change. Teams can accomplish this by completing small batch sizes of work in short iterative cycles with feedback and learning, as opposed to trying to specify the entire set of detailed requirements upfront. Finally, Agile is about continuously improving your product development process via feedback and experimentation. There are several different varieties of Agile development, including Extreme Programming (XP for short) and Lean Software Development. I’ll provide a brief overview of two of the most commonly used Agile methodologies: Scrum and kanban. SCRUM

Scrum is the most popular Agile framework. It’s relatively easy to adopt because there is ample prescriptive guidance available on how to practice Scrum. A key aspect of Scrum is that the team works in time-boxed increments—that is, limited to a specific timeframe. This period of work, called a sprint or iteration, is a fixed length of time. Two-week sprints are very common, but you also see companies using one-week, three-week, and four-week sprints. All work that the team completes comes from the product backlog of user stories. A backlog is a rank-ordered to-do list. User stories are written and placed on the product backlog by the Product Owner, one of the three roles specified in Scrum. The Product Owner, or PO for short, is responsible for using input from customers and stakeholders to create the prioritized backlog of user stories. The product manager on the team usually fills the Product Owner role. Some companies have a dedicated PO in addition to the product manager, and the two people coordinate closely. In smaller startups that don’t have a dedicated product manager, one of the founders usually wears this hat. The second role is “development team member.” The Scrum guidelines say that the team should be multidisciplinary with all the skills required to complete the work. Scrum teams usually include several developers, whose job is to estimate the size of stories and build the product. Three other important team roles are UX designers, visual designers, and quality assurance (QA) testers. The traditional Scrum guidelines don’t differentiate among team members, but it’s fine to



206

The Lean Product Playbook

acknowledge distinct roles within the team. The designers bring the user stories to life by designing the user experience, which they convey through design deliverables. Well-written user stories include acceptance criteria, which are used to confirm when a story is completed and working as intended. QA testers help check to see if acceptance criteria are met and ensure the quality of the product. The ideal size of a Scrum team is five to nine people. You may have heard of “the two pizza rule”: if two pizzas aren’t enough to feed your team, then it’s too big. With this size, you should have enough people to accomplish a meaningful amount of work per sprint. Yet the team is small enough to feel like a cohesive unit and avoid the communication challenges that usually occur with larger groups. The third role is Scrum Master, whose job is to help the team with the Scrum process and improve its productivity over time. Larger companies may have a Scrum Master that works with one or more Scrum teams, but a dev lead or dev manager often fills this role. Although it’s not consistent with the Scrum guidelines, sometimes the role isn’t explicitly filled by a single person—it’s either ignored or the responsibilities of the role are distributed among the team. The team carries out certain activities to prepare for the next sprint before it starts. The Product Owner will groom the backlog to make sure that stories being considered for the next sprint are well written and understood by the team. The PO usually does this with the dev lead or dev manager in a backlog grooming meeting (also called a backlog refinement meeting). See Figure 12.1 for a visual depiction of the flow of work, meetings, and deliverables in Scrum. At the start of each sprint, the team holds a sprint planning meeting where they decide which stories they plan to accomplish in the iteration and move those stories from the product backlog to the sprint backlog. Part of this process requires that the team estimate the scope of each story using story points, which are a relative measure of effort. Estimating points can often be more of an art than a science. You can find a variety of point systems in use. Some systems let the team assign any number of points to a story: 1, 2, 3, 4, 5, 6, and so forth. A common approach is to use the Fibonacci series for points, where the only valid values are 1, 2, 3, 5, 8, 13, and so forth. The benefit of this approach is that it forces distinct differences in estimated values. Another popular point system that forces even larger differences in

Build Your Product Using Agile Development

FIGURE 12.1

Scrum Framework

estimated values is the “powers of two” scale: 1, 2, 4, 8, 16, and so forth. T-shirt sizing, another popular technique, uses sizes such as small, medium, large, and extra large to estimate the scope of stories. Stories with points at the high end of your scoring range have large scope and uncertainty and should be broken down into smaller stories, as discussed in Chapter 6. Stories that are too big to complete in one iteration are called epics, which must be broken down before they can be accepted into a sprint. Many Agile tracking tools enable the use of epics to organize related stories and manage them across multiple iterations. If story points seem a bit abstract to you, it’s because they are—at least at first. The goal is to determine a team’s capacity for work by tracking how many story points they complete each iteration—which is called velocity. Once a team has calculated their average velocity, they can use that number of story points to plan their sprints. While story points start out a bit abstract, they provide a measuring stick for determining empirical values. In order to calculate velocity, story point estimates need to have a numerical value; so in the case of T-shirt sizing, the team would have to map each size to a relative number of points. See Figure 12.2 for an example of how a team tracks their velocity over multiple iterations. The horizontal axis shows the iterations,



208

The Lean Product Playbook

FIGURE 12.2

Team Velocity

numbered sequentially over time. The vertical axis shows the number of story points completed. Over these 12 iterations, the team’s velocity has been variable (between 22 and 40 story points), which is normal. Despite this variability, the trend line shows that the team has been steadily improving their velocity over time. Scrum teams use several techniques to reduce their story estimation error and achieve a more stable velocity. Teams will often discuss and estimate story points together, versus having only one team member size a given story. Some teams develop a reference set of user stories of different known sizes. Comparing stories to the reference stories helps them more accurately estimate scope. Planning Poker is a popular technique for generating quick but reliable estimates as a group. Each team member receives a set of cards, with each card corresponding to one of the possible point values (e.g., 1, 2, 3, 5, and 8). After the team finishes discussing a story, each member privately selects the card with his or her points estimate, and then everyone reveals his or her card simultaneously. If the team has relative consensus, that gives higher confidence that the estimate is accurate. If there are material discrepancies in the estimates, they discuss the story further to try to reach a consensus estimate.

Build Your Product Using Agile Development

Teams will often break each story down into the set of coding tasks required to implement it. This helps ensure that they thoughtfully consider the work required for a story and that they don’t overlook anything. Plus, it’s usually easier to estimate the effort of each of these smaller tasks compared to the whole story. Some teams estimate the size of tasks with points, while others prefer to use hours of effort. Some teams identify tasks but don’t bother estimating them, keeping their estimates at the story level. When sprint planning is complete, the team should be clear on the set of stories they plan to accomplish in the sprint. They should choose the highest priority stories from the product backlog, and the total number of points for those stories should match the team’s expected velocity for the iteration. In teams where the skill sets of developers vary, it’s also a good idea to ensure each story has been assigned to a specific developer to ensure the team is properly load balanced for the sprint. The team holds a daily Scrum meeting during the sprint, which is also called a standup because many teams stand up during the meeting to help keep it short. This meeting is usually held first thing in the morning so the team can discuss their plans for the day, and is generally time-boxed to 15 minutes. Team members each briefly describe what they did the previous day, what they plan to do today, and anything that is impeding their progress. The team implements user stories starting at the top of the sprint backlog, collaborating as necessary. Many Scrum tools are available to help teams manage and track their work—some popular ones include JIRA Agile, Rally, VersionOne, and Pivotal Tracker. These facilitate product and sprint backlog management and sprint planning. Team members use them to track the state of each use story, changing states from “to be worked on,” to “in development,” to “code complete,” to “done,” for example. Teams use a burndown chart—which shows how much work remains to be completed for the iteration—to track progress. The chart can display the remaining work in either points or hours, depending on the units your team uses for tracking. Figure 12.3 shows an example of a daily burndown chart, with the days of the sprint on the horizontal axis and the remaining story points for the sprint on the vertical axis. It starts on “day zero” of the sprint with the number of points to be completed, 45 in this case. This chart



210

The Lean Product Playbook

FIGURE 12.3

Burndown Chart

shows 10 working days, which corresponds to a two-week sprint (only weekdays are shown). Ideally, the team ends up with zero remaining story points at the end of the sprint. QA testing is conducted during the sprint. To achieve a higher velocity, team members should test stories as developers complete them. If the story meets its acceptance criteria, then it is accepted; otherwise, it is rejected and kicked back to development. The team should also reserve some time at the end of the sprint to test the entire product after development is complete and to fix any bugs they find. I discuss testing later in the chapter. The goal for the end of each sprint is to complete an “increment” of work that adds functionality to the product. The Scrum guidelines direct each team to define what “done” means for them. For many teams, “done” means a product that could be shipped, called a “shippable product” or a “potentially releasable product.” Many teams release new product with the same frequency as their iterations, launching the output of their sprint to customers shortly after the sprint ends. Others have a separate release process with a longer cycle where the work from multiple sprints is released together at one time. Regardless of your deployment process, the goal is to ensure

Build Your Product Using Agile Development

the product is in a shippable state at the end of the sprint. At the end of each sprint, the team holds a sprint review meeting (also called a sprint demo meeting) where they show what they have built. This helps ensure the product works as expected and lets everyone see the team’s progress. Ideally, customers or stakeholders attend the demo to provide feedback to be considered for future sprints. As with other Agile methodologies, Scrum also focuses on improving the team’s process over time. To that end, teams hold retrospectives to specifically reflect on how the last sprint went. At these meetings, the team discusses what worked well, what didn’t, and what improvements they want to make for the following sprint. Some teams hold retrospectives after each sprint; others do so after two or three sprints. I’ve described the basics of Scrum here—if you want to learn more, see the latest version of the Scrum guidelines at http://scrumguides .org. KANBAN

Another popular flavor of Agile development is kanban, a process adapted from the system Toyota developed to improve how they build cars. The Toyota Production System focused on just-in-time production and eliminating waste. I studied the original kanban system and Lean manufacturing, which inspired the Lean software development movement, in my graduate program at Virginia Tech. Manufacturing workers use paper kanban cards to physically signal when additional work should be pulled into the system. These cards have been adapted in software development as virtual cards that each represent a work item but don’t actually generate a pull signal. Instead, it’s up to the team members to proactively pull the next work item forward. A core principle of kanban is to visualize work. Each card is a user story or a development task that supports a user story. The cards are arranged on a kanban board, which consists of a set of columns, one for each different state of work. The columns are arranged left-to-right in the order in which work flows. See Figure 12.4 for an example of a kanban board. This kanban board has the following set of columns from left to right: “backlog,” “ready,” “in



212

The Lean Product Playbook

FIGURE 12.4

Kanban Board

development,” “development done,” “in testing,” “testing done,” and “deployed”—defined as follows: Backlog: Items to be potentially worked on, sorted in priority order. Ready: Items that have been selected from the backlog and are ready for development. ● In development: Items that a developer has started working on. ● Development done: Items that the developer has finished working on but which have not been tested yet. ● In testing: Items in the process of being tested. ● Testing done: Items that have successfully passed testing but have not yet been deployed. ● Deployed: Items that have been launched. ● ●

Some columns represent work being done (e.g., in dev, in testing) while others represent items waiting to be worked on (e.g., ready, development done). The latter type of columns are queues of work. When a team member frees up capacity after finishing work on one item, they pull the top item from the appropriate queue and start working on it.

Build Your Product Using Agile Development

As a work item progresses through each stage, its card is moved from one column to the next. It’s easy to visualize the state of what the team is working on at any point in time by just looking at the board. It’s also easy to see where the bottlenecks are by looking at which columns are accumulating the most cards. You may have noticed that instead of having just a single state for “testing,” Figure 12.4 has two: one state for items being tested (“in testing”) and a second state for “testing done” items. “Development” similarly uses two states. This helps create a clearer picture of the status of the team’s work and helps make bottlenecks easier to identify. In kanban, the quantity of active work is managed by constraining the amount of “work in progress” or WIP. The team decides on the maximum number of cards each column can contain, which is called a WIP limit. Team members pull work items forward sequentially through each state of work. However, they can only move a work item to the next column if that column has spare capacity. This rule helps smooth out the work and achieve a steady flow. Teams should fine-tune their WIP limits over time to optimize their workflow. The WIP limit is displayed above each column. As shown in Figure 12.4, teams often use a single WIP limit to constrain the total number of cards across the two related “in progress” and “done” states (versus having separate WIP limits for each of the two columns). For example, the total number of “development” cards cannot exceed 3. This helps encourage the flow of cards to the right out of the completed states. Looking at the work item cards in Figure 12.4, when the developer working on card D finishes, he would move it from “in dev” to “done.” However, he would not be able to pull Card F forward from “ready” because “development” is at its WIP limit of 3. Likewise, when QA finishes testing Card B, they would move it to “testing done” but could not pull Card C forward because “testing” is at its WIP limit of 2. For work to progress, one of the “testing done” cards needs to be deployed. Once it is, Card C can be pulled forward to “in testing,” which frees up “development” so Card F can be pulled from “ready” to “in dev.” You can further organize your kanban board with swimlanes— horizontal lines that separate cards into rows. There are a variety



214

The Lean Product Playbook

of ways to categorize cards with this technique. You can use swimlanes to prioritize cards (the higher the row, the higher the priority). You can give each epic or each user story its own row. Swimlanes can also show each person’s workflow more clearly, by having a row for each team member. You can also track multiple related projects on one board by putting each project in its own row. The focus in kanban is on the flow of work. There is no time-boxed iteration as with Scrum. Work items move continuously from left to right on the kanban board as work progresses. The scope of user stories isn’t necessarily estimated, so the Scrum concept of velocity (story points delivered per iteration) doesn’t really apply. But you can measure the team’s throughput, which is just the number of work items completed in a given timeframe, for example, 10 items per week. If you track your team’s throughput over time, it should go up as they make process improvements and become more proficient. Two commonly used metrics in kanban are cycle time—the amount of time on average from when work starts on an item to when the item is delivered to the customer—and lead time, the amount of time on average from when a work item is created (e.g., requested by a customer) to when it is delivered. It’s important to note that cycle time and lead time aren’t necessarily correlated with effort. A work item could take only an hour to complete but have a much longer lead time if it sat around for a while without anyone working on it. You can visualize the flow of work in a kanban system with a cumulative flow diagram (Figure 12.5), a stacked area chart that shows how many cards were in each work state at the end of each day. For simplicity, Figure 12.5 only uses three work states: “backlog,” “started,” and “done.” You can see the cycle time is the horizontal width of the “started” items, and the lead time is the combined horizontal width of the “backlog” and “started” items. The WIP is the vertical height of the “started” items. The kanban mindset focuses on continuous improvement—so your team should be regularly identifying and discussing ways to work better and faster. The idea is that your lead time and cycle time should go down over time as your team makes process improvements and becomes more proficient. Many teams have a constantly changing backlog; items that were considered important at one point in time become less important as

Build Your Product Using Agile Development

FIGURE 12.5

Cumulative Flow Diagram

they add new items. Unlike Scrum, where the sprint backlog is usually locked down within an iteration, team members can change a kanban backlog at any time. Cycle time may be the better metric on which to focus in such rapidly changing situations. You should still keep an eye on how long it takes to get backlog items ready for development to ensure that isn’t decreasing team throughput. If the scope of work items varies greatly, you can see a wide range in your cycle times, with smaller items having shorter cycle times and larger items having longer cycle times. Some kanban teams use the T-shirt sizing approach mentioned before (small, medium, large, etc.) for work items to enable more precise cycle time values. In that case, you would have a distinct cycle time for each T-shirt size. Kanban does not have the level of process prescription that Scrum does; so no rituals are specified, but many teams practicing kanban hold daily standups and periodic retrospectives.



216

The Lean Product Playbook

Kanban Tools

Many small product teams use a whiteboard for their kanban board, drawing a column for each work state and then using a sticky note for each work item. This makes it easy to move the items around and for anyone in the workspace to look at the board and see the status of the team’s work. There are many digital tools for managing kanban. Trello is a popular visual board application used to manage software development. In fact, many people use Trello to manage work outside of development. It’s particularly popular with product managers and designers, who may maintain their own work boards that feed into the development board. Many teams use JIRA Agile for kanban, and other popular tools include SwiftKanban and LeanKit. Although it’s not a pure kanban tool, another application worth checking out is Pivotal Tracker. I used Tracker when I had the rewarding experience of working with Pivotal Labs to build a new product. Tracker uses a visual board of columns, one for each of the pre-defined work states. The tool supports an interesting blend of kanban and Scrum (there is actually an Agile methodology called “Scrumban” which you should check out if that idea sounds appealing). Pivotal Tracker lets you estimate story points and calculate velocity if you want; if you don’t, it feels more like kanban. If you do, it feels more like Scrum, except that the backlog for the current sprint is not fixed but rather determined dynamically. Stories are listed in priority order and automatically move in and out of the current iteration based on the estimate of story points that will be completed (using calculated velocity and the time remaining in the sprint). If you want Tracker to feel more like Scrum, you can use the “manual planning” mode (also called “commit” mode), which lets you lock down the set of stories in the sprint backlog. PICKING THE RIGHT AGILE METHODOLOGY

You now have an overview of Scrum and kanban—how should you decide which one to use for your team? While devotees of each methodology may view them as vastly different, the two methodologies share many common Agile principles. I’ve found that Agile frameworks are like shoes: You really have to try them on to figure

Build Your Product Using Agile Development

out how well they fit. It’s often wise to just pick the methodology that sounds best to you and try it out for a few months. Many teams start by trying out either Scrum or kanban. If it’s working well, then they stick with it. If not, they switch to the other methodology. After trying on both pairs of shoes, your team should be able to decide which one fits better. That being said, here’s some advice to increase your odds of starting off with the best fitting shoes: kanban tends to work best with smaller development teams. The lower process overhead and the lack of a predetermined iteration length can enable faster delivery of product. But as a development organization grows to multiple teams, kanban can start to become more challenging. The lack of a defined cadence to the work can contribute to this, since there is an increased amount of communication required to keep everyone on the same page. Teams that are strong at collaboration are able to scale kanban to larger sizes. If your organization has multiple development teams across which you need to coordinate work, then the predictable cadence of Scrum can be beneficial. The idea of hard launch dates is tenuous with any Agile methodology. Most waterfall organizations are used to having a top-down roadmap that dictates what functionality they should launch each month or quarter, although those deadlines are often illusory due to delays. When these organizations transition to Agile, many still hold on to a waterfall mindset regarding their product backlog. I like using the term “Agilefall” to describe companies undergoing the awkward transition from waterfall to Agile, with a foot in both camps. If your organization would have a hard time letting go of the security blanket of hard deadlines, then Scrum is probably a better fit than kanban. At least with Scrum, you know you will have work done at the end of each iteration and can make high-level estimates for how many sprints a feature should take. Most kanban teams don’t spend time estimating effort or completion dates. By carefully tracking your cycle time and using simple statistical techniques, it’s possible to create projections with kanban that have relatively high confidence, but many teams don’t achieve that level of tracking and precision. Regardless of which flavor of Agile you choose, I highly recommend using a good tool to manage your work—and there are many available for each Agile methodology. One mistake some teams make is to use a general-purpose tool instead of one that is optimized for your



218

The Lean Product Playbook

development methodology. Again, I recommend you try out the tool you think will work best. If you’re not happy with it after a month or two, then try another one. I’ve encountered many dev teams that dislike a particular methodology or tool, which is to be expected. But I’ve also seen teams that have “the grass is greener” syndrome. They bash their current methodology or tool after trying it out for a short amount of time, switch to another one and use it for a month before complaining about that one and repeating the cycle. If your team goes through several methodologies or tools and doesn’t seem to be able to find one that works well enough, you probably need to take a step back and reflect. It might be a sign that your team lacks the requisite level of commitment, training, or both. Along those lines, having your team attend Agile training together can be very helpful. I’ve seen many a team adopt a new methodology without everyone on the team having an adequate level of understanding. Not surprisingly, many of those teams struggle. Before you adopt a new methodology, it’s a good idea to assess each team member’s level of knowledge with it. Even if several team members have worked with Scrum or kanban at prior companies, chances are that there are meaningful differences with how they practiced it there. If you don’t set new expectations, team members will likely assume you are following the practices with which they are familiar. There is significant value in everyone on the team hearing the same thing at the same time about how the product development process should work. This ensures that everyone has the same expectations, reduces misunderstandings, and should enhance productivity. SUCCEEDING WITH AGILE

Regardless of which Agile methodology you select, the additional advice below should help you succeed in building your product. Cross-Functional Collaboration

Agile depends on strong cross-functional collaboration. There should be free and frequent communication among product managers, designers, developers, QA, and any other team members, who should speak daily. It’s essential to avoid creating silos where each

Build Your Product Using Agile Development

function throws their work product “over the wall” to the next function in the workflow. A certain amount of face-to-face real-time communication is critical to maximize shared understanding and team velocity. High-performing teams also employ communication tools such as chat, a development-tracking tool (e.g., JIRA Agile), and knowledge collaboration tools (e.g., a wiki or Google Docs) to work together effectively. Every function should be involved throughout the process, though it’s natural for a particular function to be more involved than others and take the lead during a certain phase. In a nutshell: product managers write the user stories, then designers create artifacts, then developers code, and then testers test. But product development is a team sport. Developers and testers should have some involvement early in the development process so that they understand the rationale behind product decisions, user stories, and UX designs. The team should encourage them to ask questions and make contributions at all stages. Similarly, product managers and designers should be in the loop during development and testing, especially since unforeseen questions or issues often crop up then. As we used to say at Intuit: good ideas come from everywhere. You can tell the level of collaboration by how often team members refer to one another as “we” instead of “they.” Effective collaboration helps the team achieve shared vision and avoid misunderstandings, and allows the team to move faster. Each team member makes numerous decisions about the product every day. If the team has shared vision and understands the objectives and rationale, members are more likely to independently make decisions that support that vision. Ruthless Prioritization

You should maintain an up-to-date, prioritized backlog. It is important to be clear about the next set of user stories you plan to implement when resources permit. This allows you to act quickly. High-tech product teams usually operate in a dynamic environment where requirements and priorities change quickly. It’s not enough to identify items as high, medium, or low priority. If a backlog has 15 high priority items, it won’t be clear which of those items a developer should start on first when her time frees up. Priority



220

The Lean Product Playbook

levels are useful but not sufficient; you also need to rank order your backlog items within each level. I am a fan of ruthless prioritization (which, for the record, is the opposite of wishy-washy prioritization). Having your backlog rank ordered makes it clear which item should be done next. It also makes it much easier to determine where new requirements belong in the backlog when they come up. The trick is to be both rigid and flexible when it comes to prioritizing your backlog. You must be clear on your rank order priorities at any point in time; but you must also be able to quickly incorporate new or changing requirements. I use the analogy of water and ice. Most of the time, your backlog is like ice; the rank order is frozen and fixed. But when new requirements come in or priorities change, you briefly melt the ice into liquid water so you can rearrange things. Once you’re done reordering your backlog, you freeze it again. Following this approach means that your backlog will be up to date whenever anyone looks at it. A developer can reliably pull the item at the top of the stack and start working on it without having to confer with anyone. Adequately Deﬁne Your Product for Developers

It’s important to provide your developers with the information they need to build the desired product. A set of well-written user stories with accompanying wireframes or mockups usually does a good job of that. If the team already has a style guide in place and isn’t introducing any new major UX components, wireframes are usually adequate. If, however, visual design details need to be conveyed, then mockups should be used. For features that are purely back-end with no UX component, wireframes or mockups aren’t required. The team should ensure that it isn’t just the happy path—that is, the expected path of user behavior—that they’re defining. Rather, they need to think through the different conditions and states that could apply. There is a balancing act here. On one hand, you want to provide enough definition that developers can start building with confidence that you didn’t fail to think through an important aspect. On the other hand, you don’t want to experience analysis paralysis where you spend so much time fretting over every detail that implementation gets significantly delayed.

Build Your Product Using Agile Development

Stay Ahead of Developers

Many teams have struggled with integrating UX design into their Agile development process. The guidelines for Scrum don’t explicitly deal with how best to handle this. It doesn’t work well if the designer is creating wireframes for a user story at the same time that the developer is trying to code it. In order for Agile teams to achieve their highest velocity, developers need to be able to hit the ground running when they start on a new user story—which means that the team must finalize the user stories and design artifacts beforehand. Because you want to achieve a steady flow of work, designers need to be at least one or two sprints ahead of the current sprint. In other words, by the end of sprint N, they should have finalized the design artifacts for sprint N + 1 or N + 2. Of course, the designers need solid user stories on which to base their designs—so product managers need to be working one or two sprints ahead of the designers. The goal is to make sure that you never starve developers for work and always have at least one sprint’s worth of fully groomed backlog ready to go. This requires some balance, because you don’t want to specify too many sprints in advance, as things could change. And while I’ve described the situation in terms of Scrum, it also applies to kanban. Based on the designers’ cycle time, PM should ensure there are enough cards in the “ready for design” queue. Likewise, based on the developer’s cycle time, designers should ensure there are enough cards in the “ready for development” queue. Neither the product managers nor the designers should be doing their work in a vacuum. The team needs to carve out a certain amount of time in the current sprint to review and discuss user stories and designs for future sprints. Break Stories Down

Being Agile requires working in small chunks. I mentioned earlier that user stories should not be allowed to exceed some reasonable maximum size (i.e., number of story points). Beyond that, you should strive to break stories down into the smallest size possible. If you have a five-point story, try to find a way to break it into a three-point



222

The Lean Product Playbook

story and a two-point story. Better yet, try to break it into a couple of two-point stories and a one-point story. This may seem difficult at first, but like most things, you will get better with practice. If you’re unable to break the story down any further, then the developers should try to break down the tasks required to implement the story. If they are having trouble doing that, start by enumerating the steps they plan to take to get the work done. Smaller scope stories and tasks result in smaller estimation errors. Dividing user stories into smaller pieces usually requires that you think about them in more detail, which also reduces uncertainty and risk. You may realize when you break a story down that some elements of it are more important than others, which can help you refine your prioritization. The same advice applies for kanban, even if you’re not using story points. Try to break each larger scope card into several smaller scope cards. This chapter has covered a lot of ground on how to use Agile methods to build your product. Another important part of the product development process is testing, where you check the quality of what you’ve built before you release it to customers. Testing is part of quality assurance, the broader discipline of how companies ensure their customers receive a high quality product. QUALITY ASSURANCE

Software products are inherently complex. They rarely work as expected 100 percent of the time, so you need to have some plan for assuring your product’s quality before you release it to customers. Not having a good handle on your product quality can cause headaches like irate customers, lost revenue, and a disruptive drain on your team’s resources. Finding defects as soon as possible is a Lean principle that helps reduce waste. A major bug that you don’t detect until after you launch your product is much more costly than one found during development. First, it negatively impacts customers. Second, it is usually more time consuming for the team to figure out the root cause of production bugs and fix them because they are no longer actively working on that code. Third, because the defect is live, the customer pain persists until the bug is fixed and you deploy the new code to customers.

Build Your Product Using Agile Development

QA testing should play a significant role, but there are also other ways to increase the software’s quality. Coding standards help different developers on a team avoid arbitrary stylistic differences and achieve consistency in how they code, which helps eliminate inconsistencies that can result in quality issues. Coding standards also make it much easier for one developer to understand and modify another developer’s code, which makes the code easier to debug and maintain and also improves developer productivity. In a code review, one developer examines another’s code—and can catch mistakes that the original developer missed. The reviewer also often has good ideas on how to improve the code. Code reviews allow defects to be found and fixed before testing, and are a great way for developers to learn from one another. Going one step further than code reviews is pair programming—a technique where two developers work on creating the code together at the same time. They sit next to each other in front of a single computer and keyboard looking at the same screen. The developer in the “driver” role controls the keyboard and writes the code. The second developer plays the “observer” role and reviews the code as his or her partner creates it. The two developers switch roles frequently. Working in pairs promotes learning and usually results in better product designs and higher quality. Pair programming is a central tenet of Extreme Programming, another well-known Agile methodology. Getting back to QA testing, there are two main types: manual and automated testing. In manual testing, one or more people interact with the product to verify it works as expected. Manual testing is also called “black box” testing because the tester doesn’t have to have any knowledge of how the product was built or the technology behind it. Many companies have dedicated, full-time QA testers. In companies that haven’t staffed QA, the testing burden falls on the other team members (such as developers and product managers). In those situations, developers are often testing their own code. One benefit of having dedicated QA testers is that they are more likely to find unforeseen problems than a developer checking her own code because they approach testing with a fresh perspective. Additionally, the testing is usually more thorough with dedicated QA resources. First, because it’s QA’s primary job, they have more time to test. Second, good QA people approach the testing systematically, which results in checking



224

The Lean Product Playbook

more conditions. Third, skilled QA people have a knack for being able to find ways to break software and are familiar with common issues that arise. In automated testing, software is used to run tests on the product and compare the actual results with the predicted results. A person (usually the developer or the tester) has to initially define each automated test case, but once specified, they can be run whenever desired. Each time a set of tests is run, a report of which passed and which failed is generated. One benefit of automated testing is that it can save significant manual testing effort, especially for tests that are conducted repeatedly. However, there’s a potential risk in that it is only as good as the set of test cases the team writes. If the team doesn’t write test cases for certain functionality, then it won’t get tested. By applying intelligence and creativity, a human tester hammering on a product will often test many conditions and combinations not explicitly called out in automated test cases. Such discoveries from manual testing should be used to add any missing automated test cases before the product is released. In addition, when the team makes functionality or user interface changes, they must revise the associated test cases accordingly. The team should test two different aspects of the product when they build new functionality or make improvements to existing functionality. The first, called validation testing, checks to see if the new or improved functionality works as expected—that it is consistent with the associated user stories and design artifacts. Sometimes, the product is implemented differently from how it was designed, often due to a mistake or a misunderstanding. The developer might also do this deliberately because it wasn’t feasible to implement the product as specified, or he or she chose a lower-effort solution. Even in cases where the product is implemented exactly as specified, the team might then realize that they missed something or didn’t get something right. Any of those issues should get detected during validation testing. The second aspect of product testing is to ensure that none of the other existing functionality was inadvertently broken during the process of building the new or improved functionality. In other words, you add Feature D to your product and want to make sure that Features A, B, and C still work as they did before you added Feature D. This is called regression testing. In this context,

Build Your Product Using Agile Development

the word “regression” means “going back to a worse state”—that is, introducing a bug in existing functionality that wasn’t present before. Many companies use a combination of manual and automated testing, which can be very powerful. Manual testing is valuable for testing new functionality for the first time (validation testing), because the team probably hasn’t thought of all the relevant test cases. A manual tester can try out different combinations and conditions to help identify corner cases. As you build more functionality and your product grows over time, the burden of regression testing grows with it. While you can conduct manual regression testing when the scope of a product is small, it’s usually not feasible to scale a QA team as your product grows. That’s why automated testing is a great fit for regression testing. As the team adds new functionality, they just need to add new test cases and update previous test cases as necessary. TEST-DRIVEN DEVELOPMENT

Many Agile product teams practice test-driven development, a technique where developers write automated tests before they write code. Before coding a desired new functionality or improvement, the developer thinks about how to test it and writes a new test case. The test case should fail when the developer first runs it—because the code has not been changed yet. If the initial test doesn’t fail, it indicates that the developer did not write the test correctly. The developer writes code until she thinks she is done and then runs the test again. If the test doesn’t pass, the developer keeps working until the test passes. After a successful test, the developer will often refactor the code to improve its structure, readability, and maintainability without altering its behavior (while ensuring it still passes the test). Test-driven development, also called TDD, has several advantages. First, it usually leads to higher test coverage, which is the percentage of your product’s functionality that is covered by automated tests. As a result, you’ll tend to miss fewer regression bugs—and enhance the team’s confidence when they modify existing code (since automated testing lets them easily verify that they didn’t break anything). TDD does require some overhead to maintain tests as the product changes over time. But if a team wants to scale their automated regression testing as the product grows, then they need to write new



226

The Lean Product Playbook

test cases as new functionality is developed—whether they decide to practice TDD or not. CONTINUOUS INTEGRATION

Many product teams use continuous integration to iterate their product development more quickly. In order to explain continuous integration, I need to start with how software developers manage their code. Development teams use a version control system to keep track of every single revision made to the code; this makes it easy to see and manage changes. Version control also simplifies the process of restoring the code base to any prior state, so unwanted changes can be reverted. As of the time of this writing, Git is arguably the most popular version control system for Agile development. When developers make changes or additions, they start with the current, stable version of the code base, called the mainline or trunk. Version control lets developers start with separate copies of the trunk (called branches), that they can modify without affecting the trunk. When developers are done building new functionality, they commit their changes to the version control system. Before doing so, each developer should perform unit testing of his or her code by writing the relevant test cases and ensuring they all pass. A team of developers all work in parallel, each committing their changes. Before merging the new code with the trunk and releasing it, all the changes are combined or “integrated” to build the new version of the whole product. Integration testing is performed at this point to ensure that the new product works as intended. Historically, integration has typically been a manual process. Continuous integration uses an automated build process to create a new version of the product based on the latest code commits. The new build is automatically tested, and the team is notified about which tests passed or failed. They fix any issues and once the new code passes all the tests, clear it for deployment. Different teams conduct continuous integration with different frequencies: some daily, some multiple times a day, and some after each individual code commit. Continuous integration helps teams identify and resolve product development issues sooner than they otherwise would, which improves the speed with which the team can iterate. This is consistent

Build Your Product Using Agile Development

with the Lean principle of detecting defects as early as possible to minimize waste. Instead of letting issues unknowingly pile up into a big mess between less frequent integrations, continuous integration lets the team deal with each issue as it arises. Another benefit is that your code is always in a shippable state, giving you more flexibility to deliver your updated product whenever you choose. Your test coverage impacts how beneficial continuous integration is: the higher, the better. CONTINUOUS DEPLOYMENT

Many teams that practice continuous integration also practice continuous deployment, where code that successfully passes all tests is automatically deployed. Some companies automatically deploy to a staging environment (an internal environment that customers can’t access), while others deploy straight to production. This requires automating your deployment process. Advances in automating operational tasks are being driven by the emerging field of DevOps, which focuses on building and operating rapidly changing, resilient systems at scale. A key part of a successful continuous deployment system is having the ability to quickly revert to the previous version of the code if any problems are detected, which is called automated rollback. Metrics that track the health of the product are used to trigger an automated rollback. Let’s walk through an example. A developer commits new code that implements a new feature on a website. The committed change goes through continuous integration, passes all the tests, and is automatically deployed to production. Right after the new code is deployed, the page load times on the website increase to unacceptably high levels, resulting in very slow performance for customers. The high page load times trigger an automated rollback that reverts the version of the product that is live back to the previous version of the code. In order to work well, continuous deployment requires a robust analytics system. Technical metrics that track server health and performance are required to make sure the system is working properly, as are metrics that track product usage. The system needs to be able to tell if a new deployment prevents users from logging in or using some other key functionality. You also need analytics that track



228

The Lean Product Playbook

the health of the business. For example, if you had an e-commerce site and the number of orders being placed by customers suddenly decreased sharply after deploying some new code, you’d want that to automatically trigger a rollback.

This chapter covers a lot of ground related to product development. I’ve shared advice on and provided an overview of several important concepts. Many of the topics I discuss have entire books dedicated to them. These best practices—in the areas of Agile development, QA, and DevOps—have elevated the state of the art and made product teams much more effective. The common theme across these ideas is that they all help you build a great product more quickly with less risk. Once you’ve launched your product, you can take advantage of the power of analytics. A robust analytics platform helps you understand how your business is doing and how customers are using your product. Analyzing your metrics over time and as you make changes gives you valuable insights that can help you drive improvements. The next two chapters cover how to use analytics to optimize your product and business.

Chapter 13


The customer research techniques available to you when you are building a new, v1 product differ before and after launch. Because you don’t yet have a customer base before launch, you rely heavily on qualitative research with prospective customers for direct feedback on your product. While you can of course still conduct customer interviews to solicit feedback on your product after launch, your learning opportunities grow when you have a live product and a customer base using it. You can now take advantage of additional quantitative learning methods: namely, analytics and A/B testing. This chapter will explain how to use analytics to model and measure your product and your business. The next chapter builds on the lessons in this chapter by providing a structured process for using analytics to make improvements, and also includes a case study. ANALYTICS VERSUS OTHER LEARNING METHODS

Before diving into analytics, I want to share a useful framework created by my colleague Christian Rohrer, a successful UX design and research executive. It categorizes the various ways you can learn from customers. Figure 13.1 shows a simplified version of Rohrer’s framework. The vertical axis depicts the type of information you are collecting: attitudinal or behavioral. Attitudinal information is what customers say about their attitudes and opinions. Let’s say you show a customer a mockup of a landing page. He tells you he likes the green color scheme, and that he would be very likely to click the big “buy” button. Those statements both convey attitudinal information. In contrast, behavioral information has to do with what customers actually do. If you launch that landing page, you can conduct one-on-one user tests and see which customers click on the “buy” button. You can also use analytics to see what percentage of users


230

The Lean Product Playbook

FIGURE 13.1

Research Methods Framework

visiting the landing page click the button. Those both provide behavioral information. On the horizontal access of the Figure 13.1 framework is the approach for collecting the information, which is either qualitative or quantitative. Let’s say you conduct one-on-one interviews with 10 prospective customers in an effort to understand their pain points and preferences. Or, you watch a customer use your website. Both are examples of qualitative tests, the kind of research that relies on direct observation of customers. In contrast, you generate quantitative information by aggregating the results from many customers. You are not observing each individual customer but rather looking at statistical results for a large group. Say you track the conversion rate on your “buy” button to see what percentage of customers have clicked on it, or you email a survey to thousands of users to ask about their attitudes and preferences. In both cases, analyzing the results would yield quantitative learning.

Measure Your Key Metrics

OPRAH VERSUS SPOCK

Both qualitative and quantitative learning are important and actually complement each other. Quantitative research can tell you how many customers are doing (or not doing) something. But it won’t tell you why the customers are doing it (or not doing it). On the flip side, qualitative research will help you get at the underlying reasons for why customers do what they do. But it won’t tell you how many people do what they do for each particular reason. In market research, it’s very common to start with qualitative research to understand the relevant questions to ask and the responses that customers give you (the “why”). Armed with this information, you then proceed to quantitative research to find out how many customers give each answer (the “how many”). I like to refer to the qualitative and quantitative methods as “Oprah versus Spock,” respectively, to highlight the difference between the two. Popular television personality Oprah is purely qualitative; she talks to her guests one-on-one and conducts long, in-depth interviews where she gets to know them and what their opinions are. Spock, the logical character from Star Trek, is purely quantitative; he bases decisions strictly on what the objective data and numbers say. When you are validating product-market fit for a v1 product, the Oprah approach is most important. And while you can still use the Oprah approach after launch, you can then also start using the Spock approach to optimize your product. USER INTERVIEWS

Each of the four quadrants in the Figure 13.1 framework represents a distinct type of learning. User interviews fall in the lower left quadrant of qualitative and attitudinal. In these interviews, you attempt to understand a user’s needs and preferences. You want to determine how they think about the problem and the relevant context. You aren’t trying to observe any behavior. You ask open-ended questions, but you are mainly listening to their thoughts and attitudes. See the advice in Chapter 9 about how to conduct effective customer interviews.



232

The Lean Product Playbook

USABILITY TESTING

Usability testing falls in the upper left quadrant of qualitative and behavioral. Like user interviews, usability testing is also qualitative because you are paying attention to what each user has to say. However, usability testing is more concerned with behavior. Instead of having a user tell you if they would or wouldn’t take a certain action in your product (attitudinal), you want to see if they actually do or don’t (behavioral). The main goal is to gain behavioral learning by observing the customer use your prototype or your product. Conducting usability tests on a competitor’s product can also yield valuable insights. Even when the focus is on usability, most user testing inevitably yields a mix of attitudinal and behavioral information. In many user tests, you will want to explicitly ask customers some discovery questions, as I discuss in Chapter 9. It’s important to keep straight in your head the type of information you’re seeking from the customer and the type of information the customer is giving you. SURVEYS

Surveys fall in the lower right quadrant of quantitative and attitudinal. They are quantitative because your goal is to obtain results from a large number of users to see the overall results, and they are attitudinal because customers are telling you what they think; you are not capturing behavioral data of them using your product. I haven’t spoken as much about surveys as the other types of user research—because I have seen them misused so often. A well-designed survey can generate useful information. But you have to know what you can use them for and not exceed those limits. If you survey 1,000 people and ask them to rate on a scale of 1 to 10 how likely they would be to use a new, easy-to-use photo-sharing app you plan to build, you’re pushing it. Why? First, they know next to nothing about your product. Your product description, “a new, easy-to-use photo sharing app,” conveys just eight words of information. It’s not a live app, it’s not a set of clickable wireframes, and it’s not a mockup—so customers don’t have much information to go on. How could people possibly predict with any accuracy if they would use it? Time and again, I see survey creators asking respondents to answer questions about which they certainly do not have enough information.

Measure Your Key Metrics

Second, because surveys provide attitudinal data, you have to take the results with a grain of salt. People can give optimistic or pessimistic answers to how likely they would be to use a new product; but their opinions often don’t end up matching behavior. I’d rather do a smoke test with a landing page for the new product. I’d include a “buy” button or ask people to provide their email address to be placed on the beta wait list and observe the conversion rate. This behavioral data would answer the same question in a more reliable manner than a survey. Third, survey question results can be highly sensitive to the specific wording of the question and the answers you allow the respondent to select. If you’re relying on survey data to make some important decisions, it’s a little scary to think that you could get quite different results depending on how you ask things. If you can’t tell the difference between high-quality and low-quality question design, you could be setting yourself up for that. There are people who earn PhDs in market research for a reason. Don’t get me wrong; you can and should use surveys. Just make sure a survey is the right tool for the learning you want and apply good survey design skills (or find someone who can). So if surveys are bad for certain kinds of questions, what are they good for? Well, they are good for simple questions about the respondent’s attitudes where they have the information required to answer. Chapter 4 discussed the use of surveys to measure importance and satisfaction, for example. Surveys can help you see how people feel about your product and brand. You can also use them to see how customers perceive your product relative to competitors. Tracking surveys, where customers are asked the same questions at periodic intervals, can be useful to identify trends over time. Net Promoter Score

One of the mostly widely used survey-based metrics is the Net Promoter Score, or NPS for short. This metric is based on the results of a single question, “How likely are you to recommend [product X] to a friend or colleague?” A “likelihood to recommend” scale from 0 to 10 is provided, with 10 being “extremely likely” and 0 being “not at all likely.” Customers that give you a 9 or 10 are called promoters; those that give you a 7 or 8 are called passives; and those who answer 0 through 6 are called detractors. To compute your NPS, you take the



234

The Lean Product Playbook

percentage of promoters and subtract the percentage of detractors. NPS can range from –100 to 100. NPS is an attitudinal measure of customer satisfaction with your product, and is a proxy indicator of product-market fit. Customers are only going to recommend a product with which they are very satisfied. The average score from a single wave of NPS surveys is somewhat useful. But the main value comes from tracking your NPS over time with periodic surveys—since it should increase as you improve product-market fit. Of course, your NPS can also decrease as issues arise. Because it measures overall customer sentiment, it can alert you to issues in a wide range of areas beyond just your product, including customer service or support. This is why it’s important to include in your survey an open-ended question asking customers why they gave the score they did. You can also compare your NPS to your competitors’ scores and to benchmarks for your product category. Sean Ellis’ Product-Market Fit Question

Sean Ellis is a talented marketer and Lean Startup practitioner—he coined the term “growth hacker” and runs the community site http://growthhackers.com. Ellis is also CEO of customer insights company Qualaroo http://qualaroo.com. He has helped many companies achieve high customer growth. Ellis advocates, as I do, that you should not invest in trying to grow your business until after you have achieved product-market fit. So he developed a survey question to assess your level of product-market fit. In the survey, you ask the users of your product the question, “How would you feel if you could no longer use [product X]?” The four possible responses are: Very disappointed Somewhat disappointed ● Not disappointed (it isn’t really that useful) ● N/A—I no longer use [product X] ● ●

After conducting this survey with many products, Ellis found empirically that products for which 40 percent or more of users reply “very disappointed” tend to have product-market fit. There can be some variability in that threshold based on the product category, but it is a good general rule of thumb. For an accurate

Measure Your Key Metrics

read, Ellis recommends sending the survey to a random sample of customers who have used your product at least twice and have used it recently. When this survey question is asked, it should also be followed by the open-ended prompt, “Please help us understand why you selected this answer,” as I recommended for NPS. ANALYTICS AND A/B TESTING

The upper right quadrant in Figure 13.1 is quantitative and behavioral. This is where analytics and A/B testing live. Analytics allow you to measure real customer behavior—so you don’t have to worry about any disconnect between what customers say they will do and what they actually do. And unlike qualitative research on user behavior, analytics aggregate many customers’ behavior—thereby enabling you to reach statistically significant conclusions. For example—let’s say you have a landing page. You see from your analytics that your conversion rate is only 5 percent, much lower than you think it should or could be; so you design a new, improved version of the landing page. You conduct usability tests and ask customers for their feedback on the new page. The feedback is generally positive; 9 out of 10 users indicate that they would click the “sign up” button. So you decide to launch the page. Before launching, you aren’t really in a position to estimate the impact of the new design. The real conversion rate is not likely to be 90 percent. That value is artificially high because of the nature of moderated usability testing. You may expect the conversion rate to go up, but it would be hard to quantify by how much from just the usability test results. A/B testing allows you to send a portion of your customer traffic to the new version and the rest to the old version, while tracking the results for each. This way you can know the difference in conversion rate. If you have a high volume of traffic, you will be able to quantify the difference with a high degree of confidence. Analytics are critical for any product team to fully understand how their customers are using the product. Analytics can’t give you the entire picture; you also need qualitative research to know your customers. But you are flying blind without analytics. To paraphrase Peter Drucker, you can’t manage what you don’t measure. A/B testing builds on analytics to give you a way to confidently know the impact of changes you make. It provides a platform for



236

The Lean Product Playbook

experimentation and is a powerful tool that enables Lean teams to innovate rapidly. It’s worth mentioning that the full version of Rohrer’s framework also includes a third dimension for “context of use.” He distinguishes between the different contexts of product use for each research method: “natural use” (e.g., analytics), “scripted use” (e.g., usability tests), and “not using the product” (e.g., discovery interviews). I encourage you to explore his full framework, which categorizes 20 different UX research methods. You can find it on the Nielsen Norman Group website at http://nngroup.com/articles/which-uxresearch-methods. You can see Rohrer’s other publications and blog posts at http://xdstrategy.com. Now that it’s clear where analytics and A/B testing fit in, let’s discuss some frameworks for using these powerful tools. ANALYTICS FRAMEWORKS

For any business, there are a multitude of metrics that you could track to describe how it’s performing. Because there are so many different metrics that you could try to improve, it’s very helpful to have a holistic analytics framework that encompasses your entire business. This allows you to be clear about how the various metrics fit together and can help you identify where you should focus. Analytics at Intuit

After launching a new web product at Intuit, I wanted to track and improve our product and business. I created an analytics framework that covered the four main elements of our business: 1. Acquisition: How many prospects (new visitors) are our marketing programs driving to our website? 2. Conversion: What percentage of prospects that come to our website sign up as customers? 3. Retention: What percentage of our customers remain active over time? 4. Revenue: How much money do our customers generate? After we launched, customers were signing up for our product and we were generating revenue. So we were feeling pretty good

Measure Your Key Metrics

about product-market fit. But we realized that we had a conversion problem: The percentage of prospects signing up was lower than we had expected it to be. The nature of our product required a sign-up process that was several pages long. Using analytics, we measured how many prospects were dropping off at each point in the sign-up process. We then conducted usability testing with users focused on the biggest problem areas we found. We discovered several UX design issues. We then used these insights to quickly make targeted UX design improvements. When we rolled out the improvements, we saw a 40 percent improvement in our conversion rate. The funny thing is, because we had built such a detailed model of the different use cases and usability issues and had such accurate metrics data, we were able to predict the improvement within a couple percentage points. This example is a great illustration of how quant and qual can work together. Quant was the smoking gun that told us we had a conversion problem and identified where people were dropping off the most, but it couldn’t tell us why. Qual gave us the insights we needed to understand and address the issues. After we rolled out the improvements, quant showed us the impact of the changes we made. Analytics at Friendster

Two years later, I joined Friendster, the pioneering social network. I again wanted to use analytics to track and improve the product and business, so I developed a framework to do that. The company agreed that the main metric that mattered was our number of users. Social products benefit from network effects, where their value grows exponentially with the number of users. Plus, we were in the early days of social networking with market leadership still up for grabs. The best way to grow our user base was to have our existing customers invite as many noncustomers to join Friendster as possible. If, on average, each customer generates enough prospects that subsequently convert into more than one new active customer, then you have viral growth. Social products have high potential for viral growth (as well as high, nonviral growth rates). The detailed steps by which an existing customer generates a new customer are called your viral loop. At Friendster, I built an analytics framework for our viral loop so I could optimize our virality. My framework included viral acquisition but not nonviral acquisition. It also included conversion,



238

The Lean Product Playbook

since prospects had to go through our registration flow to become customers. And since only active customers invited their friends to join, my framework also included retention. It did not include revenue (which we tracked separately). I share a detailed case study about my Friendster analytics framework in the next chapter. The business goals for the two analytics frameworks I just discussed are certainly not unique to those two businesses. In fact, they are widely applicable to all businesses. At a high level, almost every company has these five common goals: 1. It wants to make prospective customers aware of its product. 2. It wants to convert those prospects into customers. 3. It wants to retain as many of its customers as it can over time. 4. It wants to generate revenue from its customers. 5. It wants its customers to spread the word about the product to generate prospects. Startup Metrics for Pirates

In 2007, I had the good fortune of meeting Dave McClure. Chances are you’ve heard of Dave—but for those of you who haven’t, he describes himself as a “geek, marketer, investor, blogger, and troublemaker.” He is the founding partner of 500 Startups (http://500.co), a startup seed fund and accelerator. That year, Dave gave a talk where he shared his “Startup Metrics for Pirates” framework. I was excited to see that his framework was very similar to the ones I had developed working at Intuit and Friendster. Dave presented his ideas in such a simple, effective way that the value and wide applicability of his framework was readily apparent. Dave and I just had two minor differences in terminology. First, Dave used the term activation instead of conversion. For Dave, the term activation is a slightly broader term that includes conversion as I’ve defined it; however, it also includes other ways in which a prospect can engage with your product short of becoming a customer. For example, a prospect may not sign up for your service, but may give you his or her email address to be notified about product news. That action wouldn’t qualify as conversion to a full customer but could be measured as an activation metric. Second, Dave used the word referral—an excellent, catch-all term—to describe the concept of your existing customers taking actions that lead to new prospective

Measure Your Key Metrics

customers learning about your product. Dave called his framework “Startup Metrics for Pirates” because if you make an acronym for his five metrics—acquisition, activation, retention, revenue, and referral—it spells “AARRR!” (with an exclamation point added for good measure). In his talk, Dave recommended tracking two or three key metrics for each of the five elements of his framework. That is a good idea because your conversion funnel, for example, isn’t really just one overall metric; you can (and should) track the more detailed metrics. So we can make a distinction between the macro-metrics and the micro-metrics that relate to them. Identifying the best micro-metrics to track for a given macro-metric is part of what I call “peeling the analytics onion,” which I will discuss later. KISSmetrics created an excellent diagram to depict the AARRR framework, which I modified slightly (see Figure 13.2). This isn’t too surprising—since KISSmetrics CEO and founder Hiten Shah is one of the top Lean Startup and analytics thought leaders.

FIGURE 13.2

AARRR Metrics Framework



240

The Lean Product Playbook

IDENTIFY THE METRIC THAT MATTERS MOST

At any point in the life of your business, one of the five macro-metrics in the AARRR model will be more important than the others. I call this the “metric that matters most”—or the MTMM for short. You could improve your business by improving other metrics. But your MTMM is the metric that offers the highest ROI opportunity for improving your business right now—and the “right now” is an important aspect. At some point, after you make significant progress on your MTMM, it will no longer be the MTMM—since a different metric will now offer higher ROI opportunities. For example, let’s say after launching your product you realize that only 10 percent of customers who start your sign-up process actually complete it. You decide your sign-up conversion rate is the MTMM for your business right now, so you conduct user testing of your sign-up process and discover several usability issues. You also discover the form doesn’t work with one particular browser. You check your server logs and realize that sometimes an error occurs, causing the form not to work as expected. You work hard with your team to fix all these issues and see your sign-up conversion rate improve to 90 percent. At this point, the sign-up conversion rate is no longer your MTMM. Some other metric now offers higher ROI opportunities to improve your business. The MTMM changes due to the phenomenon of diminishing returns. When you first focus on optimizing a particular metric for your business, you will quickly find the low-hanging fruit: the ideas that can lead to a large improvement without much effort. After you make these improvements, the ROI on the next set of opportunities is lower, and continues to decrease as you make more progress. With a new product, there is often a natural order in which it makes sense to optimize your macro-metrics. A common scenario is for the MTMM to start out as retention and then change to conversion, followed by acquisition. Let’s explore why. Optimize Retention First

When you are working on a new product, you need to first achieve product-market fit. Until you know that customers find your product

Measure Your Key Metrics

valuable, it doesn’t make sense to spend lots of resources trying to acquire customers. Nor does it make sense to optimize conversion. Not only will spending time on those areas have less of an impact on your business, but doing so would take valuable time away from what is most important right now. If customers find value in your product, they will continue using it; otherwise, they won’t. Retention is the macro-metric most closely related to product-market fit. For this reason, it is typically the first MTMM for a new product. Optimize Conversion before Acquisition

Once you confirm strong product-market fit with a healthy retention rate, you know that a high enough percentage of customers that get through your front door and use your product will stick around. It usually makes the most sense to focus next on making sure the highest percentage of prospects who show up at your front door make it inside. Conversion, the macro-metric that tracks this, has now become the MTMM. Why not focus on acquisition instead? That would mean sending a lot more prospects to your front door. However, many of those prospects aren’t going to become customers if your conversion rate is lower than it should be. By optimizing conversion first you will see a much higher return on your investment when you do focus on acquisition, because a higher percentage of prospects will turn into customers. Optimizing Acquisition

Once you have optimized retention and conversion, it often makes sense to focus on acquisition—that is, identifying new and better ways of attracting prospects. You can explore new and different acquisition channels, segments within your target market, messaging, pricing, promotions, and so forth. You usually undergo this kind of exploration with a series of experiments to test out each new idea with a small sample size. Once an experiment shows that a particular new idea works well, you then roll it out at a larger scale. At a high level, you can divide acquisition into “paid acquisition” and “free acquisition.” Paid acquisition requires you to pay money to attract prospects—for example, advertising your product on Google



242

The Lean Product Playbook

or Facebook. Viral marketing is free. Your users’ actions drive other people to try your product, but you’re not paying them anything. Organic search is another free acquisition channel. The distinction between paid and free acquisition is important because it impacts whether it makes sense to focus on acquisition or revenue first. If your acquisition is largely free or inexpensive, then you can optimize acquisition and worry about revenue separately because you’re not relying on the revenue to fund your acquisition efforts. If, on the other hand, your business relies on expensive paid acquisition, you may decide that it’s important to focus on optimizing revenue before acquisition to reduce your risk. Once you know that each customer is going to generate a certain amount of revenue, you can more confidently spend money on acquiring more. There are countless metrics that you can track and optimize. Since building a successful new product starts with achieving productmarket fit, it would be valuable to identify the best way of measuring it. In my talks and workshops, I often ask my audience, “If you could only track one metric to measure your product-market fit, which would it be?” I usually get a variety of answers. Some people argue that revenue is the ultimate measure. Others think that the growth rate of your customer base is most important. Those two metrics could be the MTMM for a business, depending on its situation. However, I deliberately word my question very carefully by including “to measure your product-market fit.” Retention rate is the single best metric to measure your product-market fit. Let’s dig deeper into retention and how to measure it. RETENTION RATE

Retention rate measures what percentage of your customers are actively using your product. To calculate it, divide the number of active customers by the total number of customers. You want to track retention rate over time to see what percentage of customers keep using your product—and do this in an aggregate way to understand what’s going on across all users. One complication that doesn’t exist with other metrics is that different customers start using your product on different dates, so you can’t just think of retention in terms of calendar dates (as you can with most other

Measure Your Key Metrics

metrics). For retention, it’s most intuitive to aggregate the data using “relative days,” where you count the number of days since each user signed up. Retention Curves

Retention curves are an intuitive way to visualize your customer retention. See Figure 13.3 for a sample retention curve. The vertical axis is the percentage of users returning. The horizontal axis is the number of days (or weeks or months) since first use. The value for each point on the curve has been calculated based on sign-up and usage data for a population of users. Retention curves always start at 100 percent on day zero (the day each user signed up) and then tend to decrease over time as more and more customers fail to return to use your product. There can be quite a drop-off in retention on day 1 (the day after sign up). As a result, day zero is usually not

FIGURE 13.3

Retention Curve



244

The Lean Product Playbook

shown on the graph, and day 1 is the first day displayed; this makes the graph more readable. In this particular retention curve, notice how the value at day 1 is around 20 percent. That means around 80 percent of customers that used this product never came back. This “initial drop-off rate” is one of the key distinguishing parameters of a retention curve. Different product categories have different initial drop-off rates. This retention curve is for a mobile application, a product category that has very high initial drop-off rates. Think about it: people install and use new mobile applications all the time. But after they finish using an app for the first time, they often don’t go back and use it because it’s not front-and-center in their mind. The application icon is usually buried in a sea of other icons on their phone. Unless there is some trigger to remind users about the existence of that app they used, they are likely to forget about it. That’s why notifications are so important for mobile apps in order to combat this “out of sight, out of mind” problem. The second distinct parameter of a retention curve is the rate at which it decreases from that initial value. Some retention curves drop very quickly while others descend more slowly over time. The curve can either keep descending towards zero or eventually flatten into a horizontal line (an asymptote). If the curve goes to zero, then that means you eventually lose all of the customers in that group. If the curve becomes flat at a certain value, then that is the percentage of customers you eventually retain. The terminal value for retention curves that flatten out is the third distinct parameter. One product may flatten out at 5 percent while another flattens out at 20 percent. Those three distinct retention curve parameters I mentioned— initial drop-off rate, rate of descent, and terminal value—are direct measures of product-market fit. The stronger your product-market fit, the lower your initial drop-off rate, the lower your rate of descent, and the higher your terminal value. The weaker your product-market fit, the higher your initial drop-off rate, the higher your rate of descent, and the lower your terminal value. Terminal value is the most important of these three parameters, since it answers the question, “What percentage of customers who tried your product continue to use it in the long run?” If you told me product A had a terminal value of 1 percent and product B had a terminal value of

Measure Your Key Metrics

50 percent, I could tell you which one had better product-market fit (product B) without knowing anything else about the two products. Product-market fit seems like a somewhat fuzzy and difficult-tomeasure concept. So it’s great that retention curves give you hard numbers you can use to measure product-market fit. And while that’s the main reason retention rate is the ultimate metric of product-market fit, there are several other reasons. Another benefit of retention rate is that it is a pure measure of product-market fit that is not conflated with any other components of the macro-metrics framework (e.g., acquisition). What do I mean by this? Well, let’s say you were using the number of active users as your measure of product-market fit, and let’s say your number of active users is trending up and to the right, as we all hope. That can only happen if you are adding new users (via acquisition and conversion). The trend you’re seeing in active user growth could be due to modest new user growth with decent retention. However, the same trend could also result from very high growth and very poor retention. By only tracking active users, you wouldn’t be able to tell the difference between these two scenarios. This is why it is critical when tracking your user counts over time to distinguish between new users and returning users, the latter being the metric used in the numerator when calculating the retention rate. New users are customers who use your product for the first time (during a certain time period). Returning users are customers who use your product during a certain time period who first became users before that time period. Tracking returning users over time is valuable. You’d obviously like the graph of returning users to be trending up and to the right with the highest slope possible. Be mindful, however, that unlike retention rate, returning users isn’t a pure measure of retention. It is conflated with acquisition and conversion. The number of returning users represents the total number of customers you have captured and managed to keep at a given point in time. In contrast, because of the way it’s calculated, retention rate answers the question “of the customers that I captured, what percentage are still active?” at a given point in time. Since retention curves measure product-market fit, they give you a way to measure how much you’re improving your product-market fit over time. You can see how your retention changes over time



246

The Lean Product Playbook

by generating multiple retention curves: one for each slice of time. For example, you might generate a new retention curve monthly. This would give you a set of retention curves, with a curve based on the data for the customers that signed up each given month. Cohort Analysis

A group of users that share a common characteristic—such as the month that they signed up—is called a cohort. Cohort analysis—the analysis of metrics for different cohorts over time—is a powerful tool. Figure 13.4 depicts a graph with three cohort retention curves. The horizontal axis is the number of weeks since sign up (instead of days, as in Figure 13.3). As you can see, Cohort A has the lowest initial drop-off, highest rate of decay, and lowest terminal value. Cohort C has the highest initial drop-off, lowest rate of decay, and highest terminal value. The parameters of Cohort B’s curve are in between those of Cohorts A and B. So which of the three cohort retention curves would you prefer to have? I’d choose the Cohort C curve

FIGURE 13.4

Cohort Retention Curves

Measure Your Key Metrics

because it has the highest terminal value. From week 3 on, Cohort C has a higher percentage of active users than the other two curves. That translates into more revenue. As an aside, once you have more than five cohort curves on the same graph, it becomes difficult to read—especially since cohort data can be noisy, with curves crossing one another. Table 13.1 shows the standard format for storing the data used to generate retention curves for multiple cohorts. From the first column, you can see that each row is a cohort. This example shows monthly cohorts for January through May—this would be the snapshot as of June. For each cohort, you capture the initial number of users in the second column. In each subsequent column, you capture the number of active users for the cohort as a function of the number of months since they signed up. The older the cohort, the more data points you will have for that curve. The data in Table 13.1 is used to calculate the values in Table 13.2. The percentage in each cell of Table 13.2 is the retention rate for the combination of that row’s cohort and that column’s timeframe. Each TABLE 13.1

Raw Data for Cohorts

Cohort

New Users

Active Users Month 1 Month 2 Month 3 Month 4 Month 5

Jan Feb Mar Apr May

10,000 8,000 9,000 11,000 13,000

3,000 2,700 3,200 4,200 5,200

TABLE 13.2

2,000 2,000 2,500 2,500

1,000 1,000 1,500






Cohort Retention Rates Retention Rate

Cohort Month 0 Month 1 Month 2 Month 3 Month 4 Month 5 Jan Feb Mar Apr May

100% 100% 100% 100% 100%

30% 34% 36% 38% 40%

20% 25% 28% 23%

10% 13% 17%

5% 9%

3%



248

The Lean Product Playbook

retention rate is calculated by dividing the number of active users (for that cohort and timeframe) by the cohort’s initial number of users. Each row of Table 13.2 is plotted as a separate cohort curve on the retention graph. Watching Your Product-Market Fit Improve

If you are improving your product-market fit over time, your cohort retention curves will be moving up, reaching higher terminal values for newer cohorts. Figure 13.5, which shows the retention curves for three cohorts using our product, shows an example of how this would ideally look. Cohort A users signed up when we launched our MVP 24 months ago. Cohort B users signed up 18 months ago, and Cohort C users signed up 12 months ago. As you can see, we’ve improved our product-market fit over time, with our retention curve moving up. Each subsequent cohort has a lower initial drop-off, lower decay rate, and higher terminal value than the previous one.

FIGURE 13.5

Improving Retention Rate over Time

Measure Your Key Metrics

THE EQUATION OF YOUR BUSINESS

It’s great that the AARRR framework applies to all businesses at a high level and helps you focus on the right metric at the right time. But at some point, you need to take into account your specific business model to further optimize your business. There are several common business models, including e-commerce, subscription, and advertising. I’ve helped my consulting clients use analytics to optimize their results across all of those business models. I’ve used the same powerful tool to do so in each case: the equation of your business. When there’s something I want to optimize, my engineering and math training make my first instinct to express it as an equation. Countless times in school, one variable Y would be expressed as a function of another variable X and the goal was to find the value of X that resulted in the maximum possible value of Y. More advanced versions of this exercise involved multiple variables. The starting point was always an equation that told you how Y was calculated from X (or the multiple variables). That’s the theoretical world of mathematics—and you can apply a similar technique in the real world of business. Every business can be expressed as an equation. The goal is to come up with a quantitative representation of your business constructed from a set of metrics that you can use to optimize your business results. If it isn’t entirely clear how to do that just yet, let me walk through an example. There’s one equation you can start with that applies to every business: Profit = Revenue − Cost This equation tells you that you can increase profit by increasing revenue or decreasing cost. You can apply it to any given period of time (e.g., day, week, or month). The metrics of revenue and cost are too high level to be actionable, but this is a good starting point. You are going to break down these higher-level metrics into formulas of more detailed metrics to go several levels deeper. This is what I call “peeling the analytics onion.” Most high-tech companies, especially those trying to achieve product-market fit, are much more focused on increasing revenue



250

The Lean Product Playbook

than reducing cost. That is because the economics of most high-tech products are such that as you achieve a higher volume of sales, the incremental revenue from each additional unit (called marginal revenue) exceeds the incremental cost to produce that additional unit (called the marginal cost). And the gap between marginal revenue and marginal cost grows larger as the volume grows larger. Facebook is a good example to illustrate this. They have over 1 billion users. Serving up the Facebook website and mobile app in a timely manner to so many users requires a lot of servers, storage, networking hardware, and bandwidth to run Facebook’s software. Does Facebook need to develop any additional software for each new user? Do they need to add an additional server when a new user joins? No. The only real incremental resources required would be a tiny amount of storage to save the user’s data and a tiny amount of additional bandwidth. For all intents and purposes, the marginal cost of a new Facebook user is zero. Facebook mainly makes money from advertisements it displays in its products. That new user will generate some small amount of incremental ad revenue. So a marginal cost of almost zero and a small marginal revenue result in a small marginal profit. Let’s return to the equation to break revenue down into actionable metrics. There are different ways to do this, but doing so on a “per user” basis usually works best: Revenue = Users × Average Revenue per User This equation tells you that there are basically two ways to increase revenue: increase the number of users or increase the average revenue per user (ARPU). Perhaps you’ve heard the term ARPU before; it’s a key metric tracked by many businesses. The Equation of Your Business for an Advertising Revenue Model

The best way to break users and ARPU down further into more detailed metrics depends on the revenue model. For this example, let’s assume we have a business that generates revenue from display advertising. With many ad-based products, the people who see the

Measure Your Key Metrics

ads don’t have to be registered users. Think of most popular content sites, such as YouTube or the New York Times website: We use the term visitors in these cases. Given the nuances of how web analytics tracking works, the term unique visitors makes it clear we are only counting each visitor once during the particular time period. So, for an advertising business: Revenue = Visitors × Average Revenue per Visitor Display advertising is sold to advertisers on the basis of ad impressions, a term that just means that an ad was served on a page that a person visited. It doesn’t necessarily mean that the visitor actually saw the ad. Let’s say an advertiser buys a campaign of 100,000 impressions. The media site that sold the impressions would serve the ads, keeping track of how many they’ve served, and end that ad campaign once 100,000 have been served. The cost of the ads is specified in units of “CPM,” or cost per thousand impressions (here “M” is the Roman numeral for 1,000). The CPM for this campaign was $10, making the total cost of the campaign $1,000 for 100,000 impressions. CPM can be a good way to compare different types of advertising on an apples-to-apples basis. As a result, you will often hear “effective CPM” used as a broader, catchall term. We can now expand average revenue per visitor: Average Revenue per Visitor = Impressions per Visitor × Effective CPM ÷ 1,000 We can’t really break down effective CPM any further. As we can see from our equations, it’s a detailed metric that has a proportionate impact on revenue. If you double effective CPM, you will double revenue. How can we break down impressions per visitor further? Remember, each of these equations applies for a particular time period. So, what factors determine the number of ad impressions served to a visitor in a given time period? Since visitors may visit our site multiple times in the same time period, we can model that. Ads are displayed on web pages, so the more web pages the average visitor visits (called pageviews), the more impressions. Finally, we



252

The Lean Product Playbook

control how many ad impressions are served on each page, so we can account for that. So we can expand impressions per visitor as follows: Impressions per Visitor =

Visits Pageviews Impressions × × Visitor Visit Pageview

Each of the three metrics in this equation is a variable that we control or can try to influence. A change in the value of any of these metrics will result in a proportional change in revenue. We can drive more frequent visits to our site, for example, by updating our content often or sending emails enticing visitors back to our site. And we can try to get visitors to view more pages each time they visit by spreading articles across more than one page or including links to related articles. We can try to cram more ads per page, although at some point that will likely negatively impact the user experience enough that it will affect our retention metrics. Now that we’ve expanded average revenue per visitor as much as we can, we can go back and expand visitors. As I mentioned earlier, it is beneficial to distinguish between new and returning users (or visitors). Visitors = New Visitors + Returning Visitors New visitors are people who visit your product for the first time (during a certain time period). The total number of new visitors can be broken down in a variety of ways—one of which is via the channel or source from which they came. Many businesses categorize new users by those who come via free versus paid channels, for example, organic search versus pay-per-click advertising. If your product has a viral loop, you can split new users into those who were acquired virally versus those who weren’t. You could further break down the number of viral new users into a formula of viral loop metrics. Returning visitors are people who visit your product during a certain time period who had already visited your product before the current time period. We can express returning visitors in terms of the total number of visitors we had in the prior time period multiplied by a retention rate. This retention rate is a little different from the one we discussed earlier because its context would only be from

Measure Your Key Metrics

period-to-period (e.g., one month to the next) and not the entire life of the visitor. So let’s call it “return rate” to avoid confusion. We express returning visitors as follows: Returning VisitorsT = VisitorsT−1 × Return Rate I’ve added the subscript T to denote the current time period and T – 1 to denote the previous time period. Returning visitors and visitors are just values that you measure; you’re not trying to influence them directly. Return rate would be the variable we would try to improve. For example, we could add a weekly or monthly email with links to popular or recommended stories to try to lure visitors back. We would calculate return rate from the other two numbers. Return Rate =

Returning VisitorsT VisitorsT−1

If we take a step back, we can see that we started out with a very high level equation but managed to keep breaking the equation terms down until we had actionable metrics. That is what I mean by “peeling the onion.” We happened to do it for an advertising-based business, but you can do it for any business. The Equation of Your Business for a Subscription Revenue Model

Without going through the same level of detailed explanation, here is how I would peel the onion for a subscription-based business. Profit = Revenue − Cost We will again focus on increasing revenue and not break down cost. Revenue = Paying Users × Average Revenue per Paying User I’ve used the term paying users instead of users to allow for the fact that not all of our users are paid subscribers. This would be the case if we offered a 30-day free trial, for example. It could also be the case if we had a freemium business model, where we offered both free and paid subscription levels. For this example, I am going



254

The Lean Product Playbook

to assume we are not freemium (all of our services require a paid subscription), but we do offer a free trial. Paying Users = New Paying Users + Repeat Paying Users As with the previous example, we break down the number of paying users into the new ones we acquired in this time period plus the paying users we retained from the past. Repeat Paying UsersT = Paying UsersT−1 × (1 − Cancellation Rate) As with the previous example, we express repeat paying users for this time period (denoted with the subscript T) in terms of the number of paying users from the previous time period (denoted with the subscript T – 1). The cancellation rate is the percentage of paying users who cancel from one time period to the next. This is a very important metric for a subscription business to track and try to improve. We’ve broken down the metrics far enough in this direction, so let’s return to new paying users to break down that metric. New Paying Users = Free Trial Users × Trial Conversion Rate + Direct Paid Signups Some prospects who show up at our site may subscribe to one of our paid offerings right away (direct paid sign ups). Other prospects may sign up for the free trial first (free trial users). Only a certain percentage of free trial users convert to a paid subscription, measured by the trial conversion rate. I won’t write out the equations, but we could further break down the number of free trial users to account for the various channels through which we acquire prospects. Our breakdown should also include a metric for our conversion rate from prospect to free trial user. This would help us measure the effectiveness of our marketing efforts (landing pages, email campaigns, etc.) This second example of the equation of your business for a completely different revenue model shows you how versatile a tool it is—one that can be applied to any business. You should sit down with your team to determine the equation of your business. The goal is to identify the key metrics that you want to measure and try to

Measure Your Key Metrics

improve. The equation of your business helps you understand how much a change in each metric will affect your overall business results, so that you can prioritize where to make improvements. ACHIEVING PROFITABILITY

I explained the equation of your business as it applies to a certain time period (e.g., day, week, or month). Another way to apply the equation of your business is to ignore time and look at it on a per customer basis. Profit = Number of Customers × Profit per Customer This way of looking at profit wouldn’t be as relevant if you were still seeking product-market fit. But if you’ve achieved product-market fit and are trying to reach profitability, it is very valuable. In the above equation, profit per customer is the metric to improve. Let’s peel the onion another layer. Profit per Customer = Revenue per Customer − Cost per Customer There is a very powerful way to break this down further that provides insights into the “per customer” economics of your business. Unlike the last example, which was completely focused on revenue, when you are trying to achieve (or improve) profitability, you have to look at costs. But you should focus on one particular set of costs: the costs associated with acquiring a revenue-generating customer. This can be done by rearranging things a bit in the equation and introducing some new metrics. I’ll share the equation, and then explain it. Profit per Customer = Customer Lifetime Value − Customer Acquisition Cost Customer Lifetime Value

This equation is an alternate expression of profit per customer that is very useful. Customer lifetime value (LTV) is the profit that a customer generates for you without taking into account the cost to



256

The Lean Product Playbook

acquire the customer. Customer acquisition cost is the amount you pay on average to obtain a new customer. Breaking this cost out as a separate metric allows you to track and improve it. When your LTV is greater than your customer acquisition cost, then each new customer generates profit for your business. In order to have actionable metrics you can use to improve LTV, the onion needs to be peeled another layer. LTV = ARPU × Average Customer Lifetime × Gross Margin There’s ARPU again—exactly the same metric discussed earlier— average revenue per user (per time period). For example, if all of your subscribers are paying you $10 per month, your ARPU would be $10 per month. The average customer lifetime is how many time periods your average customer stays with your business. If you multiply ARPU by the average customer lifetime, that tells you how much revenue your average customer generates for you (throughout the entire time they are a revenue-generating customer). Let’s say you analyzed your customer data and found that your average customer lifetime was 10 months. Then average lifetime revenue would be $10 per month times 10 months, or $100. Gross margin is a percentage that accounts for the cost of providing the product or service to the customer. Many high-tech companies have high (over 80 percent) gross margins and therefore ignore this term for simplicity. There are more complex LTV models that account for the fact that the customer revenue isn’t generated all at once but rather over time by discounting the cash flow stream using a cost of capital discount rate. However, you don’t need that extra complexity, since the goal is not to have the most accurate measure of LTV, but simply to break it down into actionable metrics that you can track and improve. The equation shows that you can increase LTV by increasing ARPU. You could increase ARPU by raising your prices, selling more to your existing customers, or adding new higher-priced products, for example. You can also increase LTV by increasing your average customer lifetime, which you can do by decreasing your cancellation rate: the percentage of paying customers that stop paying you each time period.

Measure Your Key Metrics

This is more broadly called churn rate to include revenue models where the customer doesn’t pay you directly. You can also think of it as one minus your retention rate (from one time period to the next). The average customer lifetime can actually be calculated from the churn rate using a simple formula: Average Customer Lifetime =


Churn Rate

For example, if your churn rate is 5 percent per month, then your customer lifetime is 20 months. You can reduce your churn rate by providing better customer service and support, by improving product quality and reliability, and by ensuring your product continues to meet customer needs. Since churn rate is the metric you are going to measure and try to improve, you can restate LTV in terms of it. Setting gross margin aside, this formula makes it very clear that the two ways to increase LTV are to increase ARPU and to decrease churn rate: LTV =

ARPU × Gross Margin Churn Rate

Customer Acquisition Cost

Let’s return to customer acquisition cost (CAC), which you can calculate if you know the number of new customers you added in a given time period and your sales and marketing costs for the same time period: CAC =

Sales and Marketing Costs New Customers Added

That equation is a convenient way to calculate CAC, but isn’t really actionable. To use more actionable metrics, you can break it down as follows: Cost per Acquisition CAC = Prospect Conversion Rate The cost per acquisition (often shortened to CPA) is how much it costs on average for each prospect. Let’s say you advertise on Google AdWords and pay a cost-per-click (CPC) of $1.00. Then your CPA is $1.00 because each person who clicks on your ad will go to



258

The Lean Product Playbook

your website. To increase profit per customer, you want to decrease CAC—which you can do by decreasing CPA. You can achieve a lower CPA by finding lower-cost marketing programs and channels. Perhaps you can find other keywords with lower CPCs, or find some inexpensive ad inventory to buy. For impression-based advertising (such as display ads), you can decrease your CPA by improving the effectiveness of your ads (i.e., increasing your ad clickthrough rates). When people click on your Google ad, they arrive at your landing page or home page and become prospects. From here, they can learn more about your product and become customers. The percentage of prospects that convert into customers is your prospect conversion rate. You can improve this metric by optimizing your landing pages for conversion, which includes improving the messaging and UX design. A/B testing is a great tool for doing that. To generate a profit, you want your LTV to exceed your CAC, and the larger the difference, the larger your profit. Instead of looking at the difference, some businesses prefer to look at the ratio of LTV to CAC. For example, a general guideline for successful SaaS businesses is that your LTV-to-CAC ratio should be greater than three. In this chapter, I show how you can leverage analytics to measure your business and to create a framework for optimization. Before you launch your product, you rely more heavily on qualitative learning; but once you launch a live product, you have a wealth of analytics at your disposal. You can assess how you’re doing on product-market fit by using cohort analysis to track your retention rate over time. In addition, you can use the AARRR framework and the equation of your business to identify the key metrics to improve. You can use LTV and CAC to achieve and improve your profitability. In the next chapter, I build on what you learned in this chapter and share the Lean Product Analytics Process: a repeatable process you can follow to optimize the metrics of your business. I also share a case study that applies that process and the principles from this chapter.

Chapter 14

Use Analytics to Optimize Your Product and Business Chapter 13 covers how to define and measure your key metrics, providing the foundation for using analytics to improve your product and business. The great thing about a live product is that analytics let you clearly see the results of changes that you make. With a good A/B testing framework, you can easily conduct experiments and make improvements rapidly. Companies that do this well have an advantage over their competitors. The size of your current business becomes less relevant; instead, how quickly you can learn from customers and iterate becomes the basis of competition. Speed is a weapon—in today’s fast-paced world, David can unseat Goliath overnight. This chapter shows you how to harness the power of analytics to optimize your product and business. THE LEAN PRODUCT ANALYTICS PROCESS

I’ve worked with many companies to define and implement their analytics framework, which I then used to optimize their product and business. Along the way, I developed a simple, repeatable process for how to use analytics to drive improvements—the Lean Product Analytics Process, illustrated in Figure 14.1. The first step in the Lean Product Analytics Process is to define the key metrics for your business, which I covered in the prior chapter. Next, you need to start measuring these metrics so you can establish a baseline value for each one so you know where you stand today. This step may sound relatively trivial, but a lot of companies stumble here. Setting up metrics tracking for—or instrumenting—your product takes work. After the initial setup, it usually takes additional effort to ensure that the metrics data you’re collecting are accurate and match what you intended to track. Analytics packages such as


260

The Lean Product Playbook

FIGURE 14.1

The Lean Product Analytics Process

Google Analytics, KISSmetrics, Mixpanel, and Flurry can make this task easier. The data for many key metrics often reside in your product’s database, so many companies use a combination of third-party packages and homegrown analytics code. The goal is to have a set of analytics dashboards that make it easy to see how each metric is performing over time. Once you have accurate baseline values for your metrics, you can proceed to the next step: evaluating each metric’s upside potential. This is where you assess each metric through an ROI lens. I find it helpful to think of each metric as a dial on a gauge, as you might see on a car dashboard or an air pump. The value on the dial that the needle is currently pointing to is the baseline value for that metric. You want to evaluate how easy or hard it would be to move the needle— that is, to improve each metric. There will be a diminishing returns curve, and you want to roughly estimate where you are on that curve. Figure 14.2 shows ROI curves for three different metrics. On each chart, the vertical axis shows the value of the metric (where higher is better). From the equation of your business, you should know how much an increase in the value of each metric translates into an improvement in the higher-level metric you are trying to improve

Use Analytics to Optimize Your Product and Business

FIGURE 14.2

ROI Curves for Three Different Metrics

(e.g., revenue). The horizontal axis shows the level of investment required. Each ROI curve shows the improvement opportunities for that metric, and the circle shows the baseline value of where each metric currently is on its curve. Metric A is near the bottom of its ROI curve where the slope is still steep. Therefore, you should be able to make a meaningful improvement in the value of this metric with relatively little effort. This could be the case if you haven’t yet worked on improving metric A. In contrast, metric B is near the top of its ROI curve where the slope has flattened out. Even if you put in a lot of effort, you would only see a small improvement in the value of the metric. This could be the case if you have already spent a lot of effort improving metric B. Most metrics offer the typical diminishing returns curve shown for metrics A and B. However, there are some opportunities where just a small amount of effort can cause a major improvement in a metric’s value. I call these “silver bullets,” and metric C in Figure 14.2 illustrates such an opportunity. These are special cases where a small but profound change can cause some aspect of your product or business to work much better than before. You usually discover these silver bullet opportunities through careful analysis. Once you’ve assessed each metric’s upside potential, you move on to the next step in the process: selecting the metric that offers the most promising opportunities for improvement. This is the “metric that matters most” (MTMM) discussed in the previous chapter. As Figure 14.1 indicates, this is the point where you transition from a global perspective across all your metrics to focus on just the MTMM.



262

The Lean Product Playbook

You want to brainstorm as many improvement ideas as you can for this top metric. Then you want to estimate how much each idea will improve the metric. When you do so, you are forming hypotheses, such as “Creating a mobile optimized version of the registration page will improve the conversion rate from 20 to 30 percent.” You also want to estimate the effort for each idea so you can evaluate ROI (as discussed in Chapter 6). You then pick the highest ROI idea to pursue. Next, you design and implement the top improvement idea. Ideally, you would use an A/B testing framework to roll out the improvement to a fraction of your users. This gives simultaneous metrics results that you can compare to assess the relative performance of your improvement versus the status quo. If you don’t have an A/B testing framework and the metric you’re trying to improve has had a relatively stable value, you can roll out the change and do a before-and-after comparison. However, A/B testing is better because it reduces the risk of other unknown factors causing a difference in the results. Of course, you hope your target metric improves. But you’ve made progress even if it doesn’t because you’ve gained valuable learning that you can apply to create better hypotheses as you iterate in the future. You now revisit your list of ideas to improve the metric and select the next best idea, repeating the loop shown on the right side of Figure 14.1. Eventually, you should see the target metric improving after trying several ideas. You can continue to iterate and improve this metric and should experience diminishing returns as you do. At some point, a different metric will offer greater opportunities for improvement. As shown in Figure 14.1, that is when you jump back to the global metrics perspective and identify the next top metric for improvement (MTMM). You then apply the iterative improvement loop on that metric. Repeatedly following this process allows you to systematically drive improvements to your business. Having a robust analytics framework and set of dashboards lets you easily track how your business is doing. Having an A/B testing platform lets you continuously experiment to see if new ideas can outperform the current champion. Once you have the critical elements in place—the analytics framework, the dashboards, the A/B testing platform, and

Use Analytics to Optimize Your Product and Business

a continuous improvement process—the limiting factor just becomes how quickly you can identify and implement good, creative ideas to throw into the machine. Avoiding a Local Maximum

That brings up a good point, which is to be careful not to get stuck at a local maximum. In the process of improving a metric, you may reach a point where it seems that you can’t improve it any further. Sometimes, it’s true that you’ve fully maximized the metric and aren’t able to improve it any further. However, sometimes you are stuck at a local maximum but actually could improve the metric further by considering a completely different alternative or approach. For example—if you have a landing page, you could A/B test different colors for your primary call-to-action button to find which yields the highest conversion rate. Google famously A/B tested 41 different shades of blue for a toolbar to see which color resulted in the highest clickthrough rate. However, if you stop iterating after you find the best color for the button, you’ll probably be stuck at a local maximum. You should also experiment with different messaging, images, page layouts, and so forth, to see if you can achieve an even higher conversion rate. Your rate of improvement depends on how quickly you can identify and implement good ideas. A/B testing makes experimentation easy, but it’s up to you to determine the hypotheses to test. To avoid getting stuck at a local maximum, you want to make sure you cast a wide net in coming up with potential improvement ideas. A LEAN PRODUCT ANALYTICS CASE STUDY: FRIENDSTER

To reinforce the Lean Product Analytics Process and help bring it to life, I’ll walk through an end-to-end case study of the process in action. This is a real-world example from Friendster where I more than doubled a key metric in just one week by applying this process. When I joined social networking startup Friendster as the head of product, it was clear that viral customer acquisition was important. We had a large user base and were generating some advertising revenue, but the average revenue per user was too low to justify spending



264

The Lean Product Playbook

money to acquire customers (as is often the case with large-scale consumer businesses). Fortunately, we didn’t have to—viral marketing allowed us to acquire users for free. Because of network effects, the value of a social networking product like Friendster increases exponentially with the number of active users. We knew that rapidly growing our user base was critical to success, and viral marketing offered the best way to do that. Therefore, I made improving our viral growth one of my top objectives. Everyone in the company shared this perspective, but no one had actually measured how our viral growth was doing. So I started with the first step of the Lean Product Analytics Process: defining our key metrics. Deﬁne Your Key Metrics

We were tracking new users, and were also able to track which new users had been invited to join versus those that hadn’t. While “new users from invites” was a high-level metric we cared about, it wasn’t actionable. So I applied my equation of the business technique to break this high-level metric down into more actionable metrics that we could try to improve. I started by defining our viral loop: the steps by which we acquired a new customer from an existing customer, shown in Figure 14.3. The process starts with our current users in the bottom left box. However, not all users generate new customers through viral marketing. While we have a larger number of registered users, only our active users invite their friends to join Friendster (the inactive ones don’t). So I broke out active users separately. Using our product, active users

FIGURE 14.3

Friendster Viral Loop

Use Analytics to Optimize Your Product and Business

FIGURE 14.4

Friendster Viral Loop Metrics

send email invitations to their friends who aren’t yet using Friendster (prospective users). When a prospective user receives the email invitation, they either click on the link in the email to sign up for Friendster or they don’t. The ones that do end up going through our registration process, which some people complete and some don’t. The invitees that successfully complete registration become users, who can go on to be active users and repeat the loop again. Having defined the viral loop, I next wanted to determine the metrics I would use to track it. I didn’t want to track “atomic” metrics, such as the number of active users, because the values would fluctuate with the size of user base. Instead I wanted to identify “normalized” ratio metrics that enabled apples-to-apples comparisons over time. I came up with a set of five metrics that met that criterion and that, taken together, fully captured all aspects of our viral loop, shown in Figure 14.4: 1. Percentage of users who are active: This metric was calculated by dividing the number of active users by the total number of registered users. 2. Percentage of users sending invites: Not all users sent invites, so this metric let us isolate that factor. It was calculated by dividing the number of users who sent invites by the number of active users. 3. Average number of invites sent per sender: When users sent invitations to their friends, they could invite just one friend or



266

The Lean Product Playbook

several friends at a time. This metric was calculated by taking the total number of invites sent divided by the number of users who sent invites. 4. Invite clickthrough rate: The percentage of prospective users who clicked on the link in the email invitation, calculated by taking the number of prospective users who clicked on the link divided by the number of prospective users who were sent an invitation. 5. Registration conversion rate: The percentage of prospective users arriving at the registration page that actually completed the registration process. This metric was calculated by dividing the number of prospective users who registered by the number of prospective users who visited the registration page. These metrics apply to and can be calculated for any given timeframe (e.g., past 30 days). Multiplying these five factors together gives the viral coefficient of the loop. If your coefficient is greater than one, then your product is officially “viral,” which means that each current user generates more than one new user, resulting in exponential growth—like a nuclear reactor that goes supercritical. Products don’t remain viral for long periods of time (if they did, everyone with Internet access would become a user). When a viral product achieves high market penetration, there just aren’t as many prospective users left to join. Facebook is in this enviable position. That being said, a viral coefficient that’s less than 1 but still high—say 0.4—is nothing to sneeze at. That still means you’re growing your user base by 40 percent per time period for free through viral marketing. Measure Baseline Values for Metrics

After identifying these five metrics, the next step in the process is to establish the baseline value for each one. There is no comparison between today’s third-party analytics packages and those available when I worked at Friendster. We wrote our own code to track and calculate these metrics. We started by capturing the data for each atomic metric, such as registered users, active users, email clicks, and so forth. We then calculated the five ratio metrics from these atomic metrics.

Use Analytics to Optimize Your Product and Business

For the sake of simplicity, I’ll continue the case study by focusing on just three of the five metrics and their baseline values: Percentage of users sending invites = 15 percent Average number of invites sent per sender = 2.3 ● Registration conversion rate = 85 percent ● ●

Evaluate ROI Potential for Each Metric

The next step in the Lean Product Analytics Process was to select which metric we thought offered the greatest opportunity for improvement. Imagine for a minute that you were in my shoes. Recognizing that you only have the limited information I’ve shared here, which of these three metrics would you choose to focus on improving first? How would you decide? I realize you don’t have specific information about potential improvement ideas for each metric. Taking an ROI approach, it’s difficult to estimate the return, or increase in value, that could realistically be achieved for each metric. A hack you can use when you don’t have much information is what I call the upside potential of a metric—that is, what the maximum possible improvement could be. You estimate this by considering the metric’s current baseline value and its maximum possible value. See Figure 14.5, which illustrates the concept using our three metrics.

FIGURE 14.5

The Upside Potential of a Metric



268

The Lean Product Playbook

Let’s start by looking at registration conversion rate. It’s a percentage, so it can range from a minimum of 0 percent to a maximum of 100 percent. The current value is 85 percent. So no matter what improvements we make, we can only increase the metric an additional 15 percentage points (to 100 percent). To express this upside potential as a percentage, we take 15 percent and divide it by 85 percent, which is 18 percent. So the maximum upside potential for registration conversion rate is 18 percent. The second metric—the percentage of users sending invitations— can also range from 0 to 100 percent. Its current value is 15 percent, so we could theoretically improve this metric by as much as 85 percentage points. To express this upside potential as a percentage, we take 85 percent and divide it by 15 percent, which is 570 percent. So the percentage of users sending invitations has significantly more upside potential than the registration conversion rate. Now let’s turn to the third metric: average number of invites sent per sender. That metric is not a percentage. Its minimum value is 0. Its current value is 2.3. What is its maximum possible value? Offhand, it’s seems hard to say exactly. But we need to have at least an estimate of the maximum value to calculate the upside potential of this metric. Could it be infinity? No, because there are a finite number of people in the world. Each user could invite all of his or her friends to join Friendster. So the maximum value would be the average number of friends that a Friendster user has. What is that number? I didn’t know exactly, but I thought a reasonable estimate was between 100 and 200. In the 1990s, psychologist Robin Dunbar conducted research on the maximum number of people with whom a person can maintain stable social relationships. He concluded this limit—called Dunbar’s number—is 150, which is the middle of my estimated range. If we use 150, we see that the upside potential of the average number of invites sent per sender is 150 ÷ 2.3 = 6,520%. Even using the more conservative value of 100, the upside potential of this metric far exceeds that of the other two metrics. When you saw Figure 14.5, you may have experienced déjà vu. Take a look at the three metric ROI curves in Figure 14.2 again. Do you sense a similarity? The percentage of users sending invites is like metric A, offering a good ROI. The registration conversion rate is like metric B, offering a bad ROI. The average number of

Use Analytics to Optimize Your Product and Business

invites sent per sender could be like metric C. We won’t know for sure until we see how much we can move the needle and how much effort that takes. Select Top Metric to Improve

I decided to focus on trying to improve the average number of invites sent per sender, mainly due to its much larger upside potential. I also chose to focus on this metric because improving it didn’t necessarily involve trying to change human behavior. Fifteen percent of our users were already sending invites; we were just going to try to get them to send more. In contrast, trying to increase the percentage of users who invited friends would require behavior change. For whatever reasons, the other 85 percent of users had chosen not to invite their friends, despite our best efforts to get them to do so. It was harder to envision how we would move the needle much on that front. I also knew that our current user experience for inviting friends required too much manual effort and was confident that we could improve the user experience to make it easier. The Metric Optimization Loop

Now that I had selected the average number of invites sent per sender as the top metric to improve, I moved on to the next step in the Lean Product Analytics Process. At this point, I entered the metric optimization loop shown in the right side of Figure 14.1. I brainstormed potential improvement ideas with the team. We then discussed for each idea how much we thought it would improve the metric and how much effort it would take. After doing so, we concluded that our highest ROI idea was an address book importer. Though address book importers are commonplace now, they weren’t back then. Many of our users had stored their friends’ email addresses in an address book that was tied to their email account at providers such as Gmail and Yahoo! Mail. The address book importer would let users enter the credentials for their email website and then import their friends’ contact information into Friendster. The address book importer we designed displayed the list of imported contacts and let users select the ones they want to invite. We hypothesized that building such an importer



270

The Lean Product Playbook

would help us to significantly improve the average number of invites sent per sender. On the technical implementation side, we could leverage some of the initial development work across all of the various email providers, but there was a certain amount of work required for each different email service. At this point, I realized it would be beneficial to break the feature into smaller pieces (as discussed in Chapter 6), with a feature chunk for each email provider. To test our hypothesis with the least amount of effort, I decided to pursue an MVP address book importer that only worked with one email service. After analyzing our user information, I found that Yahoo! Mail was the most popular email service among our users. So that feature chunk offered the highest ROI. The next step in the process was to design and implement the solution, which took about one week of work for a product manager and a developer. Silver Bullet or Not?

We launched our improvement and proceeded to the next and most exciting step in the Lean Product Analytics Process: watching how our metric changed. Figure 14.6 shows a chart of the metric before and after we launched our improvement. The vertical axis shows the average number of invites sent per sender and the horizontal axis shows the date. Like many websites, we had seasonality in our usage patterns; many metrics varied quite a bit from weekday to weekend. As a result, we tracked seven-day averages for most metrics to more easily see trends. The data point for each day in Figure 14.6 actually displays the average of the metric’s value for the trailing seven days. Looking at the chart, we had over a month of baseline data where the value of the metric was quite stable, staying between 2.2 and 2.4. Where the graph changes from the smooth horizontal line and starts to shoot up and to the right corresponds with the date we launched our improvement. Because we plotted the seven-day average, it took several days for the chart to catch up with the new average value after launching the address book importer. The new value for the average number of invites sent per sender kept growing each day and then settled out around 5.3. I was ecstatic!

Use Analytics to Optimize Your Product and Business

FIGURE 14.6

Average Number of Invites Sent per Sender: Before and After

This was a silver bullet improvement: Just a week’s worth of work had more than doubled this key metric (5.3 ÷ 2.3 = 2.3×)! Going back to the equation of the business, a 2.3× improvement in this metric directly translated into a 2.3× improvement in the number of new customers we were acquiring from viral growth. And we had only built the importer for one email provider. With this clear quantitative evidence that validated our hypothesis, we proceeded to complete the incremental work required to add additional email providers to our importer, which yielded additional gains in this key metric. We continued to improve the average number of invites sent per sender for a while longer but then exhausted all the high ROI improvement ideas we could identify. At that point, we exited the improvement loop for that metric and switched our focus to a different viral loop metric that offered a higher ROI. This example shows how easy it can be to use analytics to improve your business. By applying the Lean Product Analytics Process,



272

The Lean Product Playbook

you can achieve similar results. As with the MarketingReport.com case study in Chapter 11, I didn’t do anything extraordinary; I just followed the process and principles I’ve described in this book. OPTIMIZATION WITH A/B TESTING

As Chapter 7 discusses, A/B testing, also called split testing, is a quantitative technique where you test two (or more) alternatives simultaneously to compare how they perform. At the time that I worked at Friendster, A/B testing tools were not readily available and building an in-house tool would have required a large amount of valuable engineering resources. That’s why I did a “before and after” comparison of the metric I was trying to improve, which worked out fine. Nowadays, you would ideally run an A/B test for each improvement idea you implement. Running the new version concurrent with the old one helps avoid other potential sources of variation. An important concept in A/B testing is statistical significance, which is determined by the difference in performance and the sample size. There are online tools to help you calculate the statistical confidence level for your test. So you don’t necessarily need to know the formula, but it’s important to know that statistical significance is higher for larger differences in performance and for larger sample sizes. If your sample size is too low, you won’t achieve statistically significant results. If you have two alternatives with very similar performance, it may take a very large sample size to discern any statistically significant difference. There are numerous third-party A/B testing tools available, including Optimizely, Unbounce, KISSmetrics, Visual Website Optimizer, and Google Content Experiments (part of Google Analytics). Many companies also choose to create their own in-house A/B testing platform. These tools let you specify one or more variations and then randomly distribute traffic among the variations. They keep track of the results for the metric you care about and show you how each variation is performing, along with statistical confidence levels based on the sample sizes. Many companies have incorporated A/B testing into their product release process, especially when making major changes. Instead of instantly switching from the old version of their product by

Use Analytics to Optimize Your Product and Business

launching the new version, they keep the old version running for almost all users and “launch” the new version to a small percentage of users. Then, they compare key metrics across the new and old versions. Before ramping up the percentage of users who see the new version, the product team wants to make sure the metrics targeted for improvement are performing better and that other key metrics aren’t materially worse. This process, called throttling, is a great way to apply Lean principles to reduce risk after you’ve launched your product. Eventually, if the metrics look good, 100 percent of users are switched to the new version and the old version just goes away. Netflix is known for its robust A/B testing on both the marketing and product fronts. In response to the question, “What types of things does Netflix A/B test aside from member sign-up?” on question and answer website Quora, Netflix Chief Product Officer Neil Hunt replied: “Short answer—almost everything.” Hunt explained how Netflix tests different user interface variations, recommendation algorithms, button placements and sizes, page load times, and quality levels of video streaming encoding. Hunt closed his response with: We are very proud of our empirical focus, because it makes us humble—we realize that most of the time, we don’t know up-front what customers want. The feedback from testing quickly sets us straight, and helps make sure that our efforts are really focused at optimizing the things that make a difference in the customer experience. Is A/B Testing All You Need?

A/B testing is the ultimate evidence-based product decision-making tool. You are generating data from the real-world behavior of many users, so there is no risk of a disconnect between what users tell you and what they do. You are not by their side as they experience the test, so there is no risk of you perturbing the results. Of course, a product team cannot live on quant alone—don’t forget about Oprah. There will be times when you need to complement your quant testing with qualitative learning to understand the whys behind the behavior. Product teams that reach the point of enjoying rapid, iterative A/B testing have come a long way from their pre-MVP days, when there



274

The Lean Product Playbook

was less hard data available to make decisions. Some people might be tempted to skip all the qualitative testing and learning, just launch an MVP candidate and attempt to A/B test their way to product-market fit. That approach would almost certainly waste resources and fail. In that scenario, A/B testing would most likely guide you to an inferior local maximum that leaves you far from product-market fit. Let’s refer back to the Product-Market Fit Pyramid, shown again in Figure 14.7. The hypotheses you make in one layer affect all the layers above it. Your UX is the easiest layer to change. You can also change your feature set, but it takes more effort. But the foundational elements of product-market fit—your target customers, their underserved benefits, and your value proposition—are difficult to change once you’ve built your product. Once you’ve locked in your hypotheses for these layers, they are like a set of interconnected tectonic plates. If you move one of them after you’ve already built your product, much of the product you’ve built will no longer be relevant—like an earthquake that reduces a building to rubble. When that happens, human nature can make you want to salvage and reuse as much as of your work as you can. But doing so can add onerous constraints to your solution space, which is suboptimal when you

FIGURE 14.7

Product-Market Fit Pyramid

Use Analytics to Optimize Your Product and Business

are changing your problem-space hypotheses. You would be better off building again from scratch on top of the new foundation. The Lean Product Process follows the sequence it does so that you validate your key hypotheses in the order that most reduces risk and increases your odds of achieving product-market fit. The problem space is not as amenable to A/B testing as the solution space. The three lower layers of the Product-Market Fit Pyramid require qualitative research for you to create, test, and improve your hypotheses. There is a natural progression from more qualitative learning to more quantitative learning after you launch your product. In a nutshell: qualitative helps you define your product and quantitative helps you optimize your product. You need both Oprah and Spock to create a successful product.



Chapter 15


My goal for this playbook is to help you create products that customers love. It began with the Product-Market Fit Pyramid—an actionable model that defines the components of product-market fit and how they are connected. Your market consists of your target customers and their needs, and your product is the combination of your value proposition, feature set, and user experience. When you try to achieve product-market fit, you make critical hypotheses at each of these five layers. The Lean Product Process guides you through the formulation and testing of your hypotheses with these six steps: 1. Determine your target customers 2. Identify underserved customer needs 3. Define your value proposition 4. Specify your minimum viable product (MVP) feature set 5. Create your MVP prototype 6. Test your MVP with customers The process starts in the problem space and progresses to the solution space. You begin by determining your target customers, which you describe using personas. To create the most value for customers, you use the importance versus satisfaction framework to identify their important but underserved needs. Using the Kano model, you define a differentiated value proposition that better meets those needs for your target customers. You then take an MVP approach, trying to identify the minimum set of functionality required to deliver the key parts of your value proposition. You bring your MVP feature set to life by applying the principles of great UX design to create a prototype with a usable and delightful user experience. To assess product-market fit, you test your MVP candidate with your target customers, who can give you better feedback in the


278

The Lean Product Playbook

solution space than in the problem space. To save resources and iterate more quickly, you ideally test with design deliverables such as clickable or tappable mockups before actually building your product. You use the knowledge you gain to revise your hypotheses and your MVP candidate. You continue to iterate through the hypothesize-design-test-learn loop with additional waves of user testing, hopefully achieving higher and higher levels of product-market fit. As you test, you may decide to pivot to a more promising opportunity by changing one or more of your fundamental assumptions. Once you have validated your product-market fit, it’s time to build your MVP. To reduce risk and deliver customer value more quickly, you should build your product in an incremental, iterative manner using Agile development. QA and test-driven development help achieve higher product quality; continuous integration and continuous deployment help improve the speed of your development process. After you’ve launched your product, you employ analytics to understand how customers are using it. Your retention rate gives you a quantitative measure of product-market fit, and cohort analysis shows you how it changes over time. Once you have good retention, you can focus on improving other macro-metrics in Dave McClure’s AARRR framework (acquisition, activation, retention, revenue, and referral). Defining the equation of your business helps you identify the key metrics for your particular business, and the Lean Product Analytics Process provides a systematic way of optimizing your metrics, resulting in greater revenue and profitability. I hope you find the Lean Product Process and the other guidance in this book valuable. In addition to the advice I’ve shared throughout the book, I want to leave you with this list of 10 best practices for creating successful products. 1. Have a point of view but stay open-minded. As you probably realize, building products is not for the faint of heart. You constantly have to make decisions under conditions of uncertainty. Therefore, it’s important to have a point of view and be decisive. At the same time, you should identify how to test the areas of greatest uncertainty and risk. As you test, you should avoid anchoring on your initial point of view and instead be objective and evidence-based. By listening with

Conclusion

an open mind, you will gain the most learning, which you should use to revise and improve your thinking. 2. Articulate your hypotheses. Creating a product requires that you make a lot of decisions and assumptions. An interesting way to think of a product is to view it as the collection of all the hypotheses that led to it becoming what it is. You should try to be as explicit as possible about the hypotheses you are making. Writing down your hypotheses is incredibly helpful. As Admiral Hyman G. Rickover said, “Nothing so sharpens the thought process as writing down one’s arguments.” Your teammates should do the same, and you should make your team’s hypotheses transparent. By posting your hypotheses where everyone on the team can review them and by openly discussing them, they will only get better. 3. Prioritize ruthlessly. There are many ideas contending for resources when you are creating a product, and tradeoffs are unavoidable. Being vague about your priorities usually leads to inefficiency and indecision. That’s why I recommend rank ordering your backlog and all other to-do lists. Clearly identifying what is most important helps you spend your valuable resources and time wisely. As Peter Drucker said, “Time is the scarcest resource and unless it is managed nothing else can be managed.” 4. Keep your scope small but focused. Related to prioritization is the idea of deliberately keeping your scope small. As discussed in Chapter 6, smaller batch sizes encourage focus and are completed more quickly, enabling faster feedback from customers. Be careful not to bite off more than required to accomplish your objective. This doesn’t mean that you should avoid tackling large tasks altogether—just that you should try to split them up into smaller items to reduce risk and iterate more quickly. 5. Talk to customers. Your customers are the judges of productmarket fit; they help you obtain the learning that you need to achieve it. The sooner and more frequently you talk with customers, the better. It’s worth investing the effort to establish systems that make your user testing easier to schedule and conduct, so that you talk to more users over time. Don’t allow too much time to pass since your last user test; customers will always surprise you with unexpected learning. 6. Test before you build. Many teams rush to build their product without testing any of their hypotheses. But building before you’ve



280

The Lean Product Playbook

validated product-market fit will almost certainly waste resources. It is faster and less costly to iterate with design deliverables than with an actual product. Plus, once a team builds a product, they naturally grow attached to it, which can cause them to be less open-minded and less willing to make major changes. 7. Avoid a local maximum. As you’ll recall from Chapter 14, a local maximum means you have achieved the best results possible within the range of options you have considered, but that better alternatives—that you haven’t considered—exist outside of those options. You can tell you are in a local maximum when you are unable to drive additional improvements to your product-market fit or to your key metrics. At this point, you need to take a fresh perspective to make further progress. Shift your current thinking to a higher level and use divergent thinking to come up with new ideas worth exploring. 8. Try out promising tools and techniques. Team members often employ tools and techniques with which they have prior experience. Some product teams can be somewhat insular in this area, sticking to what they know instead of seeking out potentially better solutions. In contrast, many product teams proactively investigate new tools and techniques once enough people deem them better than the status quo. You don’t want to constantly change based on the latest fad, but it’s valuable to compare notes with others and stay relatively current on this front. You should give promising new ideas a try when they could significantly improve how your team accomplishes its work. 9. Ensure your team has the right skills. As you can see from the breadth of topics this book covers, creating a successful product requires a wide range of skills. For software products, the list of skills includes product management, user research, interaction design, visual design, copywriting, Agile development, front-end coding, back-end coding, QA, DevOps, and analytics. Different product teams will possess different levels of each important skill. You should assess where your team is strong and where it is weak. Identify which skill improvements will make the biggest difference in your situation and try to augment your team accordingly (e.g., through additional hires, contractors, advisors, or training).

Conclusion

10. Cultivate your team’s collaboration. I like to say that building products is a team sport. Picture a basketball team of five players. The guards, forwards, and center each have their own role. To achieve their goal of scoring a basket, the five individuals need to coordinate their actions as a team, passing the ball to one another to execute the play. A product team creating a new feature is like a basketball team scoring a basket. The product manager drives the ball down the court by writing user stories and prioritizing the backlog. The product manager passes the ball to the interaction designer, who designs the flows and wireframes and then passes the ball to the visual designer. The visual designer creates the look and feel with high-fidelity mockups and passes the ball to the developer. The developer, who implements the product based on the user stories and mockups, shoots the ball and scores the basket. Strong skills alone don’t make a great product team. Team members must each understand their role, the other roles on the team, and how the team works together to achieve its goals. You should take an occasional break from working to discuss how you work as a team and how you can do so better. It’s fun being on a team that works well together, and strong collaboration increases your chances of building a successful product. I encourage you to visit this book’s companion website http:// leanproductplaybook.com. There you’ll find new and updated information related to the topics I covered in the book. The website is also a place for us to share and discuss ideas with others who are passionate about building great products. You can also find me online at: @danolsen on Twitter: http://twitter.com/danolsen LinkedIn: http://linkedin.com/in/danolsen98 SlideShare: http://slideshare.net/dan_o Lean Product Meetup: http://meetup.com/lean-product Olsen Solutions consulting: http://olsensolutions.com I would enjoy hearing about your experiences applying the ideas in this book as well as any questions or feedback you have. You can reach me at dan@leanproductplaybook.com. I hope you find my advice in this playbook useful and that it helps you achieve success with your products.

Preface: Traction Trumps Everything

In 2006 I sold for millions of dollars an Internet company that I had cofounded a few years earlier. It was a strange company for many reasons, not the least of which was that we had no employees from beginning to end. I wrote every line of code and did all the accounting and customer support. The terms of the deal were such that my cofounder and I didn’t have to work for the acquiring company at all. We were free to move on to other things, and we did. A few months later my wife and I moved from our 865-square-foot apartment near Boston to a country house twenty-five miles outside of Philadelphia. I had just turned twenty-seven. She went to her job and I sat at home doing nothing for the first time in my life. We knew no one for a hundred miles in any direction. Naturally, I started tinkering on the computer again, starting about a dozen side projects simultaneously. A year and a half later, I thought I was on to something. I noticed two things that bothered me about Google: too much spam (all those sites with nothing but ads) and not enough instant answers (I kept going to Wikipedia and IMDb). I thought if I could easily pick out the spam and the answers, then I’d have a more compelling search engine. Both problems were harder to solve than I initially thought, but I thoroughly enjoyed the work and kept at it. Everyone I talked to about my search engine project thought I was nuts. You’re doing what? Competing against Google? Why? How? Another year later, in the fall of 2008, I flipped the switch, unveiling my search engine to the public. DuckDuckGo had a rather uneventful launch, if you can even call it a launch. I posted it to a niche tech site called Hacker News and that was the long and short of it. The post was entitled “What do you think of my new search engine?” Like many entrepreneurs, I’m motivated by being on the cusp of something big, and I was at the point where I needed some validation. 

I can survive on little, but I needed something. I got it. Granted, the product wasn’t anything you’d want to switch to at that point, and people let me know that. It was an Internet forum, after all. However, I still felt there was genuine interest in a new search competitor. I could tell some people were growing wary of what Google was becoming. For example, those initial conversations led me to investigate search privacy and eventually become “the search engine that doesn’t track you,” years before government and corporate surveillance became a mainstream issue. In any case, the response I received was enough motivation to keep me going. Which brings me to traction. I needed some. Traction is the best way to improve your chances of startup success. Traction is a sign that something is working. If you charge for your product, it means customers are buying. If your product is free, it’s a growing user base. Traction is powerful. Technical, market, and team risks are easier to address with traction. Fund-raising, hiring, press, partnerships, and acquisitions all become much easier. In other words, traction trumps everything. My last startup had grown using two traction channels: first, search engine optimization (ranking high in search engines for relevant terms), and later, viral marketing (where your customers bring in other customers, such as by referring friends and family through use of the product). Viral marketing doesn’t work well in search because you can’t easily bake it into the product by putting stuff between people and their search results. So I tried search engine optimization. The terms “search engine” and “search engines” were too hard to rank for, as the high-ranking companies had been around for a decade and had tens of thousands of links pointing at them from their long histories. “New search engine” was more in my grasp. I worked hard for many months to rank high for this phrase. The key to good search engine optimization (SEO) is getting links. As you will read later in the SEO chapter, you need a strategy to get these links in a scalable way. Getting stories written about you in blogs and news outlets is a common SEO linking strategy. However, I hit saturation with that channel strategy pretty quickly and it didn’t get me to the top. Something more creative was required. After much brainstorming and experimenting, I eventually hit upon a good idea. I built a karma widget that would display links to your social media profiles and how many followers you had on each service. 

People would embed it on their sites and at the bottom there would be a link back to DuckDuckGo that said “new search engine.” This channel strategy worked beautifully. I was number one. Trouble was, not a ton of people make that search—about fifty a day. So while I did get some traction and a steady stream of new users, it leveled off pretty quickly. It wasn’t enough traction to be meaningful. It didn’t move the needle. I made two large traction mistakes here. First, I failed to have a concrete traction goal. In retrospect, to move the needle for my traction goals at the time, I needed more like five thousand new visitors a day, not fifty. Search engine optimization was not going to get me there. Second, I was biased by my previous experience. Just because my last company got traction in this way didn’t mean it was right for every company. These are very natural mistakes to make. In fact, most startups make them. The most common startup trajectory now goes something like the following: Founders have an idea for a company they’re excited about. Initial excitement turns into a struggle to build a product, but they do get something out the door. Launch! The founders expected customers to beat a path to their door, but unfortunately that isn’t happening. Getting traction was an afterthought, but now they are focused on it. They try what they know or what they’ve heard others do: some Facebook ads, a little local PR, and maybe a smattering of blog posts. Then they run out of money and the company dies. Sadly, this is the norm. Even sadder, often these products are actually on to something. That is, with the right traction strategy they might have actually been able to get traction and not go out of business. Given my previous startup success I thought I knew what I was doing. I was wrong. Luckily, I wasn’t dead wrong. I had the money to self-fund through my traction mistakes, and so they didn’t prove fatal for DuckDuckGo. Not everyone is as lucky. Right when I realized I was making these mistakes I also realized I didn’t know the right way to go about getting traction at all. I asked around. It turns out there was no good framework for getting traction, and that’s how this book was born, way back in 2009. Around this time I also started angel investing and more seriously advising other startups. I saw firsthand similar struggles and mistakes. 

I also partnered with Justin Mares, my coauthor. Justin founded two startups (one of which was acquired) and recently ran growth at Exceptional Cloud Services, which was acquired by Rackspace in 2013 for millions. He’s a growth expert in his own right. We set out to help startups get traction no matter what business they were in: from Internet companies to local small businesses and everything in between. We drew on our personal experiences, interviewed more than forty founders, studied many more companies, and pulled out the repeatable framework they used to succeed. That framework is Bullseye, a simple three-step process for getting traction. Bullseye works for startups of all kinds: consumer or enterprise focused, large or small. Since DuckDuckGo’s humble beginnings, we have grown five orders of magnitude (10x growth spurts), from that initial one hundred searches a day to now over ten million a day. Each step—from 100 to 1,000, 10,000 to 100,000, 1,000,000 to 10,000,000—involved figuring out how to get traction again. That’s because, as you will see, often what works in one growth stage eventually stops working. Thankfully we had Bullseye to help us find the right traction channel strategy at the right time. After my search engine optimization mistake, we shifted to using content marketing, social and display ads, publicity, and most recently business development. We’ve hit the bull’s-eye repeatedly, and so can you.

CHAPTER ONE

Traction Channels

Before we get started, let’s define traction. Traction is a sign that your company is taking off. It’s obvious in your core metrics: If you have a mobile app, your download rate is growing rapidly. If you’re running a subscription service, your monthly revenue is skyrocketing. If you’re an organic bakery, your number of transactions is increasing every week. You get the point. Naval Ravikant, founder of AngelList, an online platform that helps companies raise money, says it well: Traction is basically quantitative evidence of customer demand. So if you’re in enterprise software, [initial traction] may be two or three early customers who are paying a bit; if you’re in consumer software the bar might be as high as hundreds of thousands of users. You can always get more traction. The whole point of a startup is to grow rapidly. Getting traction means moving your growth curve up and to the right as best you can. Paul Graham, founder of startup accelerator Y Combinator, puts it like this: A startup is a company designed to grow fast. Being newly founded does not in itself make a company a startup. Nor is it necessary for a startup to work on technology, or take venture funding, or have some sort of “exit.” The only essential thing is growth. Everything else we associate with startups follows from growth. Traction is growth. The pursuit of traction is what defines a startup.

After interviewing more than forty successful founders and researching countless more, we discovered that startups get traction through nineteen different channels. Many successful startups experimented with multiple channels until they found one that worked. We call these customer acquisition channels traction channels. These are marketing and distribution channels through which your startup can get traction: real customer growth. We uncovered two broad themes through our research. First, most founders consider using only traction channels with which they’re already familiar, or those they think they should be using because of their type of product or company. This means that far too many startups focus on the same channels and ignore other promising ways to get traction. In fact, often the most underutilized channels in an industry are the most promising ones. Second, it’s hard to predict the traction channel that will work best. You can make educated guesses, but until you start running tests, it’s difficult to tell which channel is the best one for you right now. Our introductory chapters 2–5 expand on these themes. Chapter 2 introduces you to traction thinking: the mind-set you need to adopt to maximize your chances of getting traction. Chapter 3 presents our framework for getting traction called Bullseye. Essentially, it involves targeted experimentation with a few traction channels, followed by laser focus on the core channel that is most promising. Chapter 4 explains how to go about running traction tests, a central theme of Bullseye. Chapter 5 presents a second framework—called Critical Path—to help you focus on the right traction goal and ignore everything else not required to achieve it. Before you jump into this material, however, we’d like to introduce you to the nineteen traction channels and some of the people we interviewed for them. We will explore each of these channels in chapters 6–24. When going through the traction channels, try your best not to dismiss them as irrelevant for your company. Each traction channel has worked for startups of all kinds and phases. As mentioned, the right channel is often an underutilized one. Get one channel working that your competitors dismiss, and you can grow rapidly while they languish. 

Targeting Blogs

Popular startups like Codecademy, Mint, and reddit all got their start by targeting blogs. Noah Kagan, Mint’s former director of marketing, told us how he targeted niche blogs early on, and how this strategy allowed Mint to acquire forty thousand customers before launching. Publicity

Publicity is the art of getting your name out there via traditional media outlets like newspapers, magazines, and TV. We interviewed Jason Kincaid, former TechCrunch writer, about pitching media outlets, how to form relationships with reporters, and what most startups do wrong when it comes to publicity. We also talked with Ryan Holiday, media strategist and bestselling author of Trust Me, I’m Lying, to learn how startups could leverage today’s rapidly changing media landscape to get traction. Unconventional PR

Unconventional PR involves doing something exceptional like publicity stunts to draw media attention. This channel can also work by repeatedly going above and beyond for your customers. Alexis Ohanian told us some of the things he did to get people talking about reddit and Hipmunk, two startups he cofounded. Search Engine Marketing

Search engine marketing (SEM) allows companies to advertise to consumers searching on Google and other search engines. We interviewed Matthew Monahan of Inflection, the company behind Archives.com (before its $100 million acquisition by Ancestry.com), to learn how Archives relied primarily on SEM for its growth. Social and Display Ads

Ads on popular sites like reddit, YouTube, Facebook, Twitter, and hundreds of other niche sites can be a powerful and scalable way to reach new customers. We brought in Nikhil Sethi, founder of the social ad buying platform Adaptly, to talk with us about getting traction with social and display ads.

Offline Ads

Offline ads include TV spots, radio commercials, billboards, infomercials, newspaper and magazine ads, as well as flyers and other local advertisements. These ads reach demographics that are harder to target online, like seniors, less tech-savvy consumers, and commuters. Few startups use this channel, which means there’s less competition for many of these audiences. We talked with Jason Cohen, founder of WP Engine and Smart Bear Software, about the offline ads he’s used to acquire customers. Search Engine Optimization

Search engine optimization (SEO) is the process of making sure your Web site shows up for key search results. We interviewed Rand Fishkin of Moz (the market leader in SEO software) to talk about best practices for getting traction with SEO. Patrick McKenzie, founder of Appointment Reminder, also explained to us how he uses SEO to cheaply acquire lots of highly targeted traffic. Content Marketing

Many startups have blogs. However, most don’t use their blogs to get traction. We talked with Unbounce founder Rick Perreault and OkCupid cofounder Sam Yagan to learn how their blogs transformed their businesses. Email Marketing

Email marketing is one of the best ways to convert prospects while retaining and monetizing existing customers. For this chapter we interviewed Colin Nederkoorn, founder of email marketing startup Customer.io, to discuss how startups can get the most out of this traction channel. Engineering as Marketing

Using engineering resources to acquire customers is a significantly underutilized way to get traction. Successful companies have built microsites, developed widgets, and created free tools that drive thousands of leads each month. We asked Dharmesh Shah, founder of HubSpot, to discuss how engineering as marketing has driven HubSpot’s growth to tens of thousands of customers through tools like its Marketing Grader. Viral Marketing

Viral marketing consists of growing your customer base by encouraging your customers to refer other customers. We interviewed Andrew Chen, a viral marketing expert and mentor at 500 Startups, for common viral techniques and the factors that have led to viral adoption in major startups. We also talked with Ashish Kundra of myZamana, who discussed using viral marketing to grow from 100,000 users to more than 4 million in less than a year. Business Development

Business development (BD) is the process of creating strategic relationships that benefit both your startup and your partner. Paul English, cofounder and CEO of Kayak.com, walked us through the impact of Kayak’s early partnership with AOL. We also interviewed venture capitalist Chris Fralic, whose BD efforts at Half.com were a major factor in eBay’s $350 million acquisition of the company. We’ll show you how to structure deals, find strategic partners, build a business development pipeline, and approach potential partners. Sales

Sales is focused primarily on creating processes to directly exchange product for dollars. We interviewed David Skok of Matrix Partners—someone who’s taken four different companies public—to get his perspective on how the best software companies are creating sustainable, scalable sales processes. We also take a look at how to find early customers and have winning sales conversations. 


Affiliate Programs

Companies like HostGator, GoDaddy, and Sprout Social have robust affiliate programs that have allowed them to reach hundreds of thousands of customers in a cost-effective way. Weinterviewed Kristopher Jones, founder of the Pepperjam affiliate network, to learn how a startup can leverage this channel. We also talked with Maneesh Sethi to learn how affiliate marketers choose which products to promote, and some of the strategies they use to do so. Existing Platforms

Focusing on existing platforms means focusing your growth efforts on a megaplatform like Facebook, Twitter, or the App Store, and getting some of their hundreds of millions of users to use your product. Alex Pachikov, on the founding team of Evernote, explained how their focus on Apple’s App Store generated millions of customers. Trade Shows

Trade shows are a chance for companies in specific industries to show off their latest products. We interviewed Brian Riley of SureStop, an innovative bike brake startup, to learn how it sealed a partnership that led to more than twenty thousand sales from one trade show and its approach to getting traction at each event. Offline Events

Sponsoring or running offline events—from small meetups to large conferences—can be a primary way to get traction. We spoke with Rob Walling, founder and organizer of MicroConf, to talk about how to run a fantastic event. 

Speaking Engagements

Eric Ries, author of the bestselling book The Lean Startup, told us how he used speaking engagements to hit the bestseller list within a week of his book’s launch. We also interviewed Dan Martell, founder of Clarity, to learn how to leverage a speaking event, give an awesome talk, and grow your startup’s profile at such speaking gigs. 


Community Building

Companies like Wikipedia and Stack Exchange have grown by forming passionate communities around their products. In our

interview with Jeff Atwood of Stack Exchange, he detailed how he built the Stack Overflow community, which has created the largest repository of useful programming questions and answers in history. After reading this book, you will appreciate how each of these nineteen traction channels could get traction for your business. You will be equipped with the framework to find out which one to focus on, and how to go about doing so.

CHAPTER TWO

Traction Thinking

How much time should you spend on getting traction? When should you start? How do you know if it’s working? How much traction do you need to get investors? This chapter answers these and other general traction questions, empowering you with the traction thinking that will set you up for success.

THE 50 PERCENT RULE If you’re starting a company, chances are you can build a product. Almost every failed startup has a product. What failed startups don’t have is enough customers. Marc Andreessen, cofounder of Netscape and VC firm Andreessen Horowitz, sums up this common problem: The number one reason that we pass on entrepreneurs we’d otherwise like to back is they’re focusing on product to the exclusion of everything else. Many entrepreneurs who build great products simply don’t have a good distribution strategy. Even worse is when they insist that they don’t need one, or call [their] no distribution strategy a “viral marketing strategy.” A common story goes like this: Founders build something people want by spending their time making tweaks based on what early customers say they want. Then, when they think they are ready, they launch and take stabs at getting more customers, only to become frustrated when customers aren’t flocking to them. Having a product or service that your early customers love, but having no clear way to get more traction is a major problem. 

To solve this problem, spend your time constructing your product or service and testing traction channels in parallel. Traction and product development are of equal importance and should each get about half of your attention. This is what we call the 50 percent rule: spend 50 percent of your time on product and 50 percent on traction. Building something people want is certainly required for traction, but it isn’t enough. There are four common situations where you could build something people want, but still not end up with a viable business. First, you could build something people want, but for which you just can’t figure out a viable business model. The money isn’t adding up. 

For example, people won’t pay, and selling advertising won’t cover the bills. There is just no real market. Second, you could build something people want, but there are just not enough customers to reach profitability. It’s just too small a market, and there aren’t obvious ways to expand. This occurs often when startups aren’t ambitious enough and pick too narrow a niche. Third, you could build something people want, but reaching them is cost prohibitive. You find yourself in a hard-to-reach market. An example is a relatively inexpensive product that requires a direct sales force to sell it. 

That combo just doesn’t work. Finally, you could build something people want, but a lot of other companies build it too. In this situation you are in a hypercompetitive market where it is simply too hard to get customers. If you follow the 50 percent rule from the beginning, then you will have the best chance of avoiding these traps. If you don’t, then you risk realizing you’re in one of these traps too late to do anything useful. Unfortunately this happens to a lot of companies postlaunch. The sad thing is that often these products and services are useful, but the companies die because they don’t have a good distribution strategy. The flip side is that if you focus on traction from the beginning, then you can figure out very quickly if you’re on the right track. The results from your traction experiments will guide you around these traps and toward the traction channel that will drive the most meaningful growth. This 50 percent rule is hard to follow because the pull to spend all of your attention on product is strong. After all, you probably got into your startup because you wanted to build a particular product or service. You had a vision. 

A lot of the traction activities are unknown and outside of both your comfort zone and this initial vision. That’s why there is a natural tendency to avoid them. Don’t. To be clear, splitting your time evenly between product and traction will certainly slow down product development. However, it counterintuitively won’t slow the time to get your product successfully to market. In fact, it will speed it up! That’s because pursuing product development and traction in parallel has a couple of key benefits. First, it helps you build the right product because you can incorporate knowledge from your traction efforts. If you’re following a good product development process, you’re already getting good feedback from early customers. However, these customers are generally too close to you. They often tell you what you want to hear. Through traction development you get a steady stream of cold customers. It is through these people that you can really find out whether the market is taking to your product or not, and if not, what features are missing or which parts of the experience are broken. You can think of your initial investment in traction as pouring water into a leaky bucket. At first your bucket will be very leaky because your product is not yet a full solution to customer needs and problems. In other words, your product is not as sticky as it could be, and many customers will not want to engage with it yet. As a consequence, much of the money you are spending on traction will leak out of your bucket. This is exactly where most founders go wrong. 

They think because this money is leaking out that it is money wasted. Oppositely, this process is telling you where the real leaks are in your bucket (product). If you don’t interact with cold customers in this way, then you generally spend time on the wrong things in terms of product development. These interactions also get you additional data, like what messaging is resonating with potential customers, what niche you might focus on first, what types of customers will be easiest to acquire, and what major distribution roadblocks you might run into. You will get some of this information through good product development practices, but not nearly enough. All of this new information should change the first version of the product for the better and inform your distribution strategy. This is exactly what happened with Dropbox. While developing their product, they tested search engine marketing and found it wouldn’t work for their business. They were acquiring customers for $230 when their product cost only $99. That’s when they focused on the viral marketing traction channel, and built a referral program right into their product. This program has since been their biggest growth driver. In contrast, waiting until you launch a product to embark on traction development usually results in one or more additional product development cycles as you adjust to real market feedback. That’s why doing traction and product development in parallel may slow down product development in the short run, but in the long run it’s the opposite. The second key benefit to parallel product and traction development is that you get to experiment and test different traction channels before you launch anything. This means when your product is ready, you can grow rapidly. A head start on understanding the traction channel that will work for your business is invaluable. 

Phil Fernandez, founder and CEO of Marketo, a marketing automation company that IPO’d in 2013, talks about this benefit: At Marketo, not only did we have SEO [search engine optimization] in place even before product development, we also had a blog. We talked about the problems we aimed to solve. . . . Instead of beta testing a product, we beta tested an idea and integrated the feedback we received from our readers early on in our product development process. By using this content strategy, we at Marketo began drumming up interest in our solutions with so much advance notice we had a pipeline of more than fourteen thousand interested buyers when the product came to market. Marketo wouldn’t have had fourteen thousand interested buyers if they just focused on product development. It’s the difference between significant customer growth on day one—real traction—and just a product you know some people want.

MOVING THE NEEDLE Before you can set about getting traction, you have to define what traction means for your company. You need to set a traction goal. At the earliest stages, this traction goal is usually to get enough traction to either raise funding or becomeprofitable. In any case, you should figure out what this goal means in terms of hard numbers. How many customers do you need and at what growth rate? Your traction strategy should always be focused on moving the needle for your traction goal. By moving the needle, we mean focusing on marketing activities that result in a measurable, significant impact on your traction goal. It should be something that advances your user acquisition goal in a meaningful way, not something that would be just a blip even if it worked. For example, early on DuckDuckGo focused on search engine optimization to get in front of users searching for “new search engine.” This focus was successful at obtaining users, but did not bring in enough users to get close to the traction goal. It didn’t move the needle. From the perspective of getting traction, you can think about working on a product or service in three phases: Phase I—making something people want Phase II—marketing something people want Phase III—scaling your business In the leaky bucket metaphor, phase I is when your bucket (product) has the most leaks. It really doesn’t hold water. There is no reason to scale up your efforts now, but it is still important to send a small amount of water through the bucket so you can see where the holes are and plug them. When you constantly test traction channels by sending through a steady stream of new customers, you can tell if your product is getting less leaky over time, which it should be if your product development strategy is sound. In fact this is a great feedback loop between traction development and product development that you can use to make sure you’re on the right track. As you hone your product, you are effectively plugging leaks. Once you have crossed over to phase II, you have product-market fit and customers are sticking around. Now is the time to scale up your traction efforts: your bucket is no longer leaky. You are now fine-tuning your positioning and marketing messages. In phase III, you have an established business model and significant position in the market, and are focused on scaling both to further dominate the market and to profit. In each phase you will find yourself generally focused on different things because moving the needle means different things as you grow. 

In phase I, it’s getting those first customers that prove your product can get traction. In phase II, it is getting enough customers that you’re knocking on the door of sustainability. And in phase III, your focus is on increasing your earnings, scaling your marketing channels, and creating a truly sustainable business. Phase I is very product focused and involves pursuing initial traction while also building your initial product. This often means getting traction in ways that don’t scale—giving talks, writing guest posts, emailing people you have relationships with, attending conferences, and doing whatever you can to get in front of customers. As Paul Graham said in his essay “Do Things That Don’t Scale”: A lot of would-be founders believe that startups either take off or don’t. You build something, make it available, and if you’ve made a better mousetrap, people beat a path to your door as promised. Or they don’t, in which case the market must not exist. Actually startups take off because the founders make them take off. . . . The most common unscalable thing founders have to do at the start is to recruit users manually. Nearly all startups have to. You can’t wait for users to come to you. You have to go out and get them. Startup growth happens in spurts. Initially, growth is usually slow. Then it spikes as a useful traction channel strategy is unlocked. Eventually it flattens out again as this strategy gets saturated and becomes less effective. Then you unlock another strategy and you get another spike. As your company grows, smaller traction strategies stop moving the needle. If you have ten thousand visitors to your Web site each day, it will be hard to appreciate a tweet or blog post that sends twenty visitors your way. Moving the needle in the later stages requires larger and larger numbers. If you want to add 100,000 new customers, with conversion rates between 1 and 5 percent, you’re looking at reaching 2 to 10 million people in a targeted marketing campaign—those are huge numbers! That’s why traction channels like community building and viral marketing can be so powerful: they scale with the size of your user base and potential market. In any case, always consider your traction efforts in terms of whether they are moving the needle for your traction goal.

HOW MUCH TRACTION IS ENOUGH FOR INVESTORS? Startup founders hoping to scale quickly tend to focus on fund-raising. Not every company starts off planning on an eventual IPO, but any that do need outsiders buying in. As a result, they often wonder how much traction they need to get investors interested. Naval Ravikant, founder of AngelList, answered this question well a few years ago: It is a moving target. The entire ecosystem is getting far more efficient. Companies are accomplishing a lot more with a lot less. Two years ago [November 2010] you could have gotten your daily deal startup funded pre-traction. Eighteen months ago you could not have gotten a daily deal startup funded no matter how much traction you had. Twelve months ago you could have gotten your mobile app company funded with ten thousand downloads. Today it’s probably going to take a few hundred thousand downloads and a strong rapid adoption rate for a real financing to take place. The definition of traction keeps changing as the environment gets competitive. That’s why it is actually useful to look at AngelList and look at companies who just got funded; that will give you an idea of where the bar is right now. When pursuing funding, first contact individuals who intimately understand what you’re working on (perhaps because they have worked on or invested in something similar before). The better your prospective investors understand what you’re doing, the less traction they will need to see before they invest because they are more likely to extrapolate your little traction and believe it could grow into something big. On the other hand, those investors who have little real-world experience within your industry may find it hard to extrapolate and may demand more traction initially before they invest. An outlier is friends and family, who may not need to see any traction before investing, because they’re investing in you personally. 

It is easy to get discouraged when you are fund-raising because you can get so many rejections. However, you shouldn’t take rejection as a rejection of your idea. There are many reasons why investors may say no that are simply beyond your control (investment goals, timing, expertise, etc.). Sustainable product engagement growth (i.e., more customers getting engaged over time) is hard for any investor to ignore. This is true even if your absolute numbers are relatively small. So if you have only a hundred customers, but have been growing 10 percent a month for six months, that’s attractive to investors. With sustainable growth, you look like a good bet to succeed in the long run. With investing, always remember that traction trumps everything.

TO PIVOT OR NOT TO PIVOT You may come to a point where you are simply unhappy with your traction. You may not be able to raise funding or you may just feel like things aren’t taking off the way they should. How do you know when to “pivot” from what you’re doing? We strongly believe that many startups give up way too early. A lot of startup success hinges on choosing a great market at the right time. Consider DuckDuckGo, the search engine startup that Gabriel founded. Other search startups gave up after two years: Gabriel has been at it for more than seven. Privacy has been a core differentiator for DuckDuckGo (it does not track you) since 2009 but didn’t become a mainstream issue until the NSA leaks in 2013. Growth was steady before 2013, but exploded when privacy became an item on the national consciousness. It’s important to wrap your head around this timescale. If you are just starting out, are you ready to potentially do this for the next decade? In retrospect, a lot of founders feel they picked their company idea too quickly, and they would have picked something they were more passionate about if they had realized it was such a long haul. A startup can be awesome if you believe in it: if not, it can get old quickly. If you are considering a pivot, the first thing to look for is evidence of real product engagement, even if it is only a few dedicated customers. If you have such engagement, you might be giving up too soon. You should examine these bright spots to see how they might be expanded. Why do these customers take to your product so well? Is there some thread that unites them? Are they early adopters in a huge market or are they outliers? The answers to these questions may reveal some promise that is not immediately evident in your core metrics.

Another factor to consider before you pivot: startup founders are usually forward thinking and as a result are often too early to market, which is another reason why it’s important to choose a startup idea you’re willing to stick with for many years. Granted, there is a big difference between being a few years too early and a decade too early. Hardly anyone can stick around for ten years with middling results. But being a year or two early can be a great thing. You can use this time to improve and refine your product. Then, when the market takes off, you have a head start on competitors just entering your space. How can you tell whether you are just a bit early to market and should keep plugging away? Again, the best way to find out is by looking for evidence of product engagement. If you are a little early to a market there should be some early adopters out there already eating up what you have to offer.

TARGETS Put half your efforts into getting traction. Pursue traction and product development in parallel, and spend equal time on both. Think of your product as a leaky bucket. Your early traction efforts are pointing you toward the holes worth plugging. Set your growth goals. Focus on strategies and tactics that can plausibly move the needle for your company. Get some hard numbers. Learn what growth numbers potential investors respect. How much traction is needed for investors is a moving target, but a sustainable customer growth rate is hard for investors to ignore. Potential investors who understand your business are likely to appreciate your traction and thus invest earlier. Traction trumps everything. Find your bright spots. If you’re not seeing the traction you want, look for bright spots in your customer base, pockets of customers who are truly engaged with your product. See if you can figure out why it works for them and if you can expand from that base. If there are no bright spots, it may be a good time to pivot.

CHAPTER THREE

Bullseye

With nineteen traction channels to consider, figuring out which one to focus on is tough. That’s why we’ve created a simple framework called Bullseye that will help you find the channel that will get you traction. As billionaire PayPal founder and early Facebook investor Peter Thiel put it: [You] probably won’t have a bunch of equally good distribution strategies. Engineers frequently fall victim to this because they do not understand distribution. Since they don’t know what works, and haven’t thought about it, they try some sales, BD, advertising, and viral marketing—everything but the kitchen sink. That is a really bad idea. It is very likely that one channel is optimal. Most businesses actually get zero distribution channels to work. Poor distribution—not product—is the number one cause of failure. If you can get even a single distribution channel to work, you have great business. If you try for several but don’t nail one, you’re finished. So it’s worth thinking really hard about finding the single best distribution channel. We use the name Bullseye for our three-step framework because you’re aiming for the Bullseye—the one traction channel at the center of the target that will unlock your next growth stage.

THE OUTER RING: WHAT’S POSSIBLE The first step in Bullseye is brainstorming every single traction channel. If you were to advertise offline, where would be the best place to do it? If you were to give a speech, who would be the ideal audience? Imagine what success would look like in each channel, and write it down in your outer ring. Everyone starts off with biases. The outer ring is meant to help you systematically counteract your traction channel biases. It is important that you not dismiss any traction channel in this step. You should be able to think of at least one idea for every channel. In practice, a lot of founders mess up this step by not brainstorming long and deep enough to get useful ideas for each channel. For each channel, you should identify one decent channel strategy that has a chance of moving the needle. For example, social ads is a traction channel. Specifically running ads on reddit, Twitter, or Facebook is a channel strategy within social ads. Through brainstorming, identify the best channel strategy you can think of in each of the nineteen traction channels. In terms of research to feed your brainstorm, this book is a good start, but you should get much more specific to your company. You should know what marketing strategies have worked in your industry, as well as the history of companies in your space. It’s especially important to understand how similar companies acquired customers over time, and how unsuccessful companies wasted their marketing dollars.

THE MIDDLE RING: WHAT’S PROBABLE The second step in Bullseye is running cheap traction tests in the channels that seem most promising. Go around your outer ring and promote your best traction channel ideas to your middle ring. It is often the case that there are a few truly exciting and promising channel ideas in your outer ring. Stop promoting ideas where there is an obvious drop-off in excitement. That drop-off often occurs around the third channel. We want you to have more than one channel in your middle ring because we don’t want you to waste valuable time testing channels sequentially when you can do so equally well in parallel. You can run multiple experiments at the same time because tests take some time to run after they’ve been set up. Yet doing too many things in parallel leads to errors from lack of focus, which means the number needs to be somewhat low. For each traction channel in your middle ring, now construct a cheap traction test you can run to determine if the idea really is good or not. These tests should be designed to roughly answer the following three questions:

1. How much will it cost to acquire customers through this channel? 2. How many customers are available through this channel? 3. Are the customers that you are getting through this channel the kind of customers that you want right now? There isn’t a single method for testing each traction channel because every business is different. We will cover tactics for organizing and thinking about these tests in the next chapter. You should also get specific ideas for testing each traction channel throughout the rest of the book. Some founders mess up this step by prematurely scaling their marketing efforts. Keep in mind that, when testing, you are not trying to get a lot of traction with a channel just yet. Instead, you are simply trying to determine if it’s a channel that could move the needle for your startup. Your main consideration at this point is speed—to get data and to prove your assumptions. You want to design smaller-scale tests that don’t require much up-front cost or effort. For example, run four Facebook ads versus forty. You should be able to get a rough idea of a channel’s effectiveness with at most a thousand dollars and a month of time. Often, it will be cheaper and shorter.

THE INNER RING: WHAT’S WORKING The third and final step in Bullseye is to focus solely on the channel that will move the needle for your startup: your core channel. If all went well, one of the traction channels you tested in your middle ring produced promising results. In that case, you should start directing all your traction efforts and resources toward this most promising channel. You hit the Bullseye! You’ve found your core channel. At any stage in a startup’s life cycle, one traction channel dominates in terms of customer acquisition. That is why we suggest focusing on one at a time, but only after you’ve identified a channel that seems like it could actually work. The goal of this focusing step is quite simple: to wring every bit of traction out of your core channel. To do so, you will be continually experimenting to find out exactly how to optimize growth in this traction channel. As you dive deeper into it, you will uncover effective tactics and do everything you can to scale them until they are no longer effective due to saturation or rising costs.

The way this step gets most often messed up by founders is by keeping around distracting marketing efforts in other traction channels. For example, suppose you ran tests in three traction channels: search engine marketing, trade shows, and publicity. Search engine marketing was the most promising, and so you decide to focus on it and make it your core channel. However, your trade show and publicity tests were also successful, albeit much less so. There is a natural tendency to do more trade shows and publicity because you know they will somewhat work. This is a mistake. Search engine marketing was significantly better, and so you should spend all your efforts on this core channel because uncovering additional strategies and tactics within it will have a greater effect than using these secondary channels. They’re distracting. This is additionally confusing because oftentimes focusing on your core channel involves channel strategies that utilize other traction channels. One channel is still dominant, but others feed into it. For example, you will see that a focus on search engine optimization (SEO) requires getting links to your site, and a good tactic for getting links is getting publicity (another traction channel). Similarly, viral marketing is often built on email marketing or existing platforms like Facebook (two other traction channels). Yet in both these situations one channel is dominant in that it is your core traction strategy. You’re using these other channels to support that strategy, as opposed to pursuing multiple traction strategies at once. If, unfortunately, no channel seems promising after testing, the whole process should be repeated. The good news is you now have data from all the tests you just did, which will inform you as to what types of things are, and are not, resonating with customers. Look at the messaging you’ve been using, or dig deeper to see at what point each channel failed to deliver customers. If you go through the process several times and no traction channel seems promising, then your product may require more tweaking. Your bucket is still too leaky.

WHY USE BULLSEYE? Bullseye is designed to be a straightforward way to direct your traction focus and maximize your results. First and foremost, it forces you to take all the traction channels more seriously than you would otherwise. These steps systematically uncover strategies for getting traction that you wouldn’t have found using other approaches.

When we looked at companies really taking off, they were usually employing underutilized channels and channel strategies. If everyone in your industry uses social ads to grow, you might be better off using another channel. However, in this scenario social ads are probably what you know best because that is what everyone is using. Let Bullseye help you break out of your comfort zone and try channels that are unfamiliar because they may be the key to your growth. Bullseye is also meant to help you zoom in on the best ideas as quickly and cheaply as possible, while still casting a wide net. The traction channel that will ultimately succeed is unpredictable, and time is of the essence. That’s why we focus on successive rounds of quick parallel tests. It’s simple and it works. Noah Kagan talked to us about how he used a version of Bullseye at Mint, a site that helps you track your finances and was acquired by Intuit for $170 million. Its initial traction goal was 100,000 customers in the first six months after launch. Noah and his team brainstormed and picked several traction channels that seemed promising (targeting blogs, publicity, search engine marketing). Then they ran a series of cheap tests in each (sponsored a small newsletter, contacted financial celebrities like Suze Orman, placed some Google ads) to see what worked and what didn’t. Noah kept track of the test results in this spreadsheet:

After running these experiments, Mint focused on the traction channel that seemed most promising and that could move the needle for its traction goal. In this case, that meant targeting blogs was its core channel. In the early days, the channel strategies of sponsoring mid-level bloggers in the financial niche and guest posting allowed Mint to acquire its first forty thousand customers. When this channel maxed out and stopped moving the needle, Mint repeated the Bullseye process, and found a new core traction channel to focus on: publicity. Within six months of launching, it had 1 million users. We heard stories like this over and over again when talking to successful startup founders. They would research many channels, try a few in parallel, and focus on the most promising until it stopped working. Bullseye is designed to systemize this successful process. Use it!

COMPARISON TO LEAN Many good product development methodologies exist, but they don’t deal explicitly with getting traction. The Lean Startup framework is a popular one. This approach involves creating testable hypotheses regarding your product, and then going out and validating (or invalidating) those hypotheses. It’s an approach that demands a great deal of interaction with customers, discovering their needs and understanding the types of features they require. Bullseye works hand in hand with Lean or with any other product development framework. What Lean is to product development, Bullseye is to traction. The biggest mistake startups make when trying to get traction is failing to pursue traction in parallel with product development. Many entrepreneurs think that if you build a killer product, your customers will beat a path to your door. This line of thinking is a fallacy: that the best use of your time is always improving your product. In other words, “if you build it, they will come” is wrong. You are much more likely to develop a good distribution strategy with a good traction development methodology (like Bullseye) the same way you are much more likely to develop a good product with a good product development methodology (like Lean). Both help address major risks that face early-stage companies: market risk (that you can reach customers in a sustainable way) and product risk (that customers want what you’re building). Pursuing both traction and product in parallel will increase your chances of success by both developing a product for which you can actually get traction and getting traction with that product much sooner.

TARGETS Work through Bullseye. Maximize your chances of getting traction: brainstorm, prioritize, test, and then focus. Do not overlook underutilized channels. In fact, those channels are more likely to be the ones that will work best. Talk to founders a few steps ahead of you. Research how past and present companies in your space and adjacent spaces succeeded or failed at getting traction. The easiest way to do this

is to go talk to startup founders who previously failed at what you’re trying to do. Hold on to your other channel ideas. Compile your brainstorming ideas for each traction channel in a spreadsheet with educated guesses that you can confirm through testing. Even after you’ve chosen your core channel, you should keep these ideas around for future runs of Bullseye.

CHAPTER FOUR

Traction Testing

Continuous testing is the key to getting traction with Bullseye. When searching for a traction channel to focus on, you’re testing the channels in your middle ring to see which is the most promising. Then, when you find one worth your undivided attention, you test strategies and tactics within that core channel to wring the most traction from it. In this chapter, we cover how to approach testing.

MIDDLE RING TESTS The goal of middle ring tests is to find a promising channel strategy to focus on. A channel strategy is a particular way to acquire customers within a channel. For example, offline ads is a traction channel, and billboards, transit ads, and magazine ads are all channel strategies within offline ads. When you’re just starting out testing a channel, you will pick one channel strategy to pursue—the most promising one you came up with when brainstorming. In particular, your tests should be designed to answer these questions: 1. How much does it cost to acquire each customer through this channel strategy? 2. How many customers are available through this channel strategy? 3. Are the customers you are getting through this channel the ones you want right now? With limited resources, it’s almost impossible to optimize multiple channel strategies at once. Running ten social ads and testing everything about them (ad copy, landing pages, etc.) is a full-time endeavor. That is optimization, not testing.

Rather, you should be running several cheap tests (perhaps two social ads with two landing pages) that give some indication of how successful a given channel strategy could be. In other words, you should not be getting too deep into tactics at this stage; stick to the strategy level. These first channel strategy tests are often very cheap and short. For instance, if you spend just $250 on AdWords, you’ll get a rough idea of how well the search engine marketing channel works for your business. In general in phase I, you shouldn’t be spending more than a thousand dollars and a month’s time on a middle ring test, and often significantly less. When you’re further along in phases II and III, channel tests may be bigger and longer because you need bigger numbers to move the needle for your traction goal. Middle ring tests arm you with data that you can use to compare channel strategies. If all goes well, you hit the Bullseye and can move on to inner ring testing.

INNER RING TESTS Inner ring tests are designed to do two things. First, to optimize your chosen channel strategy to make it the best it can be. Second, to uncover better channel strategies within this traction channel. Really focusing on a traction channel takes significant time and resources. This time is valuable and should be used only after you have some indication that the chosen channel will likely work. You should have an idea it will likely work because this channel strategy emerged from middle ring testing. In terms of optimization, each channel strategy has a set of things you can tweak. For example, for targeting blogs you can tweak which blogs to target, what type of content to push, and what the call to action is in this content. For search engine marketing, you can tweak keywords, ad copy, demographics, and landing pages. You should be continually testing your chosen channel strategy in an effort to increase its effectiveness. These tests should be scientific so you have confidence you are tweaking things in the right direction. A common approach is to use some form of A/B testing (also known as split testing). In its most basic form, A/B testing is a science experiment with a control group (A) and an experimental group (B). 

A/B testing is often called split testing because for the best results you split people randomly into one of the two groups, and then measure what they do. The purpose of an A/B test is to measure the effectiveness of a change in one or more variables—a button color, an ad image, or a different message on one of your Web pages. You create one version of your page for your control group and a second version for your experimental group. As you track how each page performs, you can find out whether your changes are having an impact on a key metric like signups. If, after a period of time, the experimental group performs significantly better, you can apply the change, reap the benefits, and run another test. Making A/B testing a habit (even if you run just one test a week) will improve your efficiency in a traction channel by two or three times. There are many tools to help you do this type of testing online, such as Optimizely, Visual Website Optimizer, and Unbounce. These tools allow you to test optimizations without making complex changes to your code. 

In addition to optimization testing, you should also be testing additional channel strategies within your core channel. These resemble middle ring tests in that they should initially be cheap and fast and answer the same basic questions as middle ring tests. The goal here is to see if there is a better channel strategy you should be using within your core channel. Once you truly focus on a channel, you will become an expert in it. Channel experts uncover new channel strategies and tactics, which are often the best ones simply because they are new. That should be your goal with additional channel strategy testing. Andrew Chen, a startup adviser on growth, coined the Law of Shitty ClickThroughs: “Over time, all marketing strategies result in shitty click-through rates.” (“Click-through rate” refers to the response rate of a marketing campaign.) What this means is that over time, all marketing channels become saturated. As more companies discover an effective strategy, it becomes crowded and expensive or ignored by consumers, thus becoming much less effective. When banner ads first debuted, they were receiving click-through rates of over 75 percent! Once they became commonplace, click-through rates plummeted. This happens with all channel strategies. Tactics that once worked well will become crowded and ineffective. All it takes is one other competitor seriously pursuing traction in the same way to drive up its cost and drive down its efficacy.

It is likely that your first channel strategy ideas are commonplace and have already succumbed somewhat to the Law of Shitty Click-Throughs. To combat this reality you should consistently brainstorm new channel strategies and conduct small experiments. Constantly running small traction tests will allow you to stay ahead of competitors pursuing the same channels. As Andrew puts it: The . . . solution to solving the Law of Shitty Click-Throughs, even momentarily, is to discover the next untapped marketing [strategy]. . . . If you can make these [strategies] work with a strong product behind [them], then great. Chances are, you’ll enjoy a few months if not a few years of strong marketing performance before they too slowly succumb. An untapped channel strategy may mean trying something different in an established venue, but it also could mean trying a venue no one else is using. For example, you might be able to take advantage of new marketing platforms while they are still in their infancy. Zynga (the maker of FarmVille and other games) did this with Facebook, dominating its advertising and sharing features when there was relatively little competition. For a gaming company today, it’s basically impossible to leverage Facebook to grow the way Zynga did just a few years ago—it’s just too expensive and too crowded. However, the company that leverages a newer platform that’s growing quickly will have a significant advantage over companies chasing the same old methods. Another place to look for underutilized channel strategies is in using other traction channels to feed into your core traction channel. As we discussed previously, you do not want to focus separately on multiple traction channels. However, you can utilize other channels as part of your core channel strategy. For example, suppose your core channel is content marketing, centered around your company blog. To jump-start your blog you may target other blogs for guest posts (in the targeting blogs channel). 

You might also buy social ads to amplify your best posts on Twitter and Facebook (in the social and display ads channel). In both these cases, you are not solely focusing on these secondary channels to get growth; you are using them to feed into your content marketing strategy. Now, both of those examples are pretty standard. What if you could find a way to use community building, speaking engagements, or offline ads to jump-start your content marketing strategy? Once you have a core traction channel, it is often instructive to brainstorm the other eighteen traction channels in terms of how you might use them to support your core channel. Doing so could uncover some truly novel channel strategies that haven’t yet succumbed to the Law of Shitty Click-Throughs.

ONLINE TOOLS As testing is central to getting traction, you should seek online tools to help you organize and execute your tests, even if they are offline tests. Sean Ellis, growth adviser to Dropbox and Eventbrite, had this to say about this approach: The faster you run high-quality experiments, the more likely you’ll find scalable, effective growth tactics. Determining the success of a customer acquisition idea is dependent on an effective tracking and reporting system, so don’t start testing until your tracking/reporting system has been implemented. This “effective tracking and reporting system” can be as simple as a spreadsheet or as complex as an analytics tool that does cohort analysis, but it must exist. Furthermore, each test you run should have a point—to validate or invalidate specific assumptions you specify ahead of time. Every day more online tools come on the market to help you optimize traction channels. We highly recommend embracing the use of online tools to help you understand and assess the efficacy of all your traction efforts. For example, the questions below seem like they are difficult or might require a lot of research to answer: How many prospective customers landed on my Web site? What are the demographics of my best and worst customers? Are customers who interact with my support team more likely to stay customers longer? However, they are quite straightforward if you’re using the right online tools. In fact, a basic analytics tool like Clicky, Mixpanel, or Chartbeat can help you answer all three of these questions. 

These tools tell you who is coming to your site, at what frequency, and, perhaps most important, when and where they are leaving your site. We recommend using a spreadsheet to help you rank and prioritize your traction channel strategies. The questions you are answering from tests all have numeric answers, and so a spreadsheet is a natural tool to use. At a minimum, include the columns of how many customers are available, conversion rate, cost to acquire a customer, and lifetime value of a customer for a given strategy. Because these metrics are universal, you can use them to easily make comparisons across strategies. In general, we encourage you to be as quantitative as possible, even if it is just guesstimating at first. As we mentioned earlier, you should be thinking about only those traction channels and specific strategies that have a chance of moving the needle for your traction goal. You can assess what can move the needle with some simple calculations. How many new customers do you need to really move the needle? If there is no reasonable chance that a channel strategy could yield enough new customers to move the needle within your current budget, then it is not worth exploring further. For example, focusing on getting articles on tech news sites doesn’t make sense right now for DuckDuckGo because they couldn’t possibly convert enough people to make a significant difference in the company’s search numbers. However, this strategy did work in phase I. Most channels will yield you some customers, and so they are all tempting to some degree. The operative question then is, “Does this channel have enough customers to be meaningful?” A simple spreadsheet calculation can go a long way!

TARGETS Look for customers where others aren’t looking. Keep a lookout for the cutting-edge tactics that haven’t yet succumbed to the Law of Shitty Click-Throughs. Run cheap tests to quickly validate assumptions and test new ideas. Constantly optimize. You should consistently run A/B tests in your efforts to optimize a traction channel strategy. There are many online tools that can help you test more easily and evaluate your use of various traction strategies and tactics.

Keep it numerical. Look for ways to quantify your marketing efforts, especially when deciding which traction strategies to pursue and comparing them within Bullseye. You should have an idea at all times of what numbers it will take to move the needle, and focus your traction efforts only on strategies that could possibly do so.

CHAPTER FIVE

Critical Path

Startups get pulled in a lot of different directions. There are always opportunities in front of you or on the horizon that you could focus on. There are always product or service revisions you could work on. There are always background tasks nagging at you. How do you decide what to work on?

DEFINING YOUR TRACTION GOAL You should always have an explicit traction goal you’re working toward. This could be one thousand paying customers, one hundred new daily customers, or 10 percent of your market. As we say, traction trumps everything. Because of that, what you choose to focus on should relate directly back to your traction goal. The right goal is highly dependent on your business. It should be chosen carefully and align with your company strategy. You want a goal where hitting the mark would change things significantly for your company’s outcome. Perhaps you’d be profitable, be able to raise money more easily, or become the market leader. At DuckDuckGo the current traction goal is 1 percent of the general search market. Achieving that goal is meaningful because at that point the company will be taken much more seriously as an entrenched part of the market and everything that comes with that recognition (better deals, publicity, etc.). This traction goal wouldn’t work well for most other companies because usually 1 percent of a well-defined market is not that significant or valuable. It works in the general search engine space because the market is so big and there are so few companies in it. This speaks to the importance of setting a traction goal that is particularly significant for your company.

Before this traction goal, DuckDuckGo had a traction goal of 100 million searches a month, which took it to about a break-even point. Getting to breakeven was the company significance for that traction goal. Before that the goal was to get the product and messaging to a point where people were switching to DuckDuckGo as their primary search engine. The company significance there was to move from phase I to II and get true product/market fit. The importance of choosing the right traction goal cannot be overstated. Are you going for growth or profitability, or something in between? If you need to raise money in X months, what traction do you need to show to do so? These are the types of questions that help you determine the right traction goal. Once that is defined, you can work backward and set clear quantitative and time-based traction subgoals, such as reaching one thousand customers by next quarter or hitting 20 percent monthly growth targets. Clear subgoals provide accountability. By placing traction activities on the same calendar as product development and other company milestones, you ensure that enough of your time will be spent on traction. We hope it is at least half of your time!

DEFINING YOUR CRITICAL PATH The path to reaching your traction goal with the fewest number of steps is your Critical Path. You should literally enumerate the intermediate steps (milestones) to get to your traction goal. These milestones need not be traction related, but they should be absolutely necessary to reach your goal. In DuckDuckGo’s case, the traction goal was to get to 100 million searches a month. The team believed the milestones they needed to hit included a faster site, a more compelling mobile offering, and more broadcast TV coverage (from the publicity traction channel). Even though product features like images and auto-suggest were continually requested, they believed they were not absolutely necessary milestones in their Critical Path to reach that traction goal. However, now that they are on the traction goal to get to 1 percent of the search market, these product features are believed to be necessary milestones on this new Critical Path. The reasoning for why these particular features weren’t necessary initially was that even at 100 million searches a month, DuckDuckGo’s user base was motivated enough by other features to be forgiving of missing these particular ones. 

However, to get to the next traction goal the company had to get more mainstream adoption, and this next set of users is much less forgiving. In your company, your milestones will be different, but the point is to be critical and strategic in deciding what to include. That’s why it is called the Critical Path. For example, you may think that to reach your traction goal you will need to hire three people, add features A, B, and C to your product, and engage in marketing activities X, Y, and Z. These are the milestones you need to do to get where you want to go. Do you really need feature C or marketing activity Y? This is where founders often mess up: by focusing their limited company resources on things off Critical Path. You are generally competing with companies with significantly more resources than you. You cannot afford to waste what little resources you have. Another issue is that your original enumeration of necessary milestones is often wrong. For example, you thought you had to build features A, B, and C to get to your traction goal, but after building A and getting market feedback on it, you realize you actually need to skip B altogether and just build C. That’s why a hard reassessment after each milestone is necessary. The best way to make sure you’re not squandering your resources is to keep reevaluating whether what you’re doing is on your Critical Path. In other words, Critical Path is a framework to help you decide what not to do. Everything you decide to do should be assessed against your Critical Path. Every activity is either on path or not. If it is not on the path, don’t do it!

OVERCOMING YOUR TRACTION BIASES Bullseye is designed to help you find the best traction channel strategy to focus on as quickly as possible. Many founders unfortunately fail at applying Bullseye by ignoring promising traction channels due to natural biases. This is an expensive proposition as it wastes resources by sending you down the wrong path. 


To refresh your memory, here are the nineteen channels: 1. Targeting Blogs 2. Publicity 3. Unconventional PR 4. Search Engine Marketing (SEM) 5. Social and Display Ads 6. Offline Ads 7. Search Engine Optimization (SEO) 8. Content Marketing 9. Email Marketing 10. Viral Marketing 11. Engineering as Marketing 12. Business Development (BD) 13. Sales 14. Affiliate Programs 15. Existing Platforms 16. Trade Shows 17. Offline Events 18. Speaking Engagements 19. Community Building 


We’re sure that some of these channels are unfamiliar to you. Why spend time and money on a channel you know little about, or that you think is irrelevant to your business? Traction channel bias may be preventing you from getting traction. You can get a competitive advantage by acquiring customers in ways your competition isn’t. A major function of this book is simply helping you overcome your biases against particular traction channels by educating you about them. There are three reasons why founders ignore potentially profitable traction channels: 1. Out of sight, out of mind. Startups generally don’t think of things like speaking engagements because they are usually out of their field of vision. 2. Some founders refuse to seriously consider channels they view negatively, like sales or affiliate marketing. Just because you hate talking on the phone doesn’t mean your customers do. 3. Bias against schlep—things that seem annoying and timeconsuming. Channels like business development and trade shows often fall into this category.

Be honest with yourself: which traction channels are you currently biased for or against? You can overcome these traction channel biases and increase your chances of success by taking each channel seriously when using Bullseye. Good mentors can also help you here by helping you brainstorm and rank your channel ideas. Jason Cohen, whom we interviewed for offline ads, makes this point well: I’ll bet you a lot of your competition will refuse to even try these channels. And if that’s true, that’s even more reason to go try those channels! It can almost be a competitive advantage (at least a temporary one) if you can acquire customers in channels that others cannot, or refuse to try. That’s more interesting than duking it out with AdWords competitors in positions one to three. Traction is a tricky thing. Initial traction is unpredictable and can happen in many different ways—nineteen by our count. Because of this unpredictability, it makes sense to consider several channels in the pursuit of traction. 

In fact, every one of the channels listed above has been the channel for both enterprise and consumer startups to get initial traction. At the same time, no one individual is an expert on all channels. However, certain people—namely startup founders who focus on them—end up becoming experts on particular channels. We set out to collect and synthesize all this knowledge from founders and other experts with deep experience in each traction channel. The startup experts we interviewed have founded companies that have made hundreds of millions of dollars, received billion-dollar valuations, and built some of the biggest companies. With this book, we’re giving you what worked for these founders and arming you with the frameworks, strategies, and tactics to help you get similar traction. We’ve already covered a system to figure out which channel to focus on— Bullseye—and how to go about doing so—Critical Path. The rest of the book helps you approach getting traction through each channel. To your startup’s success!

TARGETS

Lay out your milestones. Determine your traction goal and define your Critical Path against that goal, working backward and enumerating the absolutely necessary milestones you need to achieve to get there. Stay on the Critical Path. Assess every activity you do against your Critical Path and consistently reassess it. Building such assessment into your management processes is a good idea. Quantify traction subgoals and put them on a calendar so you can properly monitor your progress over time. Actively work to overcome your traction channel biases. Being on the cutting edge of the right traction channel can make a huge difference in success. Which traction channels do you know most about? Which traction channels do you know least about? Mentors can help here.

CHAPTER SIX

Targeting Blogs

Targeting blogs prospective customers read is one of the most effective ways to get your first wave of customers. However, this traction channel can be difficult to scale in phases II and III due to the limited number of relevant high-traffic blogs. That’s okay. Not all traction channels are infinitely scalable. In fact, using tactics that don’t scale is one of the best ways to get your first customers. Paul Graham put it like this: The need to do something unscalably laborious to get started is so nearly universal that it might be a good idea to stop thinking of startup ideas as scalars. Instead we should try thinking of them as pairs of what you’re going to build, plus the unscalable thing(s) you’re going to do initially to get the company going. We interviewed Noah Kagan, former head of marketing at Mint and founder of AppSumo, to learn how he used this channel to get significant initial traction for both startups by targeting blogs. Mint’s story is impressive. It launched its simple money management site in 2007, and—less than two years later—Intuit acquired it for $170 million. In between, Mint was able to acquire more than 1.5 million customers, 20,000 of whom signed up before it even launched. Within six months of Mint’s launch, it had more than 1 million active users. Very few startups experience this kind of growth during their first six months. Kagan, Mint’s director of marketing at the time, drove many of its early marketing efforts. As he told us in our interview, Mint’s phase I goal was to get 100,000 users within six months. To hit those numbers, Noah created a quantbased marketing spreadsheet like this:

His spreadsheet listed traction channel strategies Mint planned to mine for potential customers. Then, Noah ran the numbers in terms of traffic, clickthrough rates (CTR), and conversions (actually signing up for the product in this case), and then calculated the number of expected customers from each channel strategy. Next, he tested the channel strategies Mint might focus on by running tests on the ones that seemed promising. To test targeting blogs, he contacted a few that were representative of different customer segments and got them to write articles about Mint. This should all sound pretty familiar at this point. Noah’s method is an implementation of Bullseye: he systematically set out to determine which channel would allow him to accomplish his specific traction goal. Mint’s initial series of tests revealed that targeting blogs should be its core traction channel.

Noah then created a long list of blogs to target, and set about focusing on this channel. Initially the focus was on more standard articles and guest posts. Through inner ring testing, Noah additionally uncovered channel strategies that further improved Mint’s traction: VIP access and direct sponsoring. Mint did something that few startups had done before to increase awareness and build excitement for its launch: it asked people on its prelaunch waiting list to recommend Mint to their friends in return for priority product access. As part of the signup process, users could embed an “I Want Mint” badge on their personal blogs, Facebook, or other Web sites. Users who drove signups through these badges were rewarded with VIP access before other invitations were sent out. The key to the success of these badges was to make them easy to share and embed. Much as YouTube provides an embed code below each video on its site, Mint provided the code necessary to make embedding badges as simple as copying and pasting. Many users were happy to place the small badge on their Web site in order to get early access to a product they wanted. 

Mint had six hundred blogs display the badge and fifty thousand users signed up through them. This strategy also gave Mint an SEO boost from the hundreds of new links pointing to Mint.com. Mint used a second innovative strategy to acquire customers through this traction channel: direct sponsoring of blogs. Each sponsored blog would place a small Mint advertisement on its site for a period of time. Noah tracked each advertisement to see which blogs were most effective and how many people signed up. Not only did this approach contribute more than ten thousand preregistrations for its product, but it also allowed the Mint team to understand the kind of customer most interested in its product. Many personal bloggers have strong readerships, but don’t make money from their writing. Noah offered them a way to show off a cool new service and make some money doing it. He simply sent them a message with “Can I send you $500?” as the subject and told them a bit about the product and what Mint was trying to do. Most were happy to share a useful product with their audiences and make some money in the process. Mint also created content partnerships with larger sites like The Motley Fool, a personal investing site. With this content partnership (each site contributed content to the other), Mint exposed its valuable, free product to more than 3 million readers who would likely be interested in its service. 

This postlaunch content partnership combined targeting blogs with elements of business development and was a big win for the Mint team. Noah used this traction channel again at AppSumo, his startup that sells software bundles and educational products at discounted prices. To get traction, Noah pulled together free bundles for blogs and conferences, such as SXSW. One of the first bundles AppSumo did was specifically for Lifehacker, a popular productivity blog. Rather than just trying to pitch Lifehacker on his new startup, Noah built a product bundle for them before reaching out. Lifehacker couldn’t turn down a bundle geared specifically toward its readers. In the blog’s words: Our love of free software here at Lifehacker is no secret, but sometimes you just need to shell out for some advanced features. AppSumo is currently offering a bundle of our favorite productivity webapps, for a fraction of the price. The offer did really well with Lifehacker’s audience, and led to strong early traction for AppSumo. Noah also sponsored blogs to run AppSumo giveaways, much as he did at Mint. AppSumo is now a profitable business with more than 800,000 customers.

TARGETING BLOG TACTICS It can be difficult to uncover the smaller blogs that cover your niche. Here are several tools you can use to find all the influential bloggers in your space: Search Engines—Simply search for things like “top blogs for x” or “best x blogs.” YouTube—Doing a simple search for your product keywords on YouTube shows you the most popular videos in your industry. These videos are often associated with influencers who have blogs, and you can use references to their videos as icebreakers to start building relationships with them. This tactic can be applied to other video sharing sites, such as Vimeo and Dailymotion.

Delicious—Delicious allows you to use keywords to find links that others have saved, which can unearth new blogs. Twitter—Using Twitter search is another easy way to find blogs in a niche. You can also use tools like Followerwonk and Klout to determine the top Twitter accounts in your industry. Social Mention—Social Mention helps you determine the sites that have the most frequent mentions for your keywords. Talk to People—The most effective way to figure out what your target audience is really reading online is by asking them directly! In addition to targeting blogs directly, you can also target the link-sharing communities that often link to them. Sharing links is at the heart of many large communities on the Web (e.g., reddit, Product Hunt, Hacker News, Inbound.org). In addition, there are hundreds of niche communities and forums that encourage and reward the sharing of links. Dropbox, the file storage startup, targeted these communities for their initial traction. By sharing a video on Hacker News, Dropbox received more than ten thousand signups. Soon, it was trending on Digg (significantly bigger at the time), which drove even more signups. Quora, Codecademy, and Gumroad saw similar success from initial postings on Hacker News because their products were a good fit for users of that site. The founders behind Filepicker.io—a file management tool for developers—also posted a basic demo there, looking for some feedback and early customers. Their submission was in the top spot for nearly three hours, during which they saw: 10,000+ visits 460 concurrent users 500+ developer signups 5,000+ files managed In a crowded online environment, reaching prospects in an arena where they choose to spend their time is a valuable way to get traction. Targeting blogs and link-sharing communities can be a great way to get your first wave of customers.

TARGETS

Run tests on a variety of smaller blogs. See what type of audience resonates best with your product and messaging. There are a variety of tools you can use to uncover relevant blogs, including YouTube, Delicious, StumbleUpon, Twitter, search engines, Google Alerts, and Social Mention. You can also ask people! Sponsor small blogs, especially personal blogs. Providing influential bloggers early access or offering early access in exchange for spreading the word are other effective strategies. Offer something unique to your best targets. Build a special offer just for them and put together a draft guest post that they can run with.

CHAPTER SEVEN

Publicity

Public relations (PR) traditionally refers to a company’s public messaging of all kinds. In this chapter we focus on publicity from traditional media like news outlets, newspapers, and magazines. Unconventional PR (like stunts and contests), content marketing, and targeting blogs are all related to publicity and can be magnified via this channel, but are covered individually in other chapters. Starting out, an article in TechCrunch or a feature in The Huffington Post can boost your stature in the eyes of potential customers, investors, or partners. These mentions lead to larger features in news outlets like The Washington Post or The New York Times, which can move the needle for you in a matter of days. Publicity also has secondary benefits like helping with fund-raising, recruiting, and partnerships.

PUBLICITY STRATEGY Startup founders are often unsure about how to get press. The first step is to understand how Internet-driven media works. Ryan Holiday, former director of marketing at American Apparel and bestselling author of Trust Me, I’m Lying, gave us a quick introduction: The news has fundamentally changed. Think of The New York Times. When they decide to publish an article about you, they are doing you a huge favor. After all, there are so many other people they could write about. There are a finite number of spots in the paper. Blogs are different, as they can publish an infinite number of articles and every article they publish is a chance for more traffic (which

means more money in their pockets). In other words, when Business Insider writes about you, you are doing them the favor. Most sites make their money from advertisements, so they want to drive as many page views as possible. If you have a fascinating story with broad appeal, media outlets now want to hear from you because you will drive visits and make them more money. This is why sites like The Huffington Post now churn out hundreds of articles a day: more articles drive more page views, which in turn allows them to make more money from advertisers. There has also been a shift in how the top media outlets operate (CNN, The New York Times, the Today show). These organizations now scour smaller outlets for captivating stories they can present to a wider audience. As Ryan says: It’s better to start smaller when targeting big media outlets. For them, the direct approach is rarely the best approach. Instead, you approach obliquely. So you find the blogs that TechCrunch reads and gets story ideas from. Chances are it will be easier to get that blog’s attention. You pitch there, which leads The New York Times to email you or do a story about you based on the information [they’ve seen] on their news radar. This means you no longer have to pitch CNN directly if you want to get on TV. Instead, you can pitch smaller sites (ones that are easier to get coverage from) whose content is often picked up by larger media outlets. If you tell your story right, you can create buzz around your company and capture the attention of larger sources. Next thing you know, you can put “As seen on CNN” on your Web site. In other words, stories and other content now filter up the media chain, rather than down. Ryan again: Blogs have an enormous influence over other blogs, making it possible to turn a post on a site with only a little traffic into posts on much bigger sites, if the latter happens to read the former. Blogs compete to get stories first, newspapers compete to “confirm” it, and then pundits compete for airtime to opineon it. 

The smaller sites legitimize the newsworthiness of the story for the sites with bigger audiences. Tech startups frequently get exposure this way. Sites like TechCrunch and Lifehacker often pick up stories from smaller forums like Hacker News and subreddits. In turn, The New York Times often picks up content from TechCrunch and wraps it into a larger narrative they’re telling. The story of DonorsChoose.org is an example of the modern-day media chain in action. DonorsChoose is a site that allows teachers to raise money for classroom projects, such as buying a digital microscope for a science class. Many teachers in New York City were using the site to raise money, and several local outlets reported about this effort. Soon after, Newsweek picked it up. The Newsweek story received some attention, but nothing major. Then Oprah happened. One of Oprah’s people saw the article in Newsweek and she decided to name DonorsChoose as one of Oprah’s favorite things for 2010. This national attention from Oprah led to sponsorship from the Gates Foundation and a major increase in donations. Though media outlets are increasingly on the lookout for good stories, there are still challenges to getting exposure. Tens of thousands of companies are clamoring for media coverage. Jason Kincaid, a former reporter at TechCrunch, told us that he personally got pitched more than fifty times every day. What gets a reporter’s attention? Milestones: raising money, launching a new product, breaking a usage barrier, a PR stunt, a big partnership, or a special industry report. Each of these events is interesting and noteworthy enough to potentially generate some coverage. Jason advises bundling smaller announcements together into one big announcement whenever possible. Breaking a usage barrier is great. Releasing a new version is noteworthy. But releasing a new version and breaking a usage barrier in the process is even more attractive to the press. Below is an email pitch Jason Baptiste sent to TechCrunch just before launching PadPressed, his startup that helps blogs look better on iPads. It’s a solid example of a good pitch: short, to the point, contains clear contact information, and links to a product demo. He even mentions he’s happy to do a product giveaway, which makes this pitch even more enticing.

Subject: Exclusive for TC: Launching PadPressed—make any blog feel like a native iPad app Hey Mike, Launching PadPressed tomorrow at noon EST and TC gets free rein on an exclusive before then. PadPressed makes any blog look and behave like a native iPad app. We’re talking accelerometer aware column resizing, swipe to advance articles, touch navigation, home screen icon support, and more. We’ve built some pretty cool tech to make this happen smoothly, and it works with your existing layout (iPad layout only activated when the blog is accessed from an iPad). Okay, I’ll shut up now and you can check out the demo links/feature pages below, which are much more interesting than my pitch. PS—Would also be happy to do giveaways to TC readers. Thanks again and feel free to reach out if you have any more questions (Skype, phone, etc. listed below). Video Demo: http://vimeo.com/13487300 Live demo site (if you’re on an iPad): jasonlbaptiste.com Feature overviews: http://padpressed.com/features Jason Kincaid warned against having your pitch come across as a “wall of text,” something busy reporters who receive hundreds of emails get tired of seeing. Be succinct and clear. When pitching to any media outlet, it’s your job to also create an angle that makes your story compelling. If you can craft a narrative and present it well, you greatly increase your chances of getting a story written. A good press angle makes people react emotionally. If it’s not interesting enough to elicit emotion, you don’t have a story worth pitching. Furthermore, your story should ideally provoke a specific feeling in readers that makes them want to share the story with others. As Ryan said, “satisfaction is a nonviral emotion”—you want readers to do something after reading your piece, not just feel satisfied. For example, Ryan worked with a client who wrote a book about how Wall Street operates. The book contained technical details about high-frequency stock trading and its impact on the economy. However, when they pitched the story,they focused on the book’s implication that the stock market is essentially rigged. 

This pitch resulted in much more press than a book about the stock market typically would receive. Rather than leaving it up to reporters to figure out how to position the story, they now had a story “handle” they could grab when writing about the book. This handle created a strong reaction in readers, which drove conversation around the book. If your pitch doesn’t draw a line in the sand —with some people shaking their heads and some people nodding—it won’t get discussed as widely as you hope. Ryan offered this template email he’s used to pitch reporters successfully: Subject: Quick question Hey [name], I wanted to shoot you a note because I loved your post on [similar topic that did a lot of traffic]. I was going to give the following to our publicist, but I thought I would go to you with the exclusive because I read and really enjoy your stuff. My [company built a user base of 25,000 paying customers in two months without advertising / book blows the lid off an enormous XYZ scandal]. And I did it completely off the radar. This means you would be the first to have it. I can write up any details you’d need to make it great. Do you think this might be a good fit? If so, should I draft something around [their average] words and send it to you, or do you prefer a different process? If not, I totally understand, and thanks for reading this much. All the best, [Your Name]

PUBLICITY TACTICS As we discussed earlier, the best way to get early publicity is to start small. A good first step is using a service like Help A Reporter Out (HARO), where reporters request sources for articles they are working on. 

While you won’t be the centerpiece of the article, assisting a reporter in this way will get you a mention in the piece and help establish your credibility. Another starting point is to offer reporters commentary on stories related to your industry. One of your many jobs as a startup founder is to stay on top of market trends. If you have a good feel for the pulse of your market, you can send reporters follow-up insights on specific stories. At the same time let them know that you are generally available as an industry source. You can also use Twitter to reach reporters online. Almost all reporters have Twitter accounts, and you’d be surprised at how few followers many of them have. Just because they’re not actively seeking followers, however, doesn’t mean they don’t have influence. It just means you have a better chance of standing out when you tweet at them. Staying in contact with reporters over Twitter gives you a leg up when you eventually reach out to them with your more formal pitch. This is exactly how DuckDuckGo was recognized as one of Time magazine’s top fifty sites in 2011: Gabriel first interacted with the reporter on Twitter and was then included in the feature piece. Once you have a solid story, you want to draw as much attention to it as you can. Here are a few ways to do it: Submit the story to link-sharing sites (reddit, Hacker News) with larger audiences. Share it on social networks to drive awareness, which you can further amplify with social ads. Email it to influencers in your industry for comment. Some of them will share it with their audiences. Ping blogs in your space and tell them that you have a story that’s getting some buzz. These writers may then want to jump in themselves to cover you. Once your story has been established as a popular news item, drag it out as long as you can. Email blogs that covered the story (as well as ones that didn’t) and offer an interview that adds to the original story. “How We Did This” follow-up interviews are popular. In addition to driving traffic, publicity can have a substantial impact on your fund-raising efforts. Ryan Holiday talked about this in our interview:

PR has a huge impact on early-stage startups. . . . Funding is obviously very important. But funding is essentially gambling. You’re saying, “I think this unproven person deserves two million dollars of my money to build a business that may or may not succeed, and if it does succeed we need to find a buyer who will pay even more to take it off our hands.” There are so many things that can go wrong. When people gamble, but they don’t tell themselves they’re gambling (as investors do), they need information to justify their decisions, and they need social proof and examples and evidence that they’re doing the right thing. They already know if they want to invest in you or not, and they’re looking for confirmation that they made the right call. Press is one of the single most effective things for pushing people over the edge and confirming they did the right thing. As your startup grows you may choose to hire a PR firm or consultant to help you with this traction channel. This is especially true if you chose to focus on publicity as your core traction channel. A good PR firm can help you: Figure out the best messaging and positioning to the press. Unify your messaging to the press. Do a lot of the legwork in setting up press engagements, especially bigger media tours and events. Break into outlets that are traditionally harder to break into, like broadcast TV and radio, where relationships with reporters and producers are harder to form. However, before making the choice to hire a PR firm, exercise caution. Many of the print reporters we talked to said they ignore almost all pitches from PR firms, but do listen to most founders. PR firms are also expensive: if you’re just testing this channel you can often do it faster and cheaper yourself.

TARGETS

Focus on the right smaller sites. Press stories often “filter up,” meaning major news outlets are often looking to major blogs for story ideas, which in turn are looking at smaller blogs and forums. That means if you can generate buzz on those sites, you can increase your chances of getting picked up by bigger publications. Build real relationships with the specific reporters covering your startup’s market. Read what they write, comment, offer them industry expertise, and follow them on Twitter. Have newsworthy milestones to share. Contact reporters only when you can package your milestones into a compelling emotional story. When you do make a pitch, keep it short and sweet!

CHAPTER EIGHT

Unconventional PR

Do you remember Richard Branson wearing a space suit to announce the launch of Virgin Galactic? How about the Old Spice man filming YouTube videos for people who tweeted at him? Or the car service Uber delivering cupcakes and kittens to employees who wanted a break from work? PR stunts like these are not only amusing, but also a proven way to generate press coverage and buzz. Unconventional PR doesn’t suffer from the crowding that the more popular traction channels face. Nearly every company attempts traditional publicity, but few companies focus on stunts and other unconventional ways to get buzz. There are two different types of unconventional PR. You’re probably familiar with the first type: the publicity stunt. A publicity stunt is anything that is engineered to get media coverage. Richard Branson made his press conferences as outlandish as possible (dressing like a woman, driving a tank through the streets) to get the media talking about whatever Virgin was launching. By creating a spectacle around every one of his product launches, Richard Branson turns uninteresting product launches into international headlines. The second type of unconventional PR is customer appreciation: smaller, more scalable actions (like holding contests or sending handwritten notes to customers) that both increase goodwill as well as generate press coverage. Small gestures like these turn your customers into evangelists, which leads to an increase in organic growth. They also add to your unique image and story, both key elements in building a strong brand.

PUBLICITY STUNTS

When done right, publicity stunts can propel a startup from anonymity to national recognition in an instant. That’s exactly what happened to Half.com. Before their launch, the team at Half.com spent weeks brainstorming ways they could get on the national radar. Eventually, they came up with the unconventional idea to rename a town! And, in one of the most well-executed startup PR stunts, for one year the little town of Halfway, Oregon, was known as Half.com. Founder Josh Kopelman launched Half.com on the Today show with the mayor of Halfway, Oregon. This stunt had everything traditional media loved in a piece: it was unique, surrounded the launch of a high-potential new startup, and told a story about how the company was creating jobs in a small town (they hired several residents). Half.com received coverage from The New York Times, PBS, The Wall Street Journal, and hundreds of other publications as a result of this one PR stunt. It launched in late 1999, before the days of ubiquitous email and social media. Even without these sharing tools, this campaign received more than 40 million impressions and gave Half.com a strong customer base right out of the gate. Just six months later it was acquired for more than $300 million by eBay. WePay, a Web payments startup, pulled another popular stunt at PayPal’s annual developer conference. Rather than marketing to PayPal’s customers traditionally, they placed a six-hundred-pound block of ice at the conference entrance. At the time, PayPal had been criticized for freezing some of its customers’ accounts. With this little stunt, WePay shifted the press conversation to focus on these frozen accounts—at PayPal’s own conference! This stunt led to thousands of signups. It also put WePay on the map as a viable alternative to PayPal at a time when few people knew it existed. As another example, DuckDuckGo (Gabriel’s search engine) bought a billboard in Google’s backyard highlighting its privacy focus. It then used the billboard to get national press stories in USA Today, Wired, and many other media outlets. The reactions from this stunt alone doubled its user base at the time. Blendtec is a blender manufacturer located in southern Utah. In 2007, its team decided to create a series of videos called “Will It Blend?” In these videos, Blendtec’s CEO stood by one of its blenders and blended items like a rake, golf balls, and even an iPhone.

The series took off shortly after the videos were posted to YouTube. The iPhone video alone has received more than 8 million views, and the “Will It Blend?” series has become one of the one hundred most-viewed on YouTube. All for a company that makes blenders! Dollar Shave Club, a subscription shaving startup, got similar attention for its launch video titled “Our Blades Are F**king Great.” It also has millions of views on YouTube and was the main source of the more than 12,000 customers it acquired within two days of launching. The video was also shared more than 31,000 times on Facebook, received over 9,500 comments, 12,000-plus likes, and more than 16,000 tweets. The company benefited in other ways as well. Though Dollar Shave Club had been around for just a short while, it quickly ranked third for the Google search “shave.” This ranking is largely thanks to the 1,500-plus sites that linked to its video. The video also led to features in Forbes, The Wall Street Journal, and Wired.

CUSTOMER APPRECIATION On the other end of the unconventional PR spectrum is the more sustainable, systematic form of this traction channel. Customer appreciation is a simple way of saying “be awesome to your customers.” The goal is still generating publicity. However, if you fail to get press coverage, you still have happy customers and a stronger, more relatable brand, which significantly increases word-of-mouth effects. We talked with Alexis Ohanian, founder of reddit and Hipmunk, about how he’s made customers fall in love with his companies. Shortly after Alexis launched Hipmunk, a travel booking site, he sent out luggage tags and a handwritten note to the first several hundred people who mentioned the site on Twitter. These tags were functional, were cute, and led to many tweets and pictures from happy early customers excited to have a chipmunk as a travel companion. Hipmunk also gave out other free items (T-shirts, stickers, handwritten notes) to show its customers’ appreciation. Alexis did the same thing at reddit. In its early days, he handed out free Tshirts with the reddit alien on the front. He personally emailed users to thank them for spending time on the site and did everything he could to make early

redditors feel appreciated for being part of the community. These stories became a central theme in reddit’s early press articles and had a pronounced effect on the brand. David Hauser took a similar approach at Grasshopper.com. Over the past several years, he’s sent customers Skittles, homemade cookies, Starbucks gift cards, books, and handwritten notes thanking them for their business. Doing these types of things has worked so well for Grasshopper that it has hired two full-time employees whose sole responsibility is to delight customers. Holding a contest is a great, repeatable way to generate publicity and get some word of mouth. Shopify.com, a popular e-commerce platform, is famous for its annual Build a Business competition (and its six-figure prize). Last year, the contest drove 1,900 new customers and more than $3.5 million in sales on its platform. Dropbox used to hold a similar contest with its annual Dropquest competition. In Dropquest, users who successfully completed an intellectually challenging online scavenger hunt were rewarded with online recognition, Dropbox-themed items, and free Dropbox packages for life. In the first year of the competition, almost half a million people went through the quest. Hipmunk has run similar events, like its Mother’s Day Giveaway. For this promotion, company staff asked customers to tell them why they love their mothers more than Hipmunk. They received hundreds of submissions via Twitter and sent flowers and chocolates to the moms of the lucky winners. For $500, Hipmunk generated a lot of attention, increased its follower count, and made several customers (and their moms) Hipmunk fans for life. Hipmunk has run other contests as well, including flying customers home for Thanksgiving and hiring a cartoonist to draw “chipmunked” versions of customers’ Facebook profile pictures. For this last promotion, Hipmunk received more than five hundred requests in less than an hour and was covered by Mashable and several other popular blogs. Its customers received funny Facebook profile pictures, and Hipmunk once again created a unique connection with its customers and increased its Facebook fans by over 350 percent. Good customer support is so rare that, if you simply try to make your customers happy, they are likely to spread the news of your awesome product on that basis alone. Zappos is one of the best-known examples of a company that has incredible customer service. Zappos focuses on creating the best customer experience possible, especially with its support team. In fact, Zappos classifies customer service as a marketing investment, which has interesting implications.

For example, if the average time per phone call at Zappos is high, they do not view that as a negative. It might mean that the support team is taking the time necessary to do an outstanding job. Zappos customer support personnel will help you however they can— whether that’s assisting with returns, ordering a pizza, or exchanging workout clothes for a deep fat fryer (real example). With policies like free next-day shipping and free returns, this focus on customer happiness has made Zappos famous among customers who rarely receive such treatment from large companies.

CASE STUDY: DAVID HAUSER Unconventional PR tactics can have incredible returns on investment. Half.com spent $70,000 and made two hires to generate hundreds of articles and more than 40 million impressions. Dollar Shave Club acquired more than twelve thousand customers with a short video that cost $5,000. Hipmunk received thousands of Facebook likes from a chipmunk-drawing contest that cost the company $500. Blendtec increased its sales by over 500 percent after starting the Will It Blend? series. David Hauser told us the story of how he and his team rebranded their service as Grasshopper.com. Rather than issue a standard press release, they decided to send chocolate-covered grasshoppers to five thousand influential people! With each package they included a link to a short video about how entrepreneurs can change the world. After launching the campaign, they received coverage from major news outlets such as Fox News and were the subject of tweets by Guy Kawasaki and Kevin Rose, entrepreneurs with millions of combined Twitter followers. For $65,000, Grasshopper became a well-known brand among entrepreneurs (its target audience). It received major media coverage, created a YouTube video that was viewed more than 200,000 times, was written up in more than 150 blog posts, and increased the number of visitors coming from Twitter and Facebook by over 3,000 percent. As with Half.com’s PR stunt, this Grasshopper 5000 campaign was a result of prolonged brainstorming and careful planning. After its success, the team decided to continue to use unconventional PR as a core channel.

David and his team pulled off another successful stunt when they introduced “The New Dork” video. They noticed a lack of startup-themed viral videos and so parodied the popular Jay-Z and Alicia Keys song “Empire State of Mind” with a video called “The New Dork.” This video received more than 1 million views and was mentioned by Ashton Kutcher, Mashable, and TechCrunch. The Grasshopper team made a conscious decision to include references to popular publications like Mashable in their video. When the video came out, they sent them a quick note about where in the video they made an appearance. This approach gave publications and individuals an incentive to show their audience how cool they were (by referencing themselves in the video) while giving Grasshopper additional exposure. David’s team at Chargify (another one of his startups) pulled off another successful stunt at the popular SXSW conference. Rather than pony up the $10,000 to $15,000 SXSW sponsors normally have to pay, they did something completely different and had a big green bull run around. For $3,000, they hired a stuntman to dress up as Chargify’s mascot and get people pumped about Chargify. Before this conference, Chargify was a virtual unknown. After SXSW attendees saw a green bull giving people high fives, doing backflips, driving a Corvette, and ultimately getting kicked out of the convention center, the Chargify team left the conference with hundreds of customers and a significant jump in brand awareness. Of course, David’s team has also had their share of flops. They’ve launched unsuccessful March Madness promotions, done failed ticket giveaways, tried to create videos of dancing grasshoppers, and done many other things that just didn’t pan out. Even with these flops, this channel has been well worth it to them. David told us that the majority of their marketing expenses are spent on the stunts and unconventional things they do to generate buzz.

TARGETS Do something big, cheap, fun, and original. A publicity stunt is anything that is engineered to generate a large amount of media coverage. They are often hard to do consistently well, but just one well-executed stunt can move the needle for your company. 

Publicity stunts need to be creative and extraordinary to succeed. Some types that have been successful repeatedly are competitive stunts and viral videos. Be awesome to your customers and good things follow. Common ways to do customer appreciation well are through gifts, contests, and amazing customer support. Excelling in this area is a way to do unconventional PR over a longer period of time. Prepare for failure. Success in this channel is unpredictable. You should have a defined process for brainstorming and selecting ideas, but also understand that not every idea will work.

CHAPTER NINE

Search Engine Marketing (SEM)

Search engine marketing (SEM) refers to placing advertisements on search engines like Google, where online marketers spend more than $100 million each day on Google’s AdWords platform. Sometimes the term may also include search engine optimization (SEO), which is a channel we cover separately. In this chapter we just focus on paid search. Paid search advertising involves buying ads for keyword searches. For example, when someone searches for “leather shoes,” a shoe company would bid to show advertisements next to or above the links that organically show up. It’s sometimes called “pay-per-click” because the shoe company pays only when a user clicks on an ad. SEM works well for companies looking to sell directly to their target customer. You are capturing people who are actively searching for solutions. Here is some basic SEM terminology you should understand before we dive in: Click-Through Rate (CTR)—the percentage of ad impressions that result in clicks to your site. For example, if one hundred people see your ad and three of them click on it, you have a CTR of 3 percent (3/100). Cost per Click (CPC)—the amount it costs to buy a click on an advertisement. CPC is the maximum amount you’re willing to pay to get a potential customer to your site. Cost per Acquisition (CPA)—CPA is a measure of how much it costs you to acquire a customer, not just a click. For example, suppose you buy clicks at $1 and 10 percent of the people who hit your site after clicking on your ad make a purchase. This puts your CPA at $10: CPA = $1/10% = $10.

You are paying $10 to acquire each customer. The 10 percent in this equation is known as the conversion percentage, which is the rate at which people “convert” by taking the action you want (in this case making a purchase). For paid search in particular this measure is calculated with the following formula: CPA = CPC / conversion percentage.

CASE STUDY: INFLECTION We interviewed Matthew Monahan, CEO and cofounder of Inflection, a company that at its peak was spending six figures per month on SEM. Inflection is the company behind Archives.com, a genealogy site, which was acquired by Ancestry.com for $100 million. Inflection’s core technology is its aggregation and management of billions of public records. It originally chose to experiment with SEM because it can provide quick customer feedback and allowed the team to test different product features and messaging. As Matthew told us: One of the things I want to really emphasize here is just how compelling SEM is as a way to get early customer data in a fairly controlled, predictable manner. So even if you don’t expect to be profitable, you can decide to spend $5,000 (or $1,000, or $500) on an advertising campaign and get an early base of customers and users. It informs a whole bunch of things that are really important in terms of the basic [metrics]: conversion rate of your landing pages, how well email captures are working . . . if you’re selling a product, what the average cost per customer is and what their lifetime value might be. Having those baseline metrics is critical for informing your strategy moving forward and determining what you need to work on. The Archives.com team used AdWords to drive traffic to landing pages before they made a big investment in building a product. Each landing page was written to test interest in a specific product approach. 

For example, one page tested “get access to census data” while another tested “get access to your family genealogy.” By measuring the CTR for each ad and conversions on the associated landing pages, they determined which product aspects were most compelling to their potential customers and what those people would actually pay for. Matthew explains further: We just wanted to learn as much as we could for as little cost as possible. So we would test different keyword segments, we would test different concepts. For example, one of the early concepts we were trying to test was whether people wanted to trace their genealogy to find as many of their ancestors as possible. We had to ask ourselves, “Are we trying to build a product experience where you can find hundreds and hundreds of your ancestors, or are we trying to build a product experience where you can go as far back as possible, like trace your family tree all the way back to the 1200s? Or are people more motivated by finding out they’re related to a celebrity or some historical figure, and therefore should we focus more on those family trees and those lineages?” Matthew’s approach exemplifies the benefits of building traction and developing your product in parallel. These tests gave the team a clear idea of the type of product their customers wanted. When they finally built their product, they built something they knew the market would want, not something they thought it would want. Archives.com’s initial SEM campaign broke even after just a few weeks, meaning their CPA was about equal to the amount of money they made from each customer. Because they had such positive early results without even optimizing their landing pages and signup flows, the team realized that SEM would be a great channel for them. And it was—the Archives.com business was essentially built on paid search. It dedicated several employees and more than $100,000 a month to customer acquisition through this channel.



SEM STRATEGY

The basic SEM process is to find high-potential keywords, group them into ad groups, and then test different ad copy and landing pages within each ad group. As data flows in, you remove underperforming ads and landing pages and make tweaks to better-performing ads and landing pages to keep improving results. Google’s AdWords is the main platform for SEM because Google has the most search engine traffic. However, Bing Ads (with its ads running on Yahoo!, Bing, and DuckDuckGo) is also worth looking at. We will focus on Google’s platform here, but the same concepts apply across all SEM platforms. Keyword research is the first core component of a strong SEM strategy. With Google’s Keyword Planner, you can discover the top keywords your target customers use to find products like yours. When you enter a term in this tool, it tells you how often your keyword (and similar terms) is searched. Other tools such as KeywordSpy, SEMrush, and SpyFu are valuable for discovering keywords your competition uses to attract customers. You can further refine this keyword list by adding more qualifying terms to the end of each base term, creating what are known as “long-tail keywords.” For example, if you wanted to reach people searching for “census data,” you could make that a more targeted search term by adding “1990” to form “1990 census data” or even more long-tail like “1990 Philadelphia census data.” Long-tail keywords are less competitive and have lower search volumes, which make them ideal for testing on smaller groups of customers. Keep in mind that SEM is more expensive for more competitive keywords. As such, you will want to limit yourself to keywords with profitable conversion rates. After you have keywords you want to target, it’s time to run experiments on the AdWords platform. You should not expect your campaigns to be profitable right away. However, if you can run a campaign that breaks even after a short period of time (as Inflection did after a few weeks), then SEM could be an excellent channel for you to focus on. A campaign is a collection of ads designed to achieve one high-level goal, like selling shoes. You first create different ad groups. For example, if you’re an e-commerce store, you might create an ad group for each product type (e.g., sneakers). You then select keywords you want your ad groups to appear for (e.g., “Nike sneakers”). After you’ve determined the ad groups and keywords you are targeting, create your first ad. When you write an ad, the title should be catchy, memorable, and relevant to the keywords you’ve paired with it. 

You will also want to include the keyword at least once in the body of your ad. Finally, you will want to conclude with a prominent call to action (CTA) like “Check out discounted Nike sneakers!” Once you set up your ads, you should use the Google Analytics URL Builder tool to create unique URLs (Web addresses) that point to your landing pages. These URLs will enable you to track which ads are converting, not just the ones that are receiving the most clicks. Matthew told us that someone just starting out in this channel should begin testing just four ads. Four ads will give you a good baseline for the performance of SEM as a whole, while still allowing you to test different messaging, demographics, and landing pages. If a test is promising, you will keep trying to optimize your campaign to make it become profitable. Building a scalable SEM campaign can take a long time because there are so many variables to test—keywords, ad copy, demographic targeting, landing pages, CPC, and more. However, this complexity can actually work to your advantage. As you test and optimize every element of your SEM strategy, you may find opportunities for huge gains. As Matthew told us: I think it’s a huge competitive advantage because, even though it can feel like tinkering or incrementalism at the time, what you’re really talking about is a major business improvement. Let’s say keywords cost 15 cents, and you’re running a Web site and for every click you’re able to generate 13 cents. Well, if you scale that, that’s a losing business. If you improve . . . and you get that 13 cents to 16 cents, now all of a sudden you have something that’s more sustainable. And if you go from 16 to 20 cents, you’re looking at 25 percent profit margins in terms of sales minus marketing. So a small gain, to go from 13 cents to 20 cents, is essentially a 50 percent gain, but it’s a complete game changer in terms of your ability to advertise and your ability to scale a business. And that 50 percent can be achieved through optimizing all of these different levers. You can use tools like Optimizely or Visual Website Optimizer to run A/B tests on your landing pages. When we asked Matthew if the approach to SEM he discussed still applied to Inflection as it entered more competitive markets, he said: The fundamental concepts of keyword research, market discovery, running split tests, ad tests, controlling your budget, trying to get as close to breaking even as possible, focusing on learning . . . I think those things all still apply. They certainly still apply to us as we build out new keyword segments. That part of the advice doesn’t change, even though the dynamics are more competitive. Each of your ads and ad groups as well as your whole AdWords account has a quality score associated with it. This score is a measure of how well customers are responding to your ads. It includes many factors, from CTR to how long people stay on your site after seeing your ad. A high quality score can get you better ad placements and better ad pricing. The quality score is Google’s way of rewarding advertisers for high-quality advertisements. Click-through rate has the biggest influence on quality score by a wide margin. Because your ad’s relevance to a particular keyword has the biggest impact on your CTR, you should tailor your ads to the keywords they’ll appear against, either manually or dynamically (for example by using AdWords’ Dynamic Keyword Insertion feature). Several sources have mentioned that an average CTR for an AdWords campaign is around 2 percent, and that Google assigns a low quality score to ads with CTRs below 1.5 percent. If any of your keywords are getting such low CTRs, rewrite those ads, test them on a different audience, or ditch them altogether. Inflection makes it a priority to have a strong quality score, which gives it an advantage over less established companies. It also means it has more runway to optimize its ads and conversions.

SEM TACTICS Once you have a search engine marketing campaign up and running successfully, you may want to start exploring some more advanced tools and features. When you set up a campaign, you can choose to advertise on the Google search network (traditional paid SEM), the Google content network (ads on nonGoogle sites), or both. For beginners and for those just testing this traction channel, the content network can be difficult to navigate. Yet once you have established profitable campaigns, you should consider expanding them to the content network, which includes millions of non-Google sites that serve Google’s ads. You should also consider luring people back to your site by retargeting through Google AdWords, or other sites like AdRoll or Perfect Audience. With retargeting, people who visit your site will see your ads elsewhere on the Internet. These ads often convert at a higher rate, as they are aimed at prospects who have already visited your site at least once. For example, suppose you’re a shoe store and a customer put some Nikes in her cart, but didn’t complete the checkout process. With retargeting, you could show her an ad for that type of shoe. This personalization makes such ads particularly effective, often generating 3x to 10x higher CTRs. However, be forewarned that it may feel a little creepy to certain people depending on what data you are using to retarget. People increasingly don’t like ads following them around the Internet, especially those that are a reflection of activities they consider personal or private. Another advanced tool is Google’s Conversion Optimizer. It analyzes your conversion tracking data and automatically adjusts your ads to perform better. After you’ve been running a campaign for a while, using this tool can make your CPAs and keyword targeting better than you would be able to on your own. If you decide to use the Conversion Optimizer, know that it can take time for Google to build a robust prediction algorithm for your campaign. You can use negative keywords to prevent your ads from showing for certain keywords. You specify words that you don’t want your ads to appear for: if you’re selling eyeglasses, you want to prevent your ads from showing to people who search “wine glasses” or “drinking glasses,” as those keywords will convert poorly. This technique can significantly improve your CTRs. One more advanced tactic is using programming scripts to automatically manage your ads. You might use scripts to set up new ads for certain keywords or to change existing ads. Scripts are especially helpful if you are managing a large number of ads or keywords.

If you’re not yet scaling up your efforts or focusing on this traction channel, advanced tactics like these are premature. However, we suggest everyone run some SEM tests because they are straightforward, are cheap to do, and can give you quick insights into your business.

TARGETS Use search engine ads to test product positioning and messaging (even before you fully build it!). Do not expect your early SEM ad tests to be profitable. If you can run an ad campaign that gets close to breakeven after a few weeks, then SEM could be the traction channel for you to focus on. A test ad campaign can be as little as four ads that you use to experiment. Measure conversions so you can test SEM variables against profitability. Areas you should be testing include keywords, ad copy, demographic targeting, landing pages, and CPC bids. Cost per acquisition (CPA) is how much it costs you to acquire a customer, and that is ultimately what you need to be testing against. Use longer keywords. Known as long-tail keywords, they are often less competitive because they have lower search volumes. As such, they are cheaper and so can be more profitable—you just may have to aggregate a lot of them to get the volume you need to move the needle. Pay close attention to your ad quality scores. High quality scores get you better placement on the page and better pricing on your ads. The biggest factor in quality scores is CTR.

CHAPTER TEN

Social and Display Ads

Display ads are the banner ads that you see on Web sites all over the Internet. Social ads are the ads on social sites, like those in or near your Facebook and Twitter timelines. Billion-dollar brands such as Rolex, American Apparel, and other household names pay millions each year for social and display ads that push their brands to the forefront of consumers’ minds. This is one of the larger traction channels, in which companies spend more than $15 billion a year. Large display advertising campaigns are often used for branding and awareness, much like offline ads. Display advertising can also elicit a direct response, such as signing up for an email newsletter or buying a product. Social ads are changing rapidly and are being used for a range of campaigns including both branding and direct response. One application that is unique to social ads and where they have performed exceptionally well is when they are used to build an audience, engage with that audience over time, and eventually convert them into customers.

DISPLAY ADS Most display advertising is run by ad networks that aggregate advertising inventory across thousands of sites (blogs, communities, media outlets, etc.) and sell that space to advertisers. For advertisers, they can buy ads on multiple sites through a single platform. At the same time, publishers can monetize their content by working with just one partner. The largest display ad networks are Google’s Display Network (also known as the Google content network), Advertising.com (owned by AOL), Tribal Fusion, Conversant, and Adblade. Each of these networks has targeting capabilities that allow you to reach specific types of demographics, and they offer a variety of ad formats like text, image, interactive, and video.

These networks are enormous. Google’s display network alone has more than 4 billion daily page views and 700 million monthly visitors, and reaches over 80 percent of the total online audience. Mike Colella, founder of Adbeat, an ad intelligence company, told us how display ads allow you to reach a broader audience than SEM ads: The interesting thing about display advertising is that somebody doesn’t have to be directly related to whatever your product is to find out about it. For instance, if you’re selling some sort of weight-loss product, you don’t have to use terms in your display campaign about losing weight. You can use terms relating to nutrition or carbohydrates, because you know if someone starts to read about those things, they have an interest in maintaining or losing weight in some way. Niche ad networks focus on smaller sites that fit certain audience demographics, such as dog lovers or Apple fanatics. One such network is The Deck, which targets the niche audience of Web creatives. As an advertiser, you know exactly the audience you’re reaching. Another network, BuySellAds, offers advertisers a self-service platform for buying ads directly from publishers. In addition to buying and selling display advertising, BuySellAds allows advertisers to purchase space on mobile Web sites, Twitter accounts, mobile apps, email newsletters, and RSS feeds. With its flexibility and low starting cost, BuySellAds is an easy way to start testing this traction channel. The last approach to display advertising is one of the simplest: go directly to site owners and ask to place an ad on their site for a fixed price. This works well when you want to reach the audience of a small site that isn’t even running ads. This approach requires only a few emails and a couple hundred dollars. To get started in display advertising, first understand the types of ads that work in your industry. Tools like MixRank and Adbeat show you the ads your competitors are running and where they place them. Alexa and Quantcast can help you determine who visits the sites that feature your competitors’ ads. Then you can determine whether a site’s audience is the right fit for you.

SOCIAL ADS Social ads work especially well for creating interest among potential new customers. People who see these ads may not have any intention of purchasing now—they may not even be familiar with the company or its products. That’s okay. The goal of social ads is often awareness oriented, not conversion oriented. A purchase takes place further down the line. We interviewed Nikhil Sethi, CEO and cofounder of social ad platform Adaptly, to discuss how startups can take advantage of social ads to get traction. Adaptly gives companies one platform to manage and place social ads across many sites. Nikhil told us about the concept of indirect response (as opposed to direct response) in social advertising: In the social context, what we’re talking about is “indirect response.” You’re still focused on a sale, an install, a signup, or whatever, but the methodology to get there is different. Instead of looking at every click and how it converts, indirect response says, “Let’s create an environment within the social context that’s geared toward the specific product or service you’re trying to offer, build affinity there, build loyalty there, and then migrate that audience toward some conversion element we want to occur at a later point in time.” Nikhil’s approach may seem counterintuitive. Rather than focusing on selling more product directly (tracking click-through rates, conversions to buying), he believes social advertising works best when you take advantage of the unique characteristics of social media platforms to build an audience. Only after building this audience do you move them toward a conversion—whether through buying, using, or sharing your product. Building an audience through social ads is more valuable than you might suspect. CareOne, a debt consolidation and relief company, conducted a study in 2011 comparing the customers it received from social ads against those it received from other channels. Here’s what it found: Social media connections filled out the consultation (leadgeneration) form at a 179% higher rate than the typical customer. Sales? They were 217% more likely to make their first payment.

 For one particular problem area (people who partially fill out the sign-up form then quit), social media prospects went back and completed the form at a 680% higher clip than non– social media leads. They made their first payment at an astonishing 732% better rate. People visit social media sites for entertainment and interaction, not to see ads. An effective social ad strategy takes advantage of this reality. Social ads give companies the opportunity to start a conversation about their products with members of their target audiences. One way startups can do this is by creating compelling content. Instead of directing people to a conversion page, direct them to a piece of content that explains why you developed your product and your broader mission, or has some other purpose than immediately completing a sale. As Nikhil told us, this is where social advertising can be extremely effective: If you have a piece of content that has high organic reach, when you put paid [advertising] behind that piece of content the magic happens. As more and more people see it, more and more people engage with it—because it’s a better piece of content. . . . Paid is fundamentally only as good as the content you put behind it. And content is only as good as how many people actually see it. Content only goes anywhere if people care about it. . . . With social, it’s word of mouth on crack. You should only employ social advertising dollars when you’ve understood that a fire is starting around your message and you want to put more oil on it. Getting that spark started is based on what you’re trying to say: startups do the opposite of this all the time where they waste tens of thousands of dollars trying to push a message that nobody cares about. With social platforms, the burden of success is on the advertiser as opposed to the platform. If you’ve invested time and energy creating a great piece of content, spending a little bit of money to ensure that content gets wide distribution makes sense. At Airbrake, one of the companies Justin ran growth for, they promoted some of their best content on Twitter and Facebook. 

In one case, after spending just $15 on Twitter ads, they received hundreds of organic retweets, tens of Facebook likes, and two submissions to reddit and Hacker News. In total, this $15 drove tens of thousands of visits to Airbrake’s site. Just a bit of paid promotion sparked a fire of organic engagement with the content, which was an interview with Stripe’s CTO. This tactic also works well with content distribution networks like Outbrain and Sharethrough. Each of these ad networks promotes your content on popular partner sites like Forbes, Thought Catalog, Vice, Gothamist, and hundreds more. These native ad platforms make your content look like any other piece of (native) content on the target site. Creating engaging social experiences is another way to succeed on social sites. Warby Parker has done this well. They will send you eyeglasses by mail, let you try them on and send them back, all for free. When you receive your glasses, they encourage you to post pictures of yourself to social sites for feedback from others. It is a fun, useful, and engaging process.

Major Social Sites Here are some well-known social sites where you could advertise. LinkedIn—LinkedIn’s social network is made up of more than 250 million business professionals. LinkedIn ads allow targeting by job title, company, industry, and other business demographics, all factors you can’t easily target elsewhere. Twitter—Twitter also has roughly 250 million users. Twitter’s ads come in the form of sponsored tweets that appear in users’ feeds. Nikhil mentioned that one of the most effective approaches on Twitter is to turn on paid advertising around real-time events that your audience cares about (e.g., sportswear ads during major sporting events). Facebook—Facebook has more than one billion active users on its social network. From an advertising perspective, Facebook offers companies the ability to buy targeted ads based on users’ interests, pages they like, or even people they’re connected with. This granular targeting allows you to target very small groups of people. In fact, Gabriel once ran a test campaign that targeted only his wife! (He targeted her by her alma mater, zip code, and interest affinities, using a picture of their son to see how long it would take for her to notice. Not very long.)

The platform also allows you to reach the larger network of people connected through your fans on Facebook. As Nikhil said: When you buy a Facebook ad, you’re buying more than just a targeted fan; you’re buying the opportunity to access that fan’s social graph. With the proper incentives, fans will share and recommend your brand to their connections. StumbleUpon—With more than 25 million “stumblers,” StumbleUpon has a large potential user base looking for new and engaging content. An interesting feature about this site is that ads don’t surround the content on StumbleUpon— they are part of the content. When people hit the “Stumble” button, they will be directed to a paid piece of content that looks just like any other site on the network. The downside to traffic from StumbleUpon is that its users are difficult to engage—most users are likely to click off your page as quickly as they came to it. This means you have to make sure to engage these people right away. Blog posts, infographics, and video content can do well on the site. Foursquare—With more than 45 million users, Foursquare is the largest location-based social network. Foursquare ads can work well if you wish to reach a targeted, local population. Foursquare’s ad platform is rapidly evolving but generally allows companies to send out ads hypertargeted at particular locations or to people who have visited those or similar locations. Tumblr—Tumblr is all about helping its 100 million–plus users discover high-quality content. Its ad platform allows brands to create and promote sponsored posts, which Tumblr’s many users can reblog and engage with. reddit—With more than 5 billion monthly page views and a thriving platform of more than 175 million monthly uniques, reddit is one of the most popular content sites in the world. reddit ads can take the form of sponsored links that hover at the top of reddit’s pages or sponsored ads along the sidebar. The most successful reddit advertisements are controversial or funny. These types of ads encourage redditors (the official name of reddit users) to engage with them by leaving comments and upvoting/downvoting as if they were any other content on the site. Smart advertisers target communities (there are more than half a million) that are relevant for their product and engage with all the commenters on their ad. As a platform for online communities, the reddit network is vast.

 Targeting a community of bacon lovers or gay gamers? There are reddit communities for that (r/bacon and r/gaymers, respectively). YouTube—With more than one billion monthly unique visitors watching more than four billion hours of video, YouTube is by far the world’s largest video site. On their platform, brands can create ads that show before a video is played (known as pre-roll) and create banner advertisements on top of videos. Others—There are plenty of other major sites you can target for social ads —BuzzFeed, Scribd, SlideShare, Pinterest, etc. Because these sites were established more recently, advertising on them and even newer ones can offer a unique window of opportunity for substantial growth. Social ads and display advertising follow similar principles. Namely, you want to understand your audience, experiment with your message, and reach people in a memorable way. They can make sense at any product phase, as they allow for very small or very large ad buys.

TARGETS Contact small sites directly for display ads. Ask them to run your ads for a small fee. This is an underutilized strategy in display ads, especially in phase I. Study your competitors’ ads to get good ideas for A/B tests to run on your ads. Use social ads to build awareness of products and create demand. The goal with social ads should be to build an audience, engage with that audience over time, and eventually move them to convert to customers. This indirect response strategy usually leads to more conversions than a direct response strategy that tries to get people to convert immediately. Create compelling social content. The best way to build a presence and engage your audience on social sites is to concentrate on creating less content, but making it highly shareable. When your content is getting naturally shared, that’s the time to promote it further with social ads.

CHAPTER ELEVEN

Offline Ads

Even today, advertisers spend more on offline ads than they do on online. There are many kinds of offline ads—TV, radio, magazines, newspapers, yellow pages, billboards, and direct mail. All of these can be utilized at almost any scale, from local campaigns to national ones. They are used by billion-dollar brands like Walmart and by local teens looking for babysitting gigs. The demographics of each advertising medium are the most important factor to consider when making an offline ads purchase. For example, ads in the classified section of a newspaper will appeal primarily to an older crowd that still buys newspapers. You’ll want to think about location, gender, race, age, income, and occupation—and how each matches up with your target audience. You should be able to answer many of these questions by asking for an audience prospectus (sometimes called an ad kit) from whatever company is selling the ad inventory. As an example, for billboards you should receive information about the aggregate demographics of the area around the billboard, approximately how many people drive by it per day, and a sense of who those people are. Many kinds of offline outlets allow you to go even further and pick not just the demographics, but also the mind-set of the customer you’re approaching. In this case you’re also taking aim at a sensibility. The kinds of people who read a local arts magazine represent a different sensibility from the people who listen to the local top-40 music station. One way to find the best offline places to advertise is to ask your target customers. What magazines do they read, or radio stations do they listen to? Where do they see ads they actually remember?

OFFLINE ADS STRATEGY

In general, the cost of an offline ad depends on its reach. A billboard in Times Square goes for much more than one in the middle of Ohio because more people see it. Most offline advertisements work similarly. Thanks to the wide variety of offline media available, you can scale your ad buys according to your budget and product phase. Not sure if magazine ads will be a good channel for you? Buy a small ad in a niche publication and give it a test. Want to see if newspapers reach your audience effectively? Buy a few advertisements in a local paper. For as little as $300, you can put out a radio ad in a market you’re targeting and see how it performs. Billboards are the same way: you can buy space on one for a few hundred bucks a month. Once you’ve established that offline ads are effective, you can save money by signing a longer advertising contract. With an up-front commitment, advertisers will give you a substantial discount. To get really cheap offline ads, look for remnant advertising. Remnant advertising is ad space that is currently being unused. For example, publications accept almost any price when selling empty inventory near print deadlines: after all, it is a complete loss for them if they don’t sell that space. Tim Ferriss, bestselling author of The 4-Hour Body and The 4-Hour Workweek, has said this on the subject: If dealing with national magazines, consider using a print or “remnant ad” buying agency such as Manhattan Media or Novus Media that specializes in negotiating discounted pricing of up to 90% off rate card. Feel free to negotiate still lower using them as a go-between. If you’re not sensitive to location or timing, you can get substantial discounts by committing to buy remnant inventory. This can be a cheap and effective strategy to reach millions of people if you have a mass-market product. Think of those “We buy ugly houses” billboards or any of the repetitive billboards that you see all over the place: they are likely using this approach. Offline ads are much harder to track as compared with online ads that have tracking built in. Successful offline tracking involves the use of unique Web addresses and promotional codes to measure effectiveness. For example, we could create flyers that link to tractionbook.com/flyer. By tracking visits to that specific URL, we could approximate how many visits originated from our flyer campaign.

There are other tracking options as well. Jason Cohen, founder of code review company Smart Bear Software, was doing all kinds of offline advertising —magazines, trade shows, newspapers, and more—to sell his software. When people signed up, Jason included a section that asked new customers, “How did you hear about us?” This question was designed to measure the efficacy of the company’s online and offline campaigns. Jason also included an offer for a free book on code review in Smart Bear ads. This book offer was another way to track the effectiveness of the ads. If an ad in Dr. Dobb’s Journal resulted in a high number of book orders, then there was a high probability that the promotion worked.

PRINT ADVERTISING Print advertising encompasses magazines, newspapers, the yellow pages, flyers, direct mail, and local directories. Among the different categories of offline ads, print trails only TV in terms of overall spending. Print advertising is appealing because it works with just about any budget and allows for precise audience targeting. There are nearly seven thousand different magazines in the United States, ranging from commercial publications with millions of subscribers to small trade publications with hundreds of readers. There are three general magazine categories: consumer publications that appeal to the larger population (these are the ones you see on newsstands and in grocery stores), trade publications covering a particular industry or business, and local magazines that you’ll see for free along sidewalks and near grocery stores. You need to understand the reader demographics, circulation, and publication frequency of any magazine you’re considering. To get this information, just ask the magazine for its ad kit (also known as a media package, media kit, or press kit). No matter how well you’ve targeted your audience, your magazine ad will not get a good response unless it is well designed. A compelling magazine ad will have an attention-grabbing header, an eye-catching graphic, and a tagline or description of the product’s benefits. Jason Cohen mentioned that the Smart Bear ads that performed well all had a strong call to action: in his case, the offer for that free book.

Newspapers share many characteristics with magazines. They are published on both a national and local scale, their pricing is largely based on the circulation of a given paper, and they allow you to choose the type of ad you want in the paper. One major difference, however, is that newspapers slant heavily toward the over-thirty demographic. Many young people still buy magazines. Not many young people still buy newspapers offline. There are some ad campaigns that are uniquely suited to a newspaper setting. A few examples are time-sensitive offers (as for events or sales), awareness campaigns (often as part of a larger marketing effort across multiple channels), and widely publicized announcements (as for product launches). Direct mail entails any printed advertising message (ads, letters, or catalogs) delivered to a specific group of consumers through the postal system. It may surprise you to learn how effectively you can target customers through direct mail. You can build up a list of customers on your own or buy a list from a mailing organization. Simply do a Web search for “direct mail lists” to find companies selling such information. Beware that buying lists can be perceived as spammy, and can be a complete waste of money if they are untargeted. You can buy lists grouped by demographic, geography, or both. These lists often sell for about $100 for one thousand consumer names, and a bit more for business names and addresses. There are even direct mail services that will buy address lists, print your marketing materials, and assemble and mail everything for you. This makes sense if you’re planning to do a high-volume mailing— otherwise, you’re the one who has to do the printing, addressing, and mailing. Here are a few good tactics to use if you are interested in pursuing direct mail: If doing a postal direct-response campaign, provide a selfaddressed envelope to increase the number of recipients who respond. Use handwritten envelopes and cards to increase the chances of someone opening and reading your mailing. Have a clear action you want the recipient to take, such as visiting your Web site, coming into your store, buying a certain product, or signing up for an email list. Investigate bulk mail with the postal service to get reduced pricing.

Local print ads include buying space in local flyers, directories, calendars, or publications such as church bulletins, community newsletters, or coupon booklets. These print ads are a good way to test print advertising because of their modest cost: just a few hundred dollars can expose you to thousands of people in a targeted area. Ads in the yellow pages are similarly inexpensive. Unorthodox strategies like hanging flyers in areas where your potential customers visit can be a surprisingly effective way to get some early traction for your company. For example, InstaCab hired cyclists to bike around San Francisco and hand out business cards to people who were trying to hail taxis. These were well targeted (it’s a good bet that someone hailing a taxi would appreciate an easier way of getting around) and got the company some good buzz and customer adoption early on.

OUTDOOR ADVERTISING If you want to buy space on a billboard, you’ll probably contact one of three companies: Lamar, Clear Channel, or Outfront Media. They are the power players in this $6 billion industry. If you want to get a sense of what is available in a given area, go to the Web sites of the above companies and contact a local representative. They will give you PDFs of local available billboards, showcasing their locations and audience. We have some personal experience with billboard advertising, as we discussed in the publicity chapter. Gabriel strategically placed a billboard in the startup-heavy SoMa district of San Francisco to call out the differences between the privacy practices of Google and DuckDuckGo. A startup search engine calling out the big guy in their backyard—that is the kind of strong message that can get you some traction. In this case, DuckDuckGo didn’t just capture the attention of the people who drove by the billboard. It also got press coverage from Wired, USA Today, Business Insider, and several other blogs and media outlets. That month, DuckDuckGo’s user base doubled! What does all this cost? Gabriel’s billboard cost $7,000 for a month. Billboards in less prominent locations can cost anywhere between $300 and $2,500 per month. Ads in Times Square, on the other hand, can run you $30,000 to $100,000 per month.

The cost of billboard space depends on the size of the ad, where it is located, the number of impressions your ad can provide, and the type of billboard it is (e.g., digital). Every billboard has an advertising score, known as a GRP score (Gross Ratings Points), based on the above factors. The number of potential impressions is based on the number of people in an area who could see the billboard: a full score means that a given billboard should reach 100 percent of the driving population during a month. The major downside of billboard advertising is that it is difficult for people to take immediate action on what they see. It is dangerous for someone to visit a Web site, call a number, or buy a product while driving on a highway. However, billboards are extremely effective for building awareness around events— concerts, conferences, or other activities coming to an area. In Las Vegas, for example, you’ll see billboards touting acts and musicians performing in the coming weeks. Transit ads are placed in or on buses, taxis, benches, and bus shelters. Most ads of this kind can be effective as a direct-response tool because people in transit are a captive audience. If you want to get started with transit advertising, we suggest checking out a company that specializes in these ads, like Blue Line Media. These media agencies can help you figure out where to advertise, how to create a memorable transit ad, and how to best measure and optimize such a campaign. Another advantage of billboards and transit media is that they are replaced only when there are new ads to go up. While a radio, TV, or print ad will run only once, there’s always a chance your billboard will stay up long past the dates you paid for.

RADIO AND TV ADVERTISING Radio ads are priced on a cost-per-point (CPP) basis, where each point represents what it will cost to reach 1 percent of the radio station’s listeners. The higher the CPP, the more it will cost you to run an ad on a station. This cost also depends on which market you’re advertising in, when your commercials run, and how many ads you’ve bought with that station. To give you an idea of what a radio ad costs, an ad running on a station for a week is often $500 to $1,500 in a local market and up to $4,000 to $8,000 in a larger market like Chicago. If you are scaling your radio buys, satellite radio is another place to consider.

 With more than 50 million subscribers, SiriusXM can help you reach a lot of people with just one advertising relationship. TV advertisements are often used as branding mechanisms. Most of us remember famous Nike, Apple, or Wendy’s commercials. When you consider that 90 percent of consumers watch TV, and the average adult watches twentysix hours of TV per week, this is an offline channel that must be considered. Quality is critical for TV ads. Production costs for actors, video equipment, editing, sound, sound effects, and shooting can run to tens of thousands of dollars. In fact, some of the higher-end commercials you’ll see can cost upward of $200,000 to make. Fortunately, there are ways for you to reduce the costs of creating a TV ad. Using animation as opposed to live actors is a lot cheaper. If you do use live actors, you can recruit local film students to perform for you. Finally, just keeping the commercial as simple as possible will go a long way toward reducing costs. In addition to the cost of creating an ad, there is the (national average) $350,000 for actual airtime. These costs make national TV campaigns tough to swallow for many small startups. However, over the last few years it’s become possible to advertise on TV without spending so much money. Local TV spots on one of the 1,300-plus TV stations in the United States can be an effective and reasonably priced way to make an impression. Prices for local commercials can range from $5 to $50 per one thousand viewers for a thirty-second time slot. As with so many other offline channels, you just have to contact the station to find out the number of viewers a station has and how much a TV spot costs. Buying TV ads is a rather opaque process that involves a lot of negotiation, as there are no rate cards in the industry like those in print advertising. Thus, for larger media buys, you will likely want to hire a media buyer or agency to handle the many sellers out there and to ensure that you get a quality spot at a fair price. Infomercials are basically long-form TV advertisements. You’ve probably seen them, from the Snuggie to ShamWow, and all sorts of knives, vacuums, and workout products. Though the products and pitchmen in them frequently become punch lines, infomercials can work surprisingly well. 

For example, they were the main growth engine behind the rise of P90X and its $400 million in sales. Traditionally, products in the following categories have used infomercials to gain serious traction: Workout equipment or programs Body care products Household products (kitchen, cleaning) Vacuum cleaners Health products (e.g., juicers) Work-from-home businesses Products like these require more time to showcase what they have to offer. Consider the Snuggie. In a fifteen-second spot, it’d be really difficult to sell you on why the Snuggie is a great product. But through two-minute shorts, the Snuggie was able to sell millions of units. Infomercials can cost anywhere between $50,000 and $500,000 to make. They can be two-minute infomercial shorts or the more traditional twenty-eightminute episodes. These ads are almost always direct response: advertisers want people to see it, then visit a Web site or call in to take advantage of a special offer. The best infomercial marketers will often test their messaging, calls to action, and bonuses by running radio ads in advance, seeing what works well, and then incorporating those bonuses and messages into their commercial.

CONCLUSION Clearly, there are lots of ways to take advantage of offline advertising. The branding potential, cost, impact, and flexibility of this channel make it a really strong one to consider when looking at how to get traction in later phases. The best way to approach this channel is to understand that there is no guaranteed way to predict what will work. But, if you keep at it, you may end up with an effective offline advertising strategy. As Jason Cohen said: One thing I learned at Smart Bear is that I have zero ability to predict what’s going to work. There’d be a magazine where I thought, “This is just some piddly magazine, surely no one reads this,” and sure enough it was cheap (due to small circulation) and it’d do terrifically! Our ROI on some of those were incredible. And you just couldn’t predict, whether on circulation size or media type, how it was going to go. And it changed over time—an ad might be good for a quarter, or a year, and then decay slowly until it wasn’t valuable anymore. It was unpredictable and decayed over time: so the only thing we were left with was trying everything and measuring what worked.



TARGETS Run cheap tests by first targeting local markets. It is hard to predict what will work, so it is often useful to run several small offline ad tests in parallel. Each offline ad medium is testable locally. Then you can scale up to regional or national campaigns if warranted. Seek out remnant ad inventory for the highest discounts. You can use remnant ads for both initial tests and scaling this channel. The downside is less targeting ability, both in terms of demographics and timing. Use unique codes or Web addresses to track the effectiveness of different offline ad campaigns. Make sure before you set up tests or campaigns that you can trace conversions back to specific offline ads.

CHAPTER TWELVE

Search Engine Optimization (SEO)

Almost all Internet users turn to search engines for answers. Search engine optimization (SEO) is the process of improving your ranking in search engines in order to get more people to your site. As Rand Fishkin, founder of the popular SEO software company Moz, told us: At its base, SEO is starting with a content strategy and finding a way to attract relevant visitors through search engines. You have to intelligently design this kind of [content] and make sure search engines can find and rank that content. SEO allows you to amplify all of the good things you’re already doing in other traction channels (publicity, unconventional PR, content marketing) and use them to bring in more customers from search engines. Though competitive, SEO can scale well at any phase, often at low cost.

SEO STRATEGY The most important thing to know about SEO is that the more high-quality links you have to a given site or page, the higher it will rank. If you’re new to SEO, we recommend starting with the Moz Beginner’s Guide to SEO to learn the fundamentals. These include making sure you’re using the keywords you want to target appropriately on your pages, like in your page titles and headings. Instead of these more obvious basics, we’ll head straight to specific strategies and tactics. In SEO, there are two high-level strategies to choose from: fat head and long tail. Let us first explain these strange names. Consider all the searches that people make, sorted by the number of times that search is made. At the top are one- and two-word searches like “Dishwashers,” “Braves,” and “Facebook.”

 They make up about 30 percent of all searches. The other 70 percent are longer searches that don’t get searched as much, but in the aggregate add up to the majority of searches made. If you graph all these searches by the number of times they are made, the first 30 percent get clumped at the front and the last 70 percent make up a long tail, because many of those are searched only a few times. These latter searches are called “long-tail” keywords because they make up this long tail. Oppositely, those searches that are searched a lot and clumped at the front are called “fathead” keywords. A fat-head strategy involves trying to rank for search terms that directly describe your company. For example, a toy company that specializes in wooden toys might try to rank for “wooden toys.” These are all fat-head keywords. On the other hand, a long-tail strategy involves trying to rank for more specific terms with lower search volumes. That same toy company might try to rank for searches in that long tail like “poisonous chemicals in wooden toy blocks” or “wood puzzles for 3 year olds.” Again, even though these searches have lower volumes, in the aggregate they account for the majority of all searches. When determining which strategy to use, you should keep in mind that the percentage of clicks you get drops off dramatically as you rank lower on a search results page. Only about 10 percent of clicks occur beyond the first ten links, so you want to be as high up on the first page as possible. Your ability to rank high on the first page should be a deciding factor in deciding whether to pursue a particular SEO strategy.

Fat-head Strategy To determine if a fat-head SEO strategy is worthwhile, first research what terms people use to find products in your industry, and then see if the search volumes are large enough to move the needle. Google currently provides a useful tool for this process called Keyword Planner (part of Google AdWords). You can type in search terms that describe your products and then see the search volumes for these terms. You can also get keywords by looking at your competitors’ Web sites and seeing what words they use in their home page titles and headers. You want to find terms that have enough volume such that if you captured 10 percent of the searches for a given term then it would be meaningful. You don’t want to spend resources ranking for a fat-head term that gets only two hundred searches per month.

Sometimes you cannot find any good terms because your product category is so new that there is no search demand for it yet. Rand used Uber as an example: The problem with Uber is that there’s not a lot of search demand for it. I mean nobody searches for “alternatives to taxicabs that I can hire via my phone.” It’s just not a thing. And this is a problem with a lot of startups that are essentially entering a niche where nothing had existed previously. . . . There’s just not search volume. The next step is determining the difficulty of ranking high for each term. Using tools like Open Site Explorer, examine the number of links competitors have for a given term. This will give you a rough idea of how difficult it will be to rank high. If a competitor has thousands of links for a term you want to rank for, just realize it will likely take lots of focus on building links and optimizing for SEO to rank above them. Next, take steps to narrow your list of targeted keywords to just a handful. Go over to Google Trends to see how your keywords have been doing. Have these terms been searched more or less often in the last year? Are they being searched in the geographic areas where you’re seeking customers? You can further test keywords by buying SEM ads against them. If these ads convert well, then you have an indication that SEO could be a strong growth channel using these keywords. Now you’re at the implementation stage of fat-head strategy. Orient your site around the terms you’ve chosen. If you are an accounting software site and “small business accounting software” is your main term, include that phrase in your page titles and home page. Finally, get other sites to link to your site, ideally using the exact terms you want to rank for. For example, an article may read something like “XYZ Company releases version of small business accounting software” (where an underline denotes the link). Exact matches give you a significant boost, and also links from higher-quality sites matter more.

Long-Tail Strategy

The majority of searches conducted through search engines are “long-tail” searches. These are longer terms that are highly specific—things like “gluten free for arthritis” or “private search engine.” Individually, searches for these terms don’t amount to much: together, though, they make up 70 percent of all searches. Because it is difficult to rank high for competitive (fat-head) terms, a popular SEO strategy for early-stage startups is to focus on the long tail. With this strategy, you bundle long-tail keywords together to reach a meaningful number of customers. As with the fat-head strategy, the Google Keyword Planner is the first way to evaluate whether a long-tail strategy may be effective for you. But this time you are seeking information on more specific, long-tail terms. What are search volumes for a bunch of long-tail keywords in your industry? Do they add up to meaningful amounts? A second way to evaluate a long-tail strategy is to look at the analytics software you have on your site (such as Google Analytics or Clicky). These applications will tell you some of the search terms people are using to get to your site right now. If you are already naturally getting a significant amount of traffic from long-tail keywords, then this strategy might be a good idea. If you do not have any content that is drawing people to your site via longtail keywords, you have two choices. First, you could create some Web pages, and then after a few months check your analytics. Second, you could look at competitors’ Web sites to determine whether they are getting meaningful longtail SEO traffic. Here are signs that they are: They have a lot of landing pages. You can see what types of pages they are producing by searching site:domain.com in a search engine. For example, if I wanted to see how many landing pages Moz has created targeting long-tail keywords, I could search site:moz.com and get a sense of how many landing pages they have. Check out Alexa search rankings and look at the percentage of visitors your competitors are receiving from search. If you look across competitors and one site receives a lot more visitors from search than others, you can guess they are using some kind of SEO strategy.

If you proceed with a long-tail SEO strategy, its success will boil down to your being able to produce significant amounts of quality content. Patrick McKenzie, founder of Bingo Card Creator and Appointment Reminder, told us how he approaches doing so: You build a machine that takes money on one end and spits rankings out the other. For example, with Bingo Card Creator I pay money to a freelancer to write bingo cards with associated content about them that get slapped up on a page on the Web site. Because those bingo cards are far too niche for any educational publisher to target, I tend to rank very well for them. For ten to twenty dollars per search term, you can pay someone to write an article that you won’t be embarrassed to put on your Web site. For many SaaS [Software as a Service] startups, the lifetime value of a customer at the margin might be hundreds or thousands of dollars. So they [articles and landing pages] don’t need much traffic at all on an individual basis to aggregate into a meaningful number to the business. The reason my business works is fundamentally because this SEO strategy works phenomenally well. In our interview, Patrick told us about his Owls of East Asia bingo cards. He uses a landing page that specifically discusses owls of East Asia and has a custom bingo card template just for this long-tail topic. This page has resulted in about $60 worth of business over three years. With a $3.50 content creation cost, it was an investment worth making. It works because few other sites on the Internet have a page specifically for people searching “owls of East Asia bingo.” In Patrick’s case, hundreds of these sorts of $3.50 investments with $60 to $100 returns add up to big profits. Patrick has built up a series of subpages on his site, each of which targets a bucket of keywords he wants to rank for. For example, there’s the bingo card category for “plants and animals.” This category includes pages like “dog breeds bingo,” “cat breeds bingo,” and (of course) “owls of East Asia bingo.” For each of these subpages, he hired a freelancer to research the term and create a unique set of bingo cards and associated landing pages.

You can implement this tactic by designing a standard landing page with some basic content and a simple layout structure. Then use oDesk or Elance to find freelancers willing to churn out targeted articles for long-tail topics that your audience is interested in. Another way to approach long-tail SEO is to use content that naturally flows from your business. To evaluate whether you could use this tactic, ask yourself: what data do we collect or generate that other people may find useful? Large businesses have been built this way: Yelp, TripAdvisor, and Wikipedia have all gained most of their traffic by producing automated long-tail content. This tactic was also the main channel for Gabriel’s last startup, Names Database. When people searched for old friends and classmates, they would come across the Names Database page of the individual they were searching for. These pages were automatically generated from the data that was naturally gathered by the product. After search engines indexed them, these pages sent a great deal of organic traffic from individuals doing long-tail searches for individual names. Sometimes this data is hidden behind a login screen and all you need to do is expose it to search engines. Other times you may need to be more creative about aggregating data in a useful manner. For example, if you want to reach individuals searching for “foreclosed homes,” creating landing pages based on geography might work well. This means generating pages matching searches like “recently foreclosed homes in Queens, New York City.”

SEO TACTICS Whether you pursue a fat-head or long-tail strategy, SEO comes down to two things: content and links. The more aligned your content is with the keywords it’s targeting, the better it will rank. Similarly, the more links you can get from credible and varying sources, the better your rankings. Getting links is often more difficult because it involves people outside of your company. Here are some ways to build links: Publicity—when you are covered by online publications, reporters will link to your Web site. Product—with some products, you can produce Web pages as part of your product that people naturally want to share. 

A great example is LinkedIn profile pages. Content marketing—creating strong, relevant content that people want to read, and thus share. Widgets—giving site owners useful things to add to their sites, which also contain links back to yours. There is a difference between creating amazing content that spreads like wildfire and hiring freelancers to write boilerplate articles for long-tail keywords. Both are valid strategies (and can work well in tandem), but there is a big difference in quality. The high-quality content is useful in natural link building, especially for fat-head strategies. Rand suggests using infographics, slideshows, images, and original research to drive links, as these are all things people naturally share. Since the end goal is to get links, you’ll want to specifically target people who will link back to you. This group of people will vary depending on the product, but in general people who run blogs are big social sharers. Reporters are usually good targets as well. Remember, links are the dominant factor in a site’s ranking for a given term. Open Site Explorer can tell you how many links you are getting and where they are coming from. You can also look at your competitors’ link profiles to get ideas of other places to target for link building. In SEO, there are a few big don’ts. The biggest: don’t buy links. Buying links is against search engine guidelines and companies get heavily penalized for doing so. Similarly, trying to trick search engines in any way can lead to serious ranking penalties. These sketchy tactics are referred to as “black-hat,” as opposed to “whitehat” or “gray-hat” (on or close to the line). You want to stay squarely in the “white-hat” arena. To be clear, black-hat tactics can work in the short term, and therefore can seem quite attractive. However, it is hard to build a long-term sustainable business on them because at some point search engines will crack down on them and you’ll lose traffic due to penalization.

CONCLUSION SEO is one of the cornerstones of what is commonly referred to as “inbound marketing.” Inbound marketing brings customers inbound, from things like social media and SEO. Rand told us that Moz gets 85 percent of its customers inbound. Mike Volpe from HubSpot said something similar: Today we have 30 people in marketing and 120 in sales, all based in Cambridge, MA (no outside sales) and we attract 45–50k new leads per month, 60–80 percent of which are from inbound marketing (not paid). The inbound leads are 50 percent cheaper and close 100 percent more than paid leads. My personal experience and industry knowledge tells me that most other SaaS [Software as a Service] companies get more like 10 percent of their leads from inbound marketing, and generate 2–5k leads per month in total, whereas we get 70–80 percent of our leads from inbound and we generate 45,000+ new leads per month.

TARGETS Find search terms that have enough search volume to move the needle for your company. If you can’t find enough search volume, or can’t rank high for those terms, SEO won’t be a great strategy for your business. If you identify some terms that could work, you can further qualify them by running search ads against them to test whether they actually convert customers. Generate long-tail landing pages by using cheap freelancers. Or, if your product can naturally produce good long-tail content, use it to create the landing pages yourself. Focus on how you will build links. Whether you pursue a fathead or long-tail strategy, SEO comes down to two things: content and links. Link building is often the more challenging of the two. Creating amazing content is one way to quickly build links. Avoid “black-hat” SEO tactics that violate search engine guidelines, especially buying links. These banned tactics will eventually come back to bite you.

CHAPTER THIRTEEN

Content Marketing

Think back to the last few Web sites you’ve used and take a look at their blogs. In all likelihood, they’re infrequently updated and have few comments or, worse, are frequently updated and an avalanche of boring. Compare that experience to reading a well-known company blog like those of Moz, Unbounce, or OkCupid. They write posts that receive hundreds of comments, lead to major publicity, and result in thousands of shares. This massive engagement leads to massive growth. In fact, for each of these companies its blog was at one time its largest source of customer acquisition. For this traction channel, we spoke with two successful entrepreneurs who have very different approaches toward content creation. Rick Perreault, founder and CEO of Unbounce, told us how Unbounce started using its blog as a marketing platform the day it started building its application. In fact, Unbounce began blogging a year before it even had a product! Unbounce’s blog raised its profile in the online marketing industry and is still its main source of traction. On the other end of the spectrum, we talked with Sam Yagan, cofounder of OkCupid. The popular online dating service launched in 2004 but didn’t start seriously blogging until 2009. Though it focused on other traction channels early on, OkCupid really started to take off when it focused on content marketing.

CASE STUDY: UNBOUNCE As we discussed earlier, many startups fall into the product trap—building a product before thinking about distribution. Unbounce, a company that provides simple landing page creation software, successfully avoided this trap. Literally from day one, founder Rick Perreault began sketching out the product features for Unbounce on the company blog. Rick’s first hire was actually a full-time blogger! As he said:

If we had not started blogging at the beginning the way we did, Unbounce would not be here today. . . . Our content still drives customers. Something we wrote in January 2010 still drives customers today. Whereas if I had spent money on advertising in January, that’s it. That money is spent. If you invest in content, it gets picked up by Google. People find it, they share it, and it refers customers almost indefinitely. By the time we launched in the summer of 2010, we were doing twenty thousand unique visitors per month to the blog. . . . It was up and running for almost a year before we launched. Now our blog is our primary source of customer acquisition. People write about Unbounce. Other people tweet about our posts. . . . Our blog is the centerpiece of all our marketing. This blog-from-the-beginning approach allowed Unbounce to launch with an email list over five thousand strong. This wasn’t your typical startup product launch. The Unbounce team relied heavily on social media to drive readers to their blog. After every post they wrote, they’d ping influencers on Twitter asking for feedback. They also engaged with their target customers by writing useful answers on targeted forums like Quora. Though actions like these may not scale, they’re okay when getting started because you’re building toward a point where your content will spread on its own. That’s exactly what happened with Unbounce, and eventually its content started spreading more organically. Unbounce further capitalized on its blog traffic by giving away free infographics and e-books to grow its email list. This meant that when it finally opened up its product beta, Unbounce could email its list and launch successfully. Getting to this point wasn’t as easy as it might seem. Even with awesome biweekly posts about online marketing, it took six months for the Unbounce team to see significant results from their blog. However, once they captured this significant audience, they never looked back.

CASE STUDY: OKCUPID

OkCupid is one of the most popular online dating sites in the United States and was acquired by Match.com for a reported $50 million. OkCupid approached its blog differently from Unbounce, making it the core traction channel only after five years of being in business. Sam Yagan told us that once they launched the blog in 2009, growth increased rapidly. Much like Unbounce, OkCupid’s blog was the focal point of all its marketing activities. Unlike Unbounce, the OkCupid team wrote longer posts with less frequency. Each of OkCupid’s posts took a month to write and drew on the data they had from studying the usage patterns of their members. They intentionally wrote controversial posts (e.g., “How Your Race Affects the Messages You Get”) to generate traffic and conversation. Because OkCupid was a free online dating site, it couldn’t afford to pay much to acquire customers—in fact, it never did any sort of paid advertising. This meant that traction channels without per-user acquisition costs (e.g., publicity, content marketing, SEO, viral marketing) needed to drive all of its growth. Interestingly, the OkCupid team received much more organic publicity after launching the blog than they did when working with PR firms. CNN, Rachael Ray, The New York Times, and many other media outlets were interested in the blog topics they covered. Their blog also had major SEO benefits. When they launched it, they were nowhere near the top of search results for the term “online dating.” About a year later, they were the first result for that highly competitive term.

CONTENT MARKETING TACTICS The most common hurdle in content marketing is writer’s block. To overcome it, simply write about the problems facing your target customers. Presumably, you know more about the industry you’re working in than your potential customers. This means that you should be able to provide insight on subjects they care about. Every single industry has issues people struggle with. In Unbounce’s case, they wrote posts about landing page optimization, PPC (pay-per-click) conversions, and so on. OkCupid’s posts, such as “Exactly What to Say in Your First Message,” addressed the concerns of online dating users in an entertaining way.

Unbounce found that infographics are shared about twenty times more often than a typical blog post and have a higher likelihood of getting picked up by other online publications. For example, in 2012 Unbounce released the “Noob Guide to Online Marketing.” This infographic drove tens of thousands of downloads and thousands of paying customers. One year later it was still shared on Twitter about once an hour. Most marketers fail to realize that quality is no substitute for quantity. Both Rick and Sam made it a point to say there’s no shortcut to creating quality content. If what you’re writing isn’t useful, it doesn’t matter how hard you try to spread your content on Twitter. It just won’t spread. The secret to shareable content is showing readers they have a problem they didn’t know about, or at least couldn’t fully articulate. A solution is nice, but it’s not as good for drawing in readers as showing them they’ve been going about some aspect of their life all wrong. In the early days, it’s unlikely that your blog will see much traffic, regardless of content quality. Even Unbounce was receiving less than eight hundred monthly visits after six months of consistently putting out good content. It took awhile longer for the blog to grow to twenty thousand monthly visitors. Fortunately, there are ways to build momentum faster. Unbounce engaged in any online forum where conversations were taking place about online marketing, and did its best to contribute. It was particularly successful reaching out to influential people on Twitter. It would simply follow marketing mini-celebrities and ask them for feedback on recent posts. One of the best methods of growing your audience is guest posting. This tactic is especially powerful in the early days when you essentially have no audience to work with yourself. Unbounce started doing guest posts on other popular blogs after just three months of blogging on its own. As you move forward, monitor social mentions and use analytics to determine which types of posts are getting attention and which are not. Many bloggers are surprised at which posts do well. That is a good reason to keep a regular content schedule: it can be hard to anticipate what exactly will resonate with your audience.

CONCLUSION

Quick: name three venture capitalists or ask your startup friends to do so. Many people will mention Fred Wilson, Brad Feld, or Mark Suster. Why? Because they have popular blogs. There are plenty of other great venture capitalists who do not have similar brand recognition. One of the best things about this traction channel is how it positions you as a leader in your space. Unbounce and OkCupid are both great examples of how a popular blog can make a company a recognized industry leader in a highly competitive space. Recognition as a primary voice in an industry leads to opportunities to speak at major conferences, give press quotes to journalists, and influence industry direction. It also means your content is shared many more times than it would be otherwise. For Unbounce, some of the biggest benefits of having a strong company blog came in the form of comarketing opportunities. When they were just starting out, they tried contacting popular companies to arrange partnerships. These types of business development pitches were ineffective early on, but that changed after their blog started getting readership. Now, they have numerous integrations (including some major ones with companies like Salesforce), and a backlog of companies who want to work with them. Having a strong company blog can positively impact at least eight other traction channels—SEO, publicity, email marketing, targeting blogs, community building, offline events, existing platforms, and business development. When it works, it drives in customers like magic. Rick put it like this: [Our blog] drives search. It drives word of mouth. The blog is top of the funnel. People find the blog, and it’s attached to our Web site. We don’t market the blog, per se, but we’re constantly —several times a week—releasing content that gets shared and drives people to the blog.

TARGETS If you blog, dedicate at least six months to it. A company blog can take a significant amount of time to start taking off. Do things that don’t scale early on. Reaching out to individuals to share posts, for instance, is okay, because you’re building toward a point where your content will spread on its own. Contacting influential industry leaders (on Twitter, etc.), doing guest posts, writing about recent news events, and creating shareable infographics are all great ways to increase the rate of growth of your audience. Produce in-depth posts you can’t find anywhere else. You need to create quality content to succeed in this traction channel. There is no silver bullet, but a decent approach is to write about problems your target customers have. Another approach (not mutually exclusive) is to run experiments or use data from your company that leads to a surprising conclusion.

CHAPTER FOURTEEN

Email Marketing

If you’re like us, you have multiple promotions sitting in your inbox right now —coupons, referrals, sales pitches, and more. This is email marketing. Many companies (Groupon, JackThreads, Thrillist, Zappos) use email marketing as their core traction channel. Email marketing is a personal channel. Messages from your company sit next to email updates from friends and family. As such, email marketing works best when it is personalized. Email can be tailored to individual customer actions such that every email communication is relevant. For this channel, we interviewed Colin Nederkoorn, the founder and CEO of Customer.io, a startup that makes it easy for companies to send email based on actions their customers take. Colin explained: If you’re running a real business, [email] is still the most effective way to universally reach people who have expressed interest in your product or site. For that, it really can’t be beat. Email marketing can be used for all stages of the customer life cycle: building familiarity with prospects, acquiring customers, and retaining the customers you already have.

EMAIL MARKETING FOR FINDING CUSTOMERS Before we dive in, let us give you a warning. Some companies will buy email lists to send bulk, unsolicited email. That is the very definition of spam. Spam makes recipients angry, hurts future email deliverability efforts, and harms your company in the long run. We do not recommend it.

Luckily, there are many legitimate ways to acquire customers using email. We urge you to build an email list of prospective customers through your other marketing efforts. This is useful whether you end up focusing on this channel or not because a list of interested prospects is an asset that you can draw on for years. Traction channels such as SEO or content marketing can help you build your email lists. At the bottom of your blog posts and landing pages, simply ask for an email address. Many companies require an email address for people to access premium content, such as videos or white papers. In our interview with Rick Perreault of Unbounce, he stated that this tactic was the single biggest driver of its email list growth. Another popular approach to building an email list is creating a short, free course related to your area of expertise. These mini-courses are meant to educate potential customers about your problem space and product. At the end of the course you put a call to action, such as asking people to purchase your product, start a free trial, or share something with their friends. In addition to your own email list, consider advertising on email newsletters complementary to your product. Many email newsletters accept advertisers, and if not, you can contact them directly and ask for a special arrangement.

EMAIL MARKETING FOR ENGAGING CUSTOMERS Customer activation is a critical and often-overlooked component of building a successful product. “Activating” a customer means getting them to engage with your product enough that they are an active customer. For Twitter, it specifically means sending out a tweet or following five people. For Dropbox, it means successfully installing the application and uploading at least one file. As you would expect, improving your activation rates can have a significant effect on your business. After all, if a customer never gets the value of your product, how can you expect them to pay for it, or recommend it to others? Email marketing is a great way to improve your customer activation rates. A popular approach is to create a sequence of emails that slowly exposes your new customers to the key features in your product. Instead of throwing everything at them right away, you can email them five days after they’ve signed up and say, “Hey, did you know we have this feature?” As Colin explains:

[Y]ou create the ideal experience for your users when they sign up for your trial. You then create all of the paths they can go down when they fail to go through the ideal experience. And you have emails in place to catch them and help them get back on that [ideal] path. Let’s take Dropbox as an example. If you create an account but never upload a file, you are not active. Maybe you signed up for the site but got busy and forgot about it. When this happens, Dropbox automatically emails you, reminding you to upload a file. With these targeted emails, Dropbox has increased the chance that you will return to the product and become an active user. For these emails, you should determine the steps absolutely necessary to get value from your product. Then create targeted emails to make sure people complete those steps. For those who fail to complete step one, create a message that automatically emails them when they’ve dropped off. Repeat this at every step where people could quit, and you will see a major uptick in the number of people finishing the activation process. With tools like Vero and Customer.io, automating these messages is easy. For example, you can use these tools to send an email to people who have not activated their account within three days of signing up for a free trial. You can also use initial emails to get customer feedback. Colin sends each new Customer.io signup an automated, personal email thirty minutes after they sign up. Here’s the email: Subject: Help getting started? Hey {{ customer.first_name }}, I’m Colin, CEO of Customer.io. I wanted to reach out to see if you need any help getting started. Cheers, Colin He mentioned that the email receives about a 17 percent reply rate, which is fantastic as far as automated emails go. It opens the channel of communication between Colin and his customers. Through these replies he’s learned a great deal about what wasn’t working in the product, which has led to many improvements.

EMAIL MARKETING FOR RETAINING CUSTOMERS For many businesses, email marketing is the most effective channel to bring people back to their sites. Take Twitter as an example. If you’re an active Twitter user, think of every email you’ve ever gotten from them about someone mentioning you in a tweet, a friend of yours who just signed up, or their weekly digest of popular tweets you may have missed. Each one of those emails is meant to keep you active on Twitter. Your retention emails will depend on the type of product you have. For example, if you have a social networking product, you could send a simple email to customers who haven’t signed in for two weeks. Dating services often showcase profiles or mention unread messages. More business-oriented products usually focus on reminders, reports, and information about how you’ve been using and getting value from the product. For infrequently used products, email marketing can be the primary form of customer engagement. Mint sends you a weekly financial summary that shows your expenses and income over the previous week. This keeps its product at the forefront of customers’ minds and allows it to provide value even if they aren’t always signed in. BillGuard, a service that monitors your credit cards for suspicious transactions, sends you a similar report every month. Email marketing is also one of the best channels to surprise and delight your customers. Brennan Dunn of Planscope (a project planning tool for freelancers) sends a weekly email to his customers telling them how much they made that week. Who wouldn’t want to get an email like that? Any sort of communication telling your customers how well they’re doing is likely to go over well. Patrick McKenzie, whom we interviewed for SEO, calls this the “you are so awesome” email. Some companies send emails that showcase your previous engagement with the product. For example, photo sites will send you pictures you took a year ago. These emails achieve both goals: they often make you feel good on an emotional level, and also invite you to come back and upload more pictures.

EMAIL MARKETING FOR REVENUE

Groupon and many other companies use email to generate hundreds of millions of dollars in revenue. Patrick said his email subscribers were seventy times more likely to buy one of his courses than those from other traction channels (targeting blogs, SEO, and content marketing). A common way to drive revenue through email marketing is sending a series of emails aimed at upselling customers. As an example, WP Engine, a WordPress hosting company, uses such a campaign to get customers on one of their premium plans. They’ve built a WordPress blog speed tester tool (at speed.wpengine.com) where interested prospects can enter their site URL and email address to get a free report about their site’s performance. Over the course of a month, WP Engine will then send that prospect an email course about WordPress speed and scalability—three quick ways to improve your site speed, why hosting is important for business, etc. Near the end of this mini-course, WP Engine will make a pitch to sign up for its premium WordPress hosting service. This seven-email sequence leads to a better conversion rate than driving potential people to a sales landing page. In fact, many companies like WP Engine now use advertising to drive leads to a landing page where they ask for an email rather than a sale. They then will use email marketing to sell a prospect over the course of a month or so. When WP Engine has prospects they know are not ready to convert, they put them on a different list where they send them less frequent (monthly) emails with relevant content. 

When it later comes time for these prospects to go looking for premium WordPress hosting, you can guess where they go. Email retargeting is another tool you can use for revenue. For example, if one of your customers abandoned a shopping cart, send her a targeted email a day or two later with a special offer for whatever item she left in the cart. Targeted emails will always convert better than an email asking for a sale out of the blue. For feature-based freemium products, emails that explain a premium feature a customer is missing out on can have high conversion rates. For example, if you run a dating Web site, you can explain that upgrading to a premium plan will lead to more dates. If you have a subscription product, ask them to upgrade to annual billing, which guarantees they will not cancel within the next year. Similarly, if you run a scaled pricing business (e.g., you pay $9/month for five users, $20/month for ten users, and so on), you can set up special emails for customers nearing their plan limits and ask them to upgrade. When you’re about to run out of Skype credits, Skype will email you asking you to re-up or upgrade to a subscription service.



EMAIL MARKETING FOR REFERRALS Due to the personal nature of email, it is also excellent for generating customer referrals. If a friend emails you to tell you about a new product she is enjoying, you’re far more likely to try it than if you saw her mention it briefly on Facebook. Groupon generates referrals by giving people an incentive to tell their friends about discounts. Unless a certain number of people have purchased a Groupon, the discount is not valid. Thus, as someone who wants 50 percent off their meal at Cheesecake Factory, we will happily email our friends so the deal happens. This kind of referral program was a major growth driver for Dropbox as well. In order to get more free space, users send referral emails asking their friends to check out Dropbox. If a friend signs up, both people get extra free space. This referral program built on top of email has been Dropbox’s biggest growth mechanism and has led to tens of millions of users. Some consumer apps, and even some B2B companies like Asana, will ask their customers to import their address books to share the site with their friends. This marketing tactic touches elements of both viral and email marketing and can be extremely effective. In fact, many of the viral products you know of (Hotmail, Facebook, LinkedIn) grew by cleverly using email marketing.

EMAIL MARKETING TACTICS Deliverability is a key factor in email. For many technical reasons, your email messages may not be reaching their intended recipients. Most companies use an email-marketing provider like MailChimp or Constant Contact to send their emails. These companies help ensure deliverability. As with other traction channels, testing is essential to maximize this channel’s impact. Effective email campaigns A/B test every aspect: subjects, formats, images, timing, and more. Timing is especially relevant to get higher open rates: many marketers suggest sending emails between nine a.m. and twelve p.m. in your customer’s time zone or scheduling emails to reach them at the time they registered for your email list (e.g., for people who signed up for your list at eight p.m., email them at eight p.m.). One of email’s strengths is that it’s a way to get feedback from your customers. One trick Colin told us about was not to send any email that comes from a “Noreply” email address (e.g., noreply@facebook.com). Intead, use that opportunity to send the automated email from a personal address and allow the recipient to reply with questions or problems they have. This can be great for support, for feature requests, and for upselling existing customers. Last, an effective email sequence will be meaningless if you don’t have great email copy. Copywriting is an art on its own, but we suggest checking out some of the resources and information that Copy Hackers provides. An email campaign can easily go from a waste of time to wildly profitable just by tweaking a few words and headlines.

TARGETS Personalize your email marketing messages. Email marketing is a personal traction channel. Messages come into your inbox along with email from your friends and family. Build an email list of prospective customers whether you end up focusing on this traction channel or not. You can utilize email marketing at any step of your relationship with a customer, including customer acquisition, activation, retention, and revenue generation. Set up a series of automated emails. Often called life cycle or drip sequences, this technique works best when the series of emails adapts to how people have interacted with your product. Use online tools to test and optimize email campaigns. These tools have built-in templates and A/B testing ability and will track open and click rates.

CHAPTER FIFTEEN

Viral Marketing

Viral marketing is the process of getting your existing customers to refer others to your product. You’ve seen examples of this traction channel in action any time a friend’s Pinterest post appeared in your Facebook feed, or whenever you’ve received an automated email from a friend telling you about a product. In the context of startups, literally “going viral” means that every user you acquire brings in at least one other user. That new user then invites at least one other user, and so on. This creates true exponential growth. Though difficult to sustain, it’s been the driving force behind the explosive growth of consumer startups like Facebook, Twitter, and WhatsApp. As great as your product may be, true viral growth is unlikely. However, this channel is so powerful that even if you can’t achieve exponential growth, you can often still get meaningful growth through viral marketing. When viral loops are working, customers sign up in great numbers at very low acquisition cost. Think about the numbers. Suppose that when your customers sign up, you get each of them to refer one new customer within the first week. You’ll go from ten customers to twenty that first week, and keep doubling every week thereafter without any additional marketing! That’s true viral growth. If only every other customer refers a friend—a result that’s really pretty good—it will take forever to get from ten to twenty on its own. 

But still, even in this case, viral marketing is helping you get two customers for every one you bring in. We will walk through this viral math in detail in this chapter. We interviewed Andrew Chen, founder of Muzy (an app with more than 25 million users) and one of the experts in the viral marketing world. According to Andrew, this traction channel is becoming increasingly important as Facebook, email, and app stores have emerged as “super-platforms” with billions of active users each. As a result, companies can go viral faster than ever before. Dropbox, Instagram, Snapchat, and Pinterest are great examples: they all leveraged virality through these super-platforms to acquire tens of millions of users in just a few years.


VIRAL MARKETING STRATEGY Viral marketing strategy begins and ends with viral loops. A viral loop in its most basic form is a three-step process: 1. A customer is exposed to your product or service. 2. That customers tells a set of potential customers about your product or service. 3. These potential customers are exposed to your product or service, and some portion become customers themselves. The process then begins again with this new set of customers. It’s called a loop because it repeats over and over again: as your customers refer other customers, those customers refer others, and so on. Though viral loops share the same basic structure, each company executes them differently. Dropbox’s loop is different from Pinterest’s, which is different from Skype’s. We’re going to describe the main kinds of viral loops and show you how companies have used them to succeed. The oldest form of virality occurs when your product is so remarkable that people naturally tell others about it—pure word of mouth. Word of mouth drove Facebook’s early growth among college students, before they started building in more explicit viral hooks (email invites, adding your friends via address books, etc.). Word of mouth also causes many movies, books, diets, and TV shows to take off. Inherent virality occurs when you can get value from a product only by inviting other customers. For example, if your friends don’t have Skype, the application is worthless. Apps like Snapchat and WhatsApp also fall into this category. This type of virality comes with the advantage of “network effects,” where the value of the network increases as more people get on it. That is, the more people who are on Skype, the more valuable it becomes. Other products grow by encouraging collaboration. In this case, the product is still valuable on its own, but becomes more so as you invite others. Google Docs is useful alone, but it is far more valuable when used collaboratively. 

This type of viral loop can take longer to spread if your customers don’t need to immediately collaborate. However, once they do, strong network effects kick in as the service becomes a central tool around which collaboration occurs. Another common case is to embed virality into communications from the product. Hotmail put “Get a free email account with Hotmail. Sign up now” as a default signature, and Apple similarly appends “Sent from my iPhone.” As a result, every message sent spreads the word about the product. Many software products do this with their free customers. MailChimp, Weebly, UserVoice, and Desk.com all add branding to free customers’ emails and Web sites by default, which can be removed by becoming a paying user. Products can also incentivize their customers to move through their viral loops and tell others about the product. Dropbox gives you more space if you get friends to sign up. Airbnb, Uber, PayPal, and Gilt give you account credits for referring the product to friends. Companies like reddit and YouTube have grown virally by using embedded buttons and widgets. On each video page, YouTube provides the code snippet necessary to embed a video on any Web site. You’ve probably also noticed such buttons for Facebook and Twitter on many Web pages: each button encourages sharing, which exposes the product to more and more people. Another type of viral loop leverages social networks to attract new customers to a product or service. In this case, a user’s activities are broadcast to his social connections, often more than once. If you’ve spent any time on Facebook, we’re sure you’ve seen your friends liking articles on other sites, playing songs on Spotify, or pinning content on Pinterest. It is instructive to think about how each of these types could possibly apply to your product or service. You can combine them as well. In fact, when you can get multiple forms working together, your viral loops will be that much stronger. 


Take Uber for example. Riding in a cab is often something you do with another person, meaning every new usage could demonstrate the product to a potential new customer. This is a form of inherent virality because people naturally find themselves together when taking an Uber. It also has parts of collaborative and incentivized virality because it is often useful to take an Uber together, both logistically and financially. To fully appreciate the viral loop concept and to understand whether viral marketing can work for you, you have to do a tiny bit of math. This viral math helps you quickly identify how close you are to getting traction through viral marketing, as well as which areas you need to focus on. The two key factors that drive viral growth are the viral coefficient and the viral cycle time. The viral coefficient, or K, is the number of additional customers you can get for each customer you bring in. The viral coefficient formula is: K = i * conversion percentage where K is the viral coefficient, i is the number of invites sent per user, and conversion percentage is the percentage of customers who sign up after receiving an invitation. For example, if your customers send out an average of three invites and two of those people usually convert to new customers, your viral coefficient would be: K = 3 * (2/3) = 2 If you were to add one hundred new customers in a week, you could expect them to send out three hundred more invites to your site and two hundred more customers to sign on with you as a result. That’s viral growth! Any viral coefficient above 1 will result in exponential growth, meaning that each new user brings in more than one additional user, creating true exponential growth. Any viral coefficient over 0.5 helps your efforts to grow considerably. There are two variables that affect your viral coefficient. The first is the number of invites (i) that each user sends out. If you can increase the average number of invites that each user sends out, say from one invite per user to two, you will double your viral coefficient. To push this number up as high as you can, consider including features that encourage sharing, such as posting to social networks. The second variable is the conversion percentage. If your product is being shared but not generating new customers, you won’t go viral. As with invites, if you double your conversion percentage (by doing things like testing different signup flows), you double your viral coefficient. The best signup flows reduce friction by making things simpler, such as cutting out pages or signup fields. For example, the conversion steps for a standard Web application often involve clicking on a link and filling out a form to create an account. In that case, you could break the conversion percentage into two percentages.

K = i * conversion percentage = i * click-through percentage * signup percentage When you break out conversion percentage in this way, you can determine the weakest part of your equation and focus on it. Your click-through percentage may be great, but your signup percentage may be subpar. This makes it clear what to focus on—the area where you can make the biggest positive impact. Viral cycle time is a measure of how long it takes a user to go through your viral loop. For example, if it takes an average of three days for invites to convert into customers, your viral cycle time is three days. Two viral loops with the same viral coefficient but different viral cycle times will end up with dramatically different outcomes: the shorter this time, the better. Viral cycle time explains the explosive growth of a company like YouTube, whose cycle times can occur in a matter of minutes—someone sees a video, clicks on it to go to the site, then copies the link and sends it to friends. Shortening your viral cycle time drastically increases the rate at which you go viral, and is one of the first things you should focus on improving if using this channel. To shorten it, create urgency or incentivize customers to move through your viral loops. Additionally, make every step in your funnel as simple as possible to increase the number of people who complete it. That’s why YouTube provides embed codes for each video: to make it simple for any user to add videos to her site or blog.

VIRAL MARKETING TACTICS To pursue this traction channel effectively, you need to measure your viral coefficient and viral cycle time from the start. Consider those measurements your baseline. Then you need to get your viral coefficient up and viral cycle time down to levels that yield enough new customers to produce steady growth for your business. We suggest running as many A/B tests as you can. Best practice suggests focusing for weeks at a time on one major area (say your signup conversion rate), trying everything you can think of to improve that metric, and then moving on to another metric that needs improvement as you run out of ideas. Andrew notes that this process can take time:

Even expert teams will take one or two engineers working two to three months, minimum, to implement and optimize a new viral channel to the point where it’s growing quickly without any ad spend. Once it gets going, though, it becomes easier to incrementally improve and grow the product. You need a strong strategy, and need to spend considerable time and resources to get something going. As you come up with your initial strategy for viral loops, create a simple dashboard of what needs to go up to be viral. Understand how new users end up helping you acquire more new users and do a lot of A/B tests (several per week) to try and improve the metrics. The best way to approach this testing is to map out every aspect of your viral loop. How many steps are in the loop? What are all the ways people can enter into the loop (landing pages, ads, invites)? Literally draw a map of the entire process and try cutting out unnecessary steps (extra signup pages, unnecessary forms or fields to fill out, etc.) and increase areas or mechanisms where customers can send out invitations. Doing so will improve your viral equation by increasing your invites sent and your conversion percentage. We interviewed Ashish Kundra, founder of Indian dating network myZamana, about the effectiveness of sharing mechanisms. He said that there are numerous viral mechanics you can build into your product, but to be really successful people need to like and repeatedly use the product. To drive usage, myZamana sends targeted emails to users based on actions they take on the site. As people use the site, their actions generate invitations to other users (e.g., “Mark liked you!”). The more people use the product, the more notifications are sent out. A nonuser’s first exposure to a product often occurs when a current user sends an invitation. The nonuser will then have to decide what to do with the invitation or whether it’s worth her time to even open it. Your goal in designing these invitations is to get potential customers to engage with the invite and follow the link (or take the next step) that the invitation contains. Invitations that work best are short and succinct. Sign up for the most viral services you can think of and you’ll see what we mean. Additionally, personal hooks really help.

People are overloaded with information about services they don’t use. This makes many people hesitant to sign up for a product they have not experienced themselves. However, viral growth is impossible if you have a low signup percentage because then you have no chance of a decent viral coefficient. For this reason, some companies allow people to use portions of their product without signing up. This allows potential customers to test-drive the product without making any sort of commitment. The pages that prospective customers land on from invitations are called conversion pages. Conversion pages work best when they use the same messaging as the invitations that preceded them. For example, if in the invitation you say so-and-so referred you to this product, you can put the exact same message on the conversion page. Understanding exactly why people are clicking on your links and signing up (e.g., curiosity, obligation, etc.) will help you think of better ways to improve your viral loop. Surveys, sites like UserTesting.com, and asking people directly are great ways to uncover this psychology. Here are some of the more common items to test and optimize: Button vs. text links Location of your call to actions Size, color, and contrast of your action buttons Page speed Adding images Headlines Site copy Testimonials Signs of social proof (such as pictures of happy customers, case studies, press mentions, and statistics about product usage) Number of form fields Allowing users to test the product before signing up Ease of signup (Facebook Connect, Twitter login, etc.) Length of the signup process (the shorter you can make the process, the higher your conversion percentage will be) First focus on changes that, if they worked, would result in a 5–10x improvement in a key metric. 

This could be something like an entirely new email auto-responder sequence, a new Web site design, or a new onboarding flow. Once you’ve made big changes, then optimize the smaller stuff. Almost no optimization is too small to test: even changing one word in a headline can have a significant impact. Because viral growth compounds, a 1 percent improvement can make a big difference over the long term. With all viral (or near-viral) growth, there will be subgroups of customers growing far more rapidly than your total customer base. We call these subgroups “viral pockets.” Figure out if you have viral pockets by calculating your viral coefficient on distinct subsets of your customers, such as those from a particular country, age group, or other trait. For example, you may be taking off in Indonesia while not doing as well in Australia. Once you find a viral pocket, you may want to cater to this group by optimizing text in their native language or some other way that will improve their experience. Since most viral loops are not self-sustaining, you need a constant stream of new customers entering your viral loop. This process is called “seeding.” When seeding new customers for your viral loop, you’re looking for people in your target audience who have not been exposed to your product. SEO and online ads are good, inexpensive candidates for seeding.

CONCLUSION Because of the potential to acquire free customers, many startups try to go viral. Andrew mentioned that he sees companies making the same mistakes: Products that aren’t inherently viral trying to add a bunch of viral features Bad products that aren’t adding value trying to go viral Not doing enough A/B tests to really find improvements (assume one to three out of every ten will yield positive results) Not understanding how users are currently communicating/sharing, and bolting on “best practice” strategies (Just add Facebook “like” buttons!) Not getting coaching/guidance from people who’ve already done it

Thinking about virality as a tactic rather than a deep part of a product strategy As he told us, the best way to figure out the right kind of loop to build is simple: copy those who have done it before. The easiest way, for a beginner—copy someone else’s viral loop until yours starts to work in a similar way. Copying someone else’s loop down to the detail is important, including text copy, etc. These are the things that drive performance. Make it something that the user wants to do, because it creates value from them. Skype with no contacts is useless—so by helping people import their address books and invite people, you’re doing them a service. Even if you cannot truly go viral, you can still use this channel to spur rapid growth by compounding your efforts on other traction channels. If you are getting a steady stream of new customers through other channels, build a viral loop to bring in more and more customers.

TARGETS Build a viral loop into the product. There are several types of viral loops, including word of mouth, inherent, collaborative, communicative, incentives, embedded, and social. Startups can combine and change types over time, but generally these loops need to be built into the product to work successfully. Shorten viral cycle time. The shorter this time, the more loops will occur and the faster you will grow. Look for viral pockets. You might already be viral in a subgroup of your customers. Find that subgroup and focus on it. More than in any other channel, test, test, test. Successful viral strategy involves constant testing, measurement, and trying new things. It is a numbers and creativity game. No test is too small, as small changes can have big effects over time. Viral loops that work well often have extremely simple components (forms, copy, email, etc.).


CHAPTER SIXTEEN

Engineering as Marketing

Your team’s engineering skills can get your startup traction directly by building tools and resources that reach more people. We call this traction channel “engineering as marketing.” You make useful tools like calculators, widgets, and educational microsites to get your company in front of potential customers. These tools generate leads and expand your customer base. In this chapter we present examples of companies like HubSpot and RJMetrics that have successfully used this underutilized channel for rapid growth.

ENGINEERING AS MARKETING STRATEGY HubSpot, a marketing automation software company, has reached tens of millions in revenue in a few short years. One key to its success is a free marketing review tool the company created called Marketing Grader. When you enter your site’s Web address into Marketing Grader, you get back a customized report about how well you’re doing with your online marketing (social media mentions, blog post shares, basic SEO). This tool is free and gives you valuable information. It also provides HubSpot with information they use to qualify you as a potential prospect. After all, someone who wants to evaluate the success of their site’s marketing is a good candidate for HubSpot’s main product. These are quality leads. We spoke with HubSpot’s founder, Dharmesh Shah, about Marketing Grader. His story provides insight on where ideas for engineering as marketing tools come from: The early story of [Marketing] Grader is interesting. There were only three people at HubSpot at the time. 

My cofounder and I would regularly “sell” (in the early days, a lot of those sales calls were with friends, and friends of friends). One of the initial steps in the sales process was for me to get a sense for how good a given company’s Web site was at inbound marketing. My cofounder [Brian Halligan] would constantly send me Web sites he wanted me to take a look at so we could determine if they were a good fit. After a few days of this, I got tired of going through the manual steps (look at Alexa, look at their page titles, check out their domain, etc.). So I built an application to automate that process for me. On a related note, I had also started angel investing at the time, and I used the same process to assess the marketing savviness of potential startups I was considering investing in. Once the app was built (it didn’t take more than a few days for the initial version), I thought it might be useful for other people, so I registered “websitegrader.com” and made the app available publicly. We eventually started collecting email addresses in the app, and kept iterating on it. Since HubSpot launched Marketing Grader, more than 3 million sites have used it. Dharmesh said that it accounts for a large portion of the fifty thousand– plus leads HubSpot gets each month. Marketing Grader is so powerful for HubSpot because it precisely serves the needs of its target audience. It’s a low-friction way to draw leads into the HubSpot sales funnel. Engineering as marketing is particularly effective for HubSpot because Marketing Grader complements its primary product so well. Another company that nails engineering as marketing is Moz, the leader in SEO software. Two of its free SEO tools, Followerwonk and Open Site Explorer, have driven tens of thousands of leads for Moz. Like Marketing Grader, each solves a problem that an ideal Moz customer has. Followerwonk allows people to analyze their Twitter followers and get tips on growing their audience. Open Site Explorer allows people to see where sites are getting links, which is valuable competitive intelligence for any SEO campaign. A key feature of these tools is their ease of use: prospects simply go to the site and enter a domain name or Twitter handle. Once someone uses the tools, companies can begin to engage these potential customers through other traction channels like sales and email marketing.

WP Engine, a WordPress hosting provider, is another prime example of a company using this channel successfully. The hosting market is saturated with hundreds of hosting companies, yet WP Engine has cornered the market on highend WordPress hosting. This is thanks in part to its free tool that checks how fast your WordPress site loads. The WP Engine speed testing tool asks for only an email address in exchange for a detailed report about your site’s speed. It also gives you the option to opt in for a free mini-course about improving the speed of your blog. Once WP Engine has a user’s email, it sends her tips about improving her site speed and ends with a sales pitch. Dharmesh mentioned that it helps him to think of these tools as marketing assets with ongoing returns, rather than ads that result in a one-time boost. I think of free tools as content (albeit interactive content). At HubSpot, we really believe in marketing channels that have high leverage (i.e., write it or build it once—and get value forever). As such, we take a very geeky and analytical approach to marketing. We think of each piece of content (blog article, app, video, whatever) as a marketing asset. This asset creates a return—often indefinitely. We contrast that to buying an ad, which does not scale as well. When you advertise, the money you’re spending is what drives how much attention you get. Want more clicks? Spend more money. Contrast this to inbound marketing whereby the cost of producing a piece of content is relatively constant. But, if it generates 10x more leads in a month, your marginal cost for those extra leads is almost zero. Further, with advertising (outbound marketing), the traffic you get generally stops when you stop paying. With inbound marketing, even after you stop producing new content, the old content can still drive ongoing visitors and leads. The case for spending engineering resources on marketing becomes much stronger when you think about the resulting tools as assets. These tools have the potential to become a continual source of leads that make up the majority of your traction.

ENGINEERING AS MARKETING TACTICS One way to boost your efforts in this traction channel is to take advantage of cyclical behavior. Take Codecademy’s Code Year microsite, which launched at the beginning of 2012. Many people claim to want to learn how to code, but don’t follow through. Code Year addressed that issue by asking people to enter their email address to receive a free lesson about programming each week during 2012. More than 450,000 individuals signed up on CodeYear.com, nearly doubling Codecademy’s user base at the time. Similarly, Patrick McKenzie from Bingo Card Creator makes holidaythemed microsites for Halloween, Christmas, and other holidays. Since they are tied to the holidays, Bingo Card Creator can use them year after year. In Codecademy’s case, you could actually sign up for Code Year at any point during the year and still receive a lesson each week. When Gabriel wrote blog posts about search privacy, he got a big response from readers. After he engaged with commenters in social media channels it became clear that this is a topic that really resonated with people. Gabriel had the idea that a microsite might address people’s concerns more fully while simultaneously exposing his search engine, DuckDuckGo, to a broader audience. In 2011, Gabriel built such a microsite, DontTrack.us, which showed how Google tracks your searches and why that can harm you. The site raised awareness about these practices and spread virally. At the same time, readers learned that DuckDuckGo does not track people or store their personal information. Even after the initial wave of press and users, this microsite has been useful. As unpredictable events unfold (like news of NSA tracking) or predictable events reoccur (like Data Privacy Day), the traffic on the ever-present microsite persists. Users of DuckDuckGo often send the site to friends and family to explain the issues surrounding search tracking. The strategy worked so well that DuckDuckGo now has several microsites. To really maximize impact, put your microsites and tools on their own domains. This simple technique does two things. First, it makes them much easier to share. Second, you can do well with SEO by picking a name that people search often so your tool is more naturally discoverable. Chris Fralic, former head of business development at Delicious and Half.com, told us that creating a Delicious bookmark widget more than tripled the adoption of its social bookmarking product.

How many times have you seen Facebook, Twitter, and other sharing buttons on a site? For each of those widgets (e.g., Facebook, StumbleUpon, Google+, and Twitter buttons), a company used engineering resources to create a marketing tool that was embeddable on sites. These widgets drive engagement, traffic, and traction for these social platforms and the sites that use these tools.

CASE STUDY: RJMETRICS We spoke with Robert Moore, founder of RJMetrics (an e-commerce analytics company) to learn how they’ve used this traction channel to drive the majority of their leads and sales. As an engineer himself, Robert said he’s been using his engineering skills to bring in customers since he founded the company. For example, RJMetrics uses its own product to discover interesting trends on some of the most popular social media sites like Twitter, Tumblr, Instagram, and Pinterest. For example, one popular post was entitled “BuzzFeed Posts: What’s the Magic Number for ‘Best Of’ Lists?” These posts produced big traffic spikes when they launched, and led to a lot of long-tail opportunities as people discovered the content over time. Robert mentioned that they’ve been approached multiple times by writers for major publications who want to cite them as a source. OkCupid has a similar strategy that we covered under content marketing. Though its engineering efforts have certainly helped its content marketing, RJMetrics seriously began to use this channel when it started building tools and microsites. It owns and creates content for domains like cohortanalysis.com and querymongo.com, which contain keywords a potential RJMetrics customer would search for. In the case of querymongo.com, RJMetrics built a tool that translates SQL queries to MongoDB syntax (two database technologies). This is useful for developers or product managers starting to use MongoDB but who are still more familiar with SQL. It also drives leads for RJMetrics, because anyone doing data analysis is a potential customer for its main product. Querymongo is RJ’s highest-trafficked microsite and drives hundreds of leads per month. Robert said they look for high ROI on engineering time: if a few days of engineering time can drive hundreds of leads, that’s an investment they make whenever they can.

CONCLUSION Engineering as marketing creates lasting assets that can serve as the engine for your growth. Zack Linford, the founder of Optimozo and Conversion Voodoo, talked about how building tools can help with publicity and SEO, while also nailing down the core value proposition for your product. As he mentioned in our interview: Building noteworthy tools that your target audience finds useful is a solid way to gain traction that also pays dividends down the road by helping build your SEO. A simple roadmap to executing this technical strategy includes: Providing something of true value for free, with no strings attached. Making that offering extremely relevant to your core business. Demonstrating that value as quickly as possible. When you build valuable tools for prospective customers, you get more leads, a stronger brand, and increased awareness while also solving a problem for the individuals you want to target. Dharmesh mentioned that engineering as marketing is especially valuable because so few companies use it: I’m a big believer in using an engineering approach to marketing. But I’m biased (being an engineer myself). And yes, there are many other marketing channels available, but creating applications has a unique investment/return profile. Since it is considerably harder to build a very popular application, fewer people do it: so the “free apps” channel is usually less saturated. The best companies to use this apps-powered model are software companies. In this case, they can launch complementary apps—or subsets—for free. This not only creates value that draws people in, it also educates people on what the main product does.

Companies have a hard time using engineering resources for anything but product development. Any technical focus on something other than product seems wasted since engineering time is so expensive. As a result, most founders and product managers use all their engineering resources to build new features for a product or service that’s struggling to acquire customers. Don’t make the same mistake. Instead, consider using some of that engineering time to build a tool that moves the needle for your business.

TARGETS Create a stand-alone, low-friction site to engage potential customers. Make sure it naturally leads to your main offering. The case for spending engineering resources on marketing becomes much stronger when you think about these marketing tools as long-term assets that bring in new leads indefinitely after only a small amount of up-front investment. Look internally for site and tool ideas. Perhaps you have already started creating something for yourself that could also be used by potential customers? Another approach is to turn a popular blog post into a microsite. Make them as simple as possible. Single-purpose tools that solve obvious pain points are best. Put them on their own Web sites and make them easy to find, particularly through search engines.

CHAPTER SEVENTEEN

Business Development (BD)

Business development is like sales with one key distinction: it is primarily focused on exchanging value through partnerships, whereas sales primarily focuses on exchanging dollars for a product. With sales, you’re selling directly to a customer. With business development, you’re partnering to reach customers in a way that benefits both parties. We interviewed Chris Fralic, former senior business development executive at AOL, Half.com, eBay, and Delicious, and current partner at First Round Capital. Chris described how he used business development successfully at each of his startups (all of which were acquired). Many companies get traction through business development. Even Google, a company whose early success is often attributed only to a superior product, got most of its initial traction from two key partnerships. In 1999, it partnered with Netscape to be the default search engine for the popular Netscape Navigator Web browser. Google also reached an agreement with Yahoo!, then (and still) one of the largest Web properties in existence, to power its online searches. These two deals were critical to Google’s eventual success as the world’s largest search engine.

BUSINESS DEVELOPMENT STRATEGY Here are the major types of business development partnerships: Standard partnerships—In a standard partnership, two companies work together to make one or both of their products better by leveraging the unique capabilities of the other. One prominent example is the Apple/Nike partnership that resulted in the Nike+ shoe that communicates with your iPod or iPhone to track your runs and play music. Joint ventures—In a joint venture, two companies work together to create an entirely new product offering. These types of deals are complex and often require large investments, long periods of time, and (sometimes) equity exchanges. If you’ve ever bought a bottled Starbucks Frappuccino or Doubleshot Espresso, you’ve purchased a product that’s the result of the decade-long joint venture between Starbucks and Pepsi. Licensing—Licensing works well when one company has a strong brand that an upstart wants to use in a new product or service. To use another Starbucks example, the company lent its brand to an ice cream manufacturer that wanted to create Starbucks-flavored ice cream. Other startups, such as Spotify and Grooveshark, are forced into licensing agreements by the nature of their business. They can’t use music content without first licensing it from the record labels that own it. 

Distribution deals—In these deals, one party provides a product or service to the other in return for access to potential customers. Groupon’s core business is structured like this: it works with a restaurant or store to offer a discount to Groupon’s mailing list. Paul English, founder of Kayak, told us how a distribution deal with AOL was responsible for Kayak’s early traction. Through this partnership, Kayak used its search technology to power an AOL-owned travel search engine, which drove a lot of traffic right out of the gate. Supply partnerships—These types of partnerships help you secure key inputs, which are essential for certain products. As we’ll see, Half.com formed several to ensure that it had enough books to sell when it launched its online bookstore. Other supply partnerships include Hulu’s relationship with channel partners and deals between suppliers and companies like Walmart. Business development can drive some amazing outcomes for your startup. However, getting traction from this channel requires something that few companies do well: strategic thinking.

For business development to work well, you must have a clear understanding of your company objectives. What metrics do you need to hit in order to maximize your chances of success? How can partnerships help you get there? Good BD deals align with your company and product strategy and are focused on critical product and distribution milestones. These deals should help you hit your key metrics, whether growth, revenue, or product related. If you’re following Bullseye and Critical Path, you should have already defined your traction goal and the milestones you need to hit to reach it. This sounds simple and obvious, but in practice it is difficult. If a big company says it’ll work with you, but only in this other way that doesn’t strictly align with your traction goal, it is still extremely tempting. So tempting, in fact, that many startups will waste resources on these deals even though they are off their Critical Path. Business development requires discipline. Chris explained how he approached BD at Half.com: In the case of Half.com, there were three key things that we needed before we launched. Number one, the site had to work. We needed technology partners (back in the pre–Amazon Cloud days) to ensure people could actually use the site. Then there was inventory. We decided we needed one million books, movies, etc., at launch because that sounded like a nice big number. So my team and I worked on how we get product on the shelves. It was our job (prior to launch) to find inventory and get it listed on the site. The third was to get distribution. So we went out and created one of the early affiliate programs and did distribution and marketing partnerships. Once their objectives were identified, Chris and his team were able to form partnerships that allowed them to launch with one of the largest selections of books and movies anywhere at the time. Understanding a partner’s goals is key to creating a mutually beneficial relationship. Chris mentioned that startups are often focused on themselves and their needs without considering why a potential partner should make the deal: It’s research and learning and understanding your partner’s business before you start picking up the phone or sending emails. You need to understand what’s on their side of the table —what are their issues? To pick one example, we wanted to find books that were half off and find big quantities of inventory. So I started doing research to find big piles of used inventory and literally started calling people, asking people questions to understand how their business worked. We found out how products moved from book publishers out to the Borders of the world, where it came back to and where it accumulated when it didn’t sell. Once we found out where, we ended up getting partnerships with those people. I even flew to Atlanta and literally worked in a used bookstore for a day. You need to understand why a potential partner should want to work with you. What are their incentives? Just as you evaluate potential partnerships in terms of your core metrics, they will be doing the same. You should also seek out forward-thinking partners. Often that means finding an advocate inside a large company or working with a company that has done deals with startups in the past. Unfortunately, not every partnership will end up working. Thus, it makes sense to build a pipeline of deals. Charlie O’Donnell, partner at VC firm Brooklyn Bridge Ventures, suggests maintaining a large list of potential partners: Create an exhaustive list of all of your possible [partners]. Don’t ever list Condé Nast without listing every single other publisher you can think of. Make a very simple spreadsheet: Company, Partner Type (Publisher, Carrier, Reseller, etc.), Contact Person/Email, Size, Relevance, Ease of Use, and then a subjective priority score. That list should be exhaustive. There’s no reason why any company shouldn’t have fifty potential business development partners in their pipeline, maybe one hundred, and be actively working the phones, inboxes, and pounding the pavement to get the deals you need to get—be it for distribution, revenue, PR, or just to outflank a competitor. 

The latter is totally underutilized. If you go in and impress the top fifty folks in your space, it makes it that much harder for a competitor to get a deal done—because you’re seen as the category leader. Once you have a list of potential partners, send it to your investors, friends, and advisers for warm introductions. Chris Fralic suggests putting potential partners into buckets based on attributes. For example, you might categorize based on revenue numbers, distribution reach, or inventory capabilities. Then, at the end of this process, choose ten to twenty partners to focus your business development efforts on. As he said: People tend to get caught up on the names—“is this a known name”—and place more emphasis on that than what might be more important elements. So I’d encourage people to think about the attributes of your partner. So rather than saying “I want to go after XYZ brand,” say “We want to go after Internet retailers that are between 50 and 250 on the IR [Internet Retailer] 500—because that puts them in this kind of revenue range—and have a director of e-commerce.” Chris pitched all kinds of deals during his time at Delicious, the social bookmarking site. While he was there, he worked on deals with The Washington Post, Mozilla, and Wikipedia to integrate Delicious tags. Delicious approached its potential partners with a clear idea of how each of them would benefit from a partnership. For The Washington Post, the value proposition was to use Delicious’s social bookmarks to optimize content for social media. The Washington Post’s decision to partner was made even easier because it was a simple integration with very little downside. After Delicious integrated with The Washington Post, the number of sites interested in a Delicious extension skyrocketed because of the Post’s role as a media leader. It even made other partnerships, like the browser integration with Mozilla’s Firefox, possible: The even more transformative partnership we did at Delicious was with Mozilla. [Mozilla] ended up promoting the Delicious extension for the Firefox browser in a really big way when they did an upgrade to Firefox 2.0. The net of it was that when huge portions of their audience were upgrading, one of the first things they saw was the Delicious extension. It ultimately more than tripled our user base, just from that partnership. Chris stressed that you can be assured that not every deal will close; in fact, most deals won’t. For example, Delicious pursued an integration deal with Wikipedia that failed. Delicious was able to get traction from business development because it developed a large pipeline of potential deals.

BUSINESS DEVELOPMENT TACTICS Once you have a few partners you’re targeting, the real action starts. You start approaching potential partners with a value-focused proposition that outlines why they should work with you. Often these are larger companies. Brenda Spoonemore, former senior VP of interactive services at the NBA, put it like this: What do you have that they [big companies] need? You’re more focused than they are. You have an idea and you’re solving a problem. You’ve developed content or technology and you have a focus. That is very difficult to do at a big corporation. To approach these deals, you want to first identify the right contact at your target company. Some companies will have a business development department that handles partnerships, but—depending on the deal—it could be someone like a product director or C-level executive you want to engage with. The most important thing is to find out who is in charge of the metric you’ve targeted. If you think your partnership will help your partner sell more T-shirts, be sure to talk to the person most in charge of selling more T-shirts. If your product will help your partner get more satisfied power users, be sure to talk to the person most charged with pleasing power users. Just because you’re offering a Web site widget doesn’t mean the Web site team is the ideal set of stakeholders. 

Once she is identified, you want to try to get a warm introduction to that person. With each introduction, you should provide the mutual contact with an overview of your proposal that can easily be forwarded. Then be sure to follow up and set time lines for the next steps. Chris Fralic mentioned that it was key for him to get a meeting or phone call set up as quickly as possible—sometimes even on the same day. After the proposal stage comes negotiation of a term sheet. The key terms will usually be the lifetime of the deal, exclusivity, how payments work (if any), the level of commitment between partners, any guarantees in the deal, and revenue sharing agreements. Both Chris and Brenda suggest making the negotiation and term sheet as simple as possible—often just one page. The simpler you can make it to work together (and the fewer lawyers who need to get involved), the easier partnering will be. Keeping it simple is especially good advice for technology partnerships. With engineering time so valuable, do as much as you can to make it easy for potential partners to work with you. For example, Delicious built The Washington Post a custom interface for its readers to post bookmarks. Rather than involving the Post’s IT resources, Delicious made it easy to get it set up and going. Once a deal is completed, obviously you want to maintain a positive relationship with the new partner. It’s also important to understand the driving factors that got the deal accomplished. Chris suggests making a “how the deal was done” memo documenting how long it took to get to milestones, key contacts, sticking points, what interested the prospect enough to become a partner, and other factors that influenced the completion of the deal. These memos help companies determine what’s working in their process and what could be improved upon. Business development has historically been a high-touch process that includes a lot of personal interactions. Reaching out to partners, understanding their needs, and negotiating terms are all part of a traditional business deal. However, businesses recently have been moving to more low-touch business development. Low-touch BD utilizes tools like application program interfaces (commonly known as APIs), feeds, crawling technology, and embed codes to reach new distribution channels and grow your influence. These methods allow you to standardize your value proposition and get more deals done. Nevertheless, it still makes sense to land a few traditional deals first, and then transition to low-touch partnerships. Delicious’s first key partnerships with Mozilla and The Washington Post happened in the traditional way.

These partnerships generated significant traction for Delicious, so it made its API publicly available to the many sites that wanted to integrate with it. This required some up-front engineering work, but also meant that Delicious could now integrate with thousands of sites interested in leveraging its product. Other companies have pursued low-touch business development in a similar way. SlideShare makes all slideshows embeddable, Disqus has its easy comment system installation, and SoundCloud makes its music library freely accessible. Such integrations fuel growth and vastly increase the pool of potential partners for a company. However, just building a great API does not mean people will come and use it. Landing those first few partners through traditional means ensures that someone is getting value from working with your startup. Later, once you have more demand, you can start to standardize and simplify the partnership and integration process. Whether you are starting out or scaling to millions of customers, business development can move the needle in any product phase. Kayak is a perfect example of this. It got its first customers through a key partnership with AOL. Later, Kayak partnered with hotel chains, rental agencies, and other groups to extend its reach to new groups of customers. The right deal at the right time can propel your company to the next phase of growth.

TARGETS Pursue mutually beneficial partnerships. In a standard partnership, two companies work together to make one or both of their products better by leveraging the unique capabilities of the other. Other major types of BD deals include partnerships focused on joint ventures, licensing, distribution, and inventory. You need to understand why a potential partner might want to work with you. What are their incentives? Just as you are evaluating potential partnerships in terms of your core metrics, they will be doing the same. Focus on meeting your startup’s core metrics. Good business development deals align with your company and product strategy and are focused on strategic product and distribution milestones. Avoid deals that don’t directly align with your traction goal. Create a pipeline of deals you’re constantly working on. For initial testing, you can reach out to a variety of potential partners to gauge interest.

CHAPTER EIGHTEEN

Sales

Sometimes hand-holding prospects can be necessary to turn them into real customers. One effective way to do that is via sales. Sales is the process of generating leads, qualifying them, and converting them into paying customers. This channel is particularly useful for enterprise and expensive products because often customers desire some form of interpersonal interaction before a purchase. Scaling this traction channel requires you to design and implement a repeatable sales model, which we cover in this chapter.

SALES STRATEGY For consumer products, your first customers will likely come through channels other than sales—SEO, SEM, targeting blogs, and the like. When targeting bigger businesses, however, closing those first few critical customers can be significantly more challenging. We interviewed Sean Murphy, owner of customer development and sales consulting firm SKMurphy, to talk about how he helps startups get their first enterprise customers: Most of the time [their first customer] is going to be somebody they know or we know. For the most part, our clients are going into a market that they understand with technology that they have developed. We help them make a list of every project they’ve worked on and everyone they’ve worked with. They reach out and say, “Here is what we are doing: do you know somebody we should talk to that makes sense?” People who’ve developed expertise by working in a field for a while are typically able to get an initial meeting—cup of coffee or lunch, these kinds of things. 

Sometimes we encourage them to shift to a different market because we find out that the technology has more applicability and offers more value there. One of the first things we help them with is what we call a lunch pitch. This is a single piece of paper that has five to ten bullets and perhaps a visual that helps them focus the conversation, making sure they understand the prospect’s problem. The early conversations are all about exploring the prospect’s problem and pain points. Talking to prospects about their problems is not only a necessary sales tactic, but also necessary for good product development. John Raguin, cofounder of insurance software company Guidewire Software, explains: We went to our potential customers, insurance companies, and proposed to do a short free consulting study that would provide [an assessment] of their operation. We would spend approximately seven to ten man-days of effort understanding their operations, and at the end we would give them a high-level presentation benchmarking them as compared to their peers. In return, we asked for feedback on what would make the best system that would meet their needs. In the end, we were able to work with over forty insurance companies this way. We were honest about our motives at all times, and we made sure to provide quality output. When it comes to structuring your initial sales conversations, we suggest using the approach developed by Neil Rackham as outlined in his book SPIN Selling. It is a four-part question framework to use when talking to prospects, based on a decade spent researching 35,000 sales calls: Situation questions. These questions help you learn about a prospect’s buying situation. Typical questions include How many employees do you have? and How is your organization structured?

Ask only one or two of these questions per conversation, because the more situation questions a salesperson asks, the less likely he or she is to close a sale. That’s because people feel like they’re giving you information without getting anything in return. This is especially true of executive decision makers who are likely more pressed for time. Make sure you ask just enough situation questions to determine if you’re talking to a likely candidate for a sale. Problem questions. These are questions that clarify the buyer’s pain points. Are you happy with your current solution? What problems do you face with it? Like situation questions, these questions should be used sparingly. You want to quickly define the problem they’re facing so you can focus on the implications of this problem and how your solution helps. Implication questions. These questions are meant to make a prospect aware of the implications that stem from the problem they’re facing. These questions are based on information you uncovered while asking your problem questions. Questions could include: Does this problem hurt your productivity? How many people does this issue impact, and in what ways? What customer or employee turnover are you experiencing because of this problem? These questions should make your prospect feel the problem is larger and more urgent than he or she may have initially thought. For example, your prospect may see hard-to-use internal software as just an annoyance, a necessary cost of doing business. Implication questions can help shed light on the problems caused by this hardto-use software: Does it lead to employee overtime because they struggle to accomplish things efficiently?

Does it decrease overall quality of work? Does it impact employee turnover? Each of the above questions helps frame the issue as a larger one in your prospect’s mind. Then you transition to the final set of questions. Need-payoff questions. These questions focus attention on your solution and get buyers to think about the benefits of addressing the problem. Such questions should stem from the implication questions you asked earlier, and can include: How do you feel this solution would help you? What type of impact would this have on you if we were to implement this within the next few months? Whose life would improve if this problem was solved, and how? The SPIN (Situation, Problem, Implication, Need-payoff) question model is a natural progression. First you clarify that the prospect is a potential customer and break the ice (situation questions). Then you get them talking about the problem (problem questions). Next, you uncover all the implications of this problem (implication questions). Finally, you focus on how your solution addresses these implications and will solve their problem (need-payoff questions). How do you get your first customers? As Steve Barsh, former CEO of SECA (acquired by MCI), said in our interview, “You get your first customers by picking up the phone.” If you are fortunate, you may be contacting people you know or were introduced to warmly by a friend. However, you may have to get your first customers by cold calling or emailing prospects. We interviewed Todd Vollmer, an enterprise sales professional with more than twenty years of sales experience, who told us his approach to cold calling. Tactically, setting daily or weekly targets for outbound calls can help you get through the process. You’ll be able to push yourself through some of those uncomfortable feelings (mainly stemming from rejections) with a concrete goal to work toward. When making cold calls, be judicious about the people you contact. Cold calling junior employees is just as difficult mentally as calling more senior employees, but has a much lower success probability because they have less decision-making authority and industry knowledge. Sean Murphy suggests that your first interaction should be with employees who have some power, but aren’t too high up: Ordinarily, it’s somebody who is one level or two levels up in the organization; they’ve got enough perspective on the problem and on the organization to understand what’s going to be involved in bringing change to the organization. As we work with them they may take us up the hierarchy to sell to more senior folks. We don’t tend to start at the top unless we are calling on a very small business, in which case you’ve got to call on the CEO or one of the key execs because no one else can make any decisions. Once you understand the potential customer’s problem and you think your solution can help solve it, you can start to focus conversations on closing the customer. Specifically, Todd recommends getting answers for five specific areas: Process—How does the company buy solutions like the one you’re offering? Need—How badly does this company need a solution like yours? Authority—Which individuals have the authority to make the purchase happen? Money—Do they have the funds to buy what you’re selling? How much does not solving the problem cost them? Estimated Timing—What are the budget and decision time lines for a purchase? After a successful first call, Todd suggests sending a follow-up email documenting what you talked about, including the problems your prospect faces and the next steps. He also suggests closing emails with a direct question such as “Will you agree to this closing time line?” 

Unfortunately, many enterprise entrepreneurs do not put enough thought into deciding on their first customer. Identifying the wrong first customer can lead towasted time and squandered resources. Sean Murphy shared some pitfalls to avoid when seeking out a first customer: One [problem] occurs when the prospect invites you in . . . [but] has no interest in buying what you have or will develop. They would like to learn a lot about this emerging technology area, or this problem area, or something like that. . . . The second situation that’s also a waste of time is when someone claims to be a “change agent.” He will tell you that your offering is going to have a huge impact; it’s going to transform all of General Motors, for example. Substitute your favorite lighthouse customer. Before you get started doing everything that he is telling you to do, you need to ask him, “Have you ever brought other technology into your company?” More often than not unfortunately he will say, “Well, no, but you know I’ve only been here six months, and this is what’s going to let me make a big difference here.” So the two typical problems are you end up giving away free consulting or you talk to somebody that in their own mind is this change agent, but they have no idea how to make it happen. You want your first customers to be somewhat progressive and willing to work with you closely. As you’re still developing your product, you want their active involvement in helping you craft the best solution. Forming a strong relationship is crucial because you want to use your first few customers as references and case studies to give your startup some measure of credibility when you start designing your sales funnel.

SALES TACTICS Picture a funnel. As applied to sales, you start with many prospects at the top, qualify the ones that make good customers in the middle, and then sell a certain number of them on your solution at the bottom. We interviewed David Skok, general partner at Matrix Partners and five-time entrepreneur (he’s taken three companies public and one was acquired), to talk about creating profitable sales funnels.

The first goal is to drive leads into the top of the funnel. Usually, this means using other traction channels to make people aware of your product. While cold calling or emailing can be an effective way to reach your first customers, David believes it’s less effective when trying to build a repeatable sales model: I’m in favor of gaining traction through some kind of marketing channel first, then using sales as a conversion tool to close [those leads] into business. It’s very, very expensive to use cold calling, and really not that effective by comparison with using marketing to get some kind of qualified prospect and then using sales to close that prospect. The next stage in a sales funnel is lead qualification. Here, you want to determine how ready a prospect is to buy, and if they’re a prospect in which you should invest additional resources. For example, many companies require an email address and some company information in order to access materials on their site (e.g., a white paper or e-book). This information is then used to determine which prospects are worth spending more time on. For example, HubSpot, which sells $5,000-plus-per-year marketing automation software, uses this information to determine how much time it should invest in a lead. If it gets a lead from someone running a small business on Etsy or eBay, HubSpot may choose to invest less time in that prospect because chances are someone running a smaller business is just not a good fit for its offering. Mark Suster, two-time entrepreneur and partner at Upfront Ventures, suggests a simple approach to bucket leads into three categories: A’s, B’s, and C’s: I define “A deals” as those that have a realistic shot of closing in the next three months, “B deals” as those that you forecast to close within three to twelve months, and “C deals” as those that are unlikely to close within the next twelve months. “A deals” should get much of the salesperson’s time (say 66 to 75 percent of time), “B deals” should get the balance as each sales rep needs to build their pipeline and bigger deals take time. And the key to scaling is that “C deals” should get no time from sales. They should be owned by marketing.

In many organizations, marketing is in charge of generating leads and doing basic lead qualification. Then the sales team further qualifies and eventually closes the leads. It is part of the marketing department’s job to make sure the sales team gets the information they need to just focus on qualified leads. Mark has this to say about marketing and sales working together: Marketing’s job in working with salespeople is twofold: To arm—which means to give the reps all of the sales collateral they’ll need to effectively win sales campaigns. This includes presentations, ROI calculators, competitive analyses, and so forth. To aim—which means helping sales reps figure out which target customers to focus on. It’s about helping weed out the nonserious leads from the urgent ones. Once you’ve qualified your leads, the final step is to create a purchase time line and convert prospects to paying customers. Todd recommends laying out exactly what you are going to do for the customer, setting up the timetable for it, and getting them to commit (with a “yes or no”) to whether or not they will buy. An agreement at this stage might look like this: “We’ll set up a pilot system for you within two weeks. After two weeks, if you like the system we’ve built and it meets your needs, you’ll buy from us. Yes or no?” Getting a yes or no answer allows you to focus your time on deals that are likely to close without wasting time on prospects that aren’t prepared to buy. Closing leads can happen in a variety of ways. For some products, it can be done completely by an inside sales team (meaning salespeople who don’t travel). This team usually calls qualified leads, does a webinar or product demo, and has an ongoing email sequence that ends with a purchase request. In other cases, you may need a field sales team that actually visits prospective customers for some part of the process—it all depends on the complexity and length of your sales cycle. Remember that, no matter how good your sales team, the customer is the one who decides to buy your product. It is crucial to keep the customer in mind as you design your sales funnel, meaning you should make their decision to buy as easy as possible. As David said in our interview:

You want to recognize that your prospect has a series of issues and questions they will want resolved before they make a buying decision. These are things like “Am I sure that this is the best product?,” “Am I sure that this will work for my situation?,” “Will I get a good return on investment?,” “Will this integrate with a system I have working in place today?,” and so on. A lot of companies design their sales cycles around how they think things should work. I believe very strongly in the notion that you have to design it from the customer standpoint inwards, as opposed to your standpoint outwards, which is the normal way I see people thinking about this stuff. Once you know what that buyer’s questions are, you want to design your process to effectively address all of their questions and recognize what kinds of things need to be handled. Ideally, as many of these questions you can handle on your Web site, the better. Your job, once you have their email, is to answer all of their buying questions and then create a trigger that gives them a strong reason to buy. You can keep track of when prospects are dropping out of your sales funnel. The points in your funnel where many prospects drop off are called “blockages.” Blockages are usually due to sales funnel complexity. You want to make purchasing your product as simple as possible. Some ways you can minimize blockages: Removing the need for IT installs with SaaS (Software as a Service) Free trials (including through open source software) Channel partners (resellers of your products) Demo videos FAQs Reference customers (such as testimonials or case studies) Email campaigns (where you educate prospective customers over time) Webinars or personal demos Easy installation and ease of use Low introductory price (less than $250/month for SMB, $10,000 for enterprises) 

Eliminating committee decision making

CASE STUDY: JBOSS, an open source provider of middleware software, created a sales funnel that drove $65 million in revenue just two years after founding (Red Hat later acquired it for $350 million). JBoss first focused on generating leads. More than 5 million people had downloaded its free software through SourceForge (a popular open source software directory), but JBoss had no contact information for these prospects. Knowing it needed a way to consistently generate leads, JBoss gave away its software’s documentation (which it had previously charged for) in exchange for a customer’s contact information. This worked out well because customers were motivated to get the software documentation, which showed them how JBoss worked. Contact information was a small price to pay. For JBoss, this information was essential, as it could now communicate with prospects about its paid offerings. This tactic generated over more than ten thousand leads per month. That many leads posed a problem of its own: the impossibility of contacting them all individually. It was now time for JBoss to qualify these leads and determine which were most likely to buy. The company used Eloqua, marketing automation software, to determine the pages and links a prospect engaged with before accessing the documentation. Prospects who spent a lot of time on support pages were good candidates for the JBoss support service, the product that generates revenue for the company. JBoss’s marketing team would call these promising leads to further qualify them. Each of these calls was made specifically to determine if a prospect had the desire to get a deal done. If so, qualified prospects were passed to sales. In this final stage of the funnel, prospects were contacted by individuals from an inside sales team. This is where the standard sales process kicks in: calls, demos, white papers, etc. The sales team closed about 25 percent of these prospects thanks to their thorough lead qualification (industry averages hover between 7 and 10 percent).

Unqualified leads not yet ready for sales were put into lead nurturing campaigns. These prospects received the JBoss Newsletter, as well as invitations to webinars, and were encouraged to subscribe to the JBoss blog. Customers who reached a certain level of interaction with nurturing campaigns (e.g., those who clicked on certain links in emails or attended a webinar) would then be put back into the sales pipeline and contacted by someone from sales. JBoss built an impressive and wildly successful sales funnel. A big reason for JBoss’s success was that its sales funnel was designed from the standpoint of the customer. The company utilized free tools to generate leads at a low cost by offering customers the documentation they wanted in exchange for contact information. It then qualified them through marketing built on internal analytics. Finally, JBoss used an inside sales team to close each deal at an average deal size above $10,000.

TARGETS Don’t rule out cold calling. Good first customers have a burning need to address a problem, are interested in your approach to solving their problem, and are willing to work with you closely. Sometimes cold calling is the only way to find them. Build a repeatable sales model. An effective sales funnel has prospects enter at the top, qualifies these leads, and closes them effectively. Map out your sales funnel, identify blockages, and remove them. Keep the buying process as simple as possible. Get the buyer to commit to time lines. To close sales effectively, get an affirmative at each point that you are on track to close. Always know exactly what steps are left. Keep the customer’s perspective in mind. Talk to people who need your product and understand their common concerns. Address those concerns specifically on your Web site.

CHAPTER NINETEEN

Affiliate Programs

An affiliate program is an arrangement where you pay people or companies for performing certain actions like making a sale or getting a qualified lead. For example, a blogger may recommend a product and take a cut when there are sales through her blog. In this case the blogger is the affiliate. Companies like Amazon, Zappos, eBay, Orbitz, and Netflix use affiliate programs to drive significant portions of their revenue. In fact, affiliate programs are the core traction channel for many e-commerce stores, information products, and membership programs. For this channel, we interviewed Kris Jones, founder of the Pepperjam affiliate network, which was acquired by eBay in 2009. Kris grew Pepperjam to become the fourth largest affiliate network in the world: at one point, it had a single advertiser generating $50 million annually through its network.

AFFILIATE PROGRAM STRATEGY Affiliate programs are frequently found in retail, information products, and lead generation. Retail affiliate programs facilitate the purchase of tangible products and account for more than $2 billion annually. Amazon, Target, and Walmart have the biggest programs and pay affiliates a percentage of each sale they make. Amazon’s affiliate program, for example, pays between 4 and 8.5 percent of each sale depending on how many items an affiliate sells each month. Some large retailers like Amazon and eBay run their own affiliate programs, but this is rare. These programs involve recruiting, managing, and paying thousands of affiliates, which is too complex and expensive for most companies to manage themselves. It is much more convenient for online retailers to go through existing retail affiliate networks.

Sites like Commission Junction (CJ), Pepperjam, and LinkShare all have strong networks of affiliates that make a living promoting others’ products. The list of companies that take advantage of these networks contains the most recognizable names in the retail industry: Walmart, Apple, Starbucks, The North Face, The Home Depot, Verizon, Best Buy, and many others. The affiliates that join these programs vary widely, but generally fall into the following major categories: Coupon/deal sites. These sites—RetailMeNot, CouponCabin, Brad’s Deals, and Slickdeals to name a few—offer discounts to visitors and take a cut of any sale that occurs. For example, when you search for “Zappos discount,” RetailMeNot is likely to rank highly for that search term. When you visit the RetailMeNot page that comes up, you get coupon codes for Zappos. If you click through and buy something using a code, RetailMeNot gets a percentage. Loyalty programs. Companies like Upromise and Ebates have reward programs that offer cash back on purchases made through their partner networks. They earn money based on the amount their members spend through retail affiliate programs. For example, if a thousand members buy gift certificates to Olive Garden, Upromise will get a percentage of every dollar spent. Then they pay part of what they earn back to their members. Aggregators. Sites such as Nextag and PriceGrabber aggregate products from retailers. They often add information to product listings, like additional ratings or price comparisons. Email lists. Many affiliates have large email lists to which they will recommend products. They then take a cut when subscribers make purchases. Vertical sites. Hundreds of thousands of sites (including individual blogs) have amassed significant audiences geared toward a vertical, such as parenting, sports, or electronics. Information products include digital products like e-books, software, music, and (increasingly) education. Since it doesn’t cost anything to make another digital copy, selling info products through affiliate programs is quite popular. 

Creators will give large percentages to affiliates that promote their products. By far the largest affiliate network for information products is ClickBank, where affiliate commissions often reach 75 percent. ClickBank has more than 100,000 affiliates and millions of products. Lead generation is a $26 billion industry. Insurance companies, law firms, and mortgage brokers all pay hefty commissions to get customer leads. Depending on the industry, a lead may include a working email address, home address, or phone number. It may also include more qualifying information like a credit score. Affiliate programs are popular with financial services and insurance companies because the value of each customer is so high. Think of how much you spend on auto or health insurance annually: that should give you a sense of why a lead is so valuable. In fact, insurance companies are top Google AdWords spenders, often paying $50 to $100 for a single click! These companies often create their own affiliate programs or go through popular lead-gen networks like Affiliate.com, Clickbooth, Neverblue, and Adknowledge.

AFFILIATE PROGRAM TACTICS Your ability to use affiliate programs effectively depends on how much you are willing to pay to acquire a customer. After all, with this channel you are paying out of pocket for the lead or sale. We recommend going through an existing affiliate network—something like Commission Junction, Pepperjam, ShareASale, or more specific networks targeted at your type of product. Using a network makes it easier to recruit affiliates because so many are already signed up on these sites. It allows you to start using this traction channel immediately. Otherwise, you’d have to first recruit affiliates on your own, which takes significant time and money. Setting up an affiliate program on one of these existing affiliate networks is relatively easy, though it does require an up-front cost. In the case of Commission Junction, that cost is more than $2,000. However, if you successfully recruit high-performing affiliates through the network, affiliate sales will quickly cover the initial fee.

The other option is to build your own affiliate program independent of an existing network. With such a program, you recruit partners from your customer base or people who have access to a group of customers you want to reach. One benefit of this approach is you don’t have to pay your affiliates all in cash. Instead, you can use the features of your product as currency. For example, if your startup has a freemium business model, you could give away certain features or extend subscriptions. Earlier, we explained how Dropbox’s referral program involves giving people free storage space. Another example is QuiBids, a top penny auction site. It built out a referral program for its current customers that gives free bids to people who refer other customers. The first place to look for potential affiliates is your own customer base. They are easy to recruit and work with because they are already familiar with and have an affinity for your brand. After getting customers involved in your affiliate program, you will want to contact content creators, including bloggers, publishers, social media influencers, and email list curators. Monetizing blogs can be difficult, so these content creators often look for other ways to make money. We interviewed Maneesh Sethi, popular blogger at HacktheSystem, to talk about how companies can build relationships with people like him. Maneesh has been an affiliate for many products he has personally used. As an example, Maneesh was a customer of a program that taught SEO tactics. He loved the program, so he contacted the company himself and worked out a deal with them to give him a commission for each new customer he sent their way. After agreeing to terms, Maneesh sent out an email to his list mentioning how the SEO program had helped him get better rankings on Google. That single offer has made him nearly $30,000 in two years, and has made the company much more. Maneesh also has recommended RescueTime, a time-tracking application that helps you be more productive. As one of its top affiliates, he has referred more than three thousand people to the product since he joined its affiliate program. Through Maneesh, RescueTime was able to reach a new audience without spending a lot of money on marketing or wasting it on leads that didn’t convert. Maneesh mentioned that the best way to reach someone like him is by building a relationship: help the content creators where you can, writing guest posts or granting free access to your product. In turn, they’ll be happy to promote you if you have a truly great product.

Well-established affiliate programs like those run by Amazon or Netflix have figured out exactly how much to pay their affiliates for each lead. As a startup, you are going to be less sure of your underlying business and should start with a simple approach. The simplest approaches are to pay a flat fee for a conversion (e.g., $5 for a customer who purchases something) or to pay a percentage of a conversion that occurs (e.g., 5 percent of the price a customer pays). More established affiliate programs get more complex by segmenting products and rewarding top affiliates. eBay gives seasonal coupon codes to its affiliates for product categories it wants to push. Tiered payout programs are also popular. In this structure, affiliates get paid a percentage (or flat fee) of each transaction. The percentage is based on the number of sales you make—if you drive more transactions, your rate goes up, and you make more money.

Major Affiliate Networks Here are the top affiliate networks, as well as a list of software tools that can help you build your own referral program without a substantial engineering investment. Commission Junction—CJ has many of the largest Internet retailers on its platform. It is also somewhat pricey: it costs upward of $2,000 to sell your product through its network. This high cost, combined with the fact that CJ curates both affiliates and publishers for performance, creates a high level of quality in its network. ClickBank—The leading platform for anyone selling digital products online (courses, e-books, digital media). ClickBank is relatively cheap to start with, as you need to pay only $50 to list a product on its platform. Affiliate.com—Affiliate.com promises a very strict affiliate approval process, which it claims means higher-quality traffic for its advertisers. Pepperjam—Started by Kris Jones (whom we interviewed for this chapter), the Pepperjam Exchange encompasses multiple channels (mobile, social, offline retail, print, etc.). 

Pepperjam promotes its customer support and transparency as selling points for its network, which costs $1,000 to join. ShareASale—This affiliate network has more than 2,500 merchants and allows advertisers to be flexible in determining commission structures. It costs about $500 to get started. Adknowledge—Adknowledge offers traditional ad-buying services in addition to affiliate campaigns. It also works in mobile, search, social media, and display advertising, giving advertisers access to affiliate and CPC outlets through one platform. LinkShare—LinkShare helps companies find affiliates and builds lead-gen programs for them. Companies like Macy’s, Avon, and Champion use them to manage affiliate programs. MobAff—MobAff is a mobile affiliate network that utilizes SMS, push notifications, click to call, mobile display, and mobile search to drive conversions for its advertisers. Neverblue—Neverblue is targeted toward advertisers that spend more than $20,000 per month. It also works with its advertising partners on their advertisements and campaigns. It counts Groupon, eHarmony, and Vistaprint as some of its clients. Clickbooth—Clickbooth uses search, email, and many Web sites to promote brands like DirecTV, Dish Network, and QuiBids. RetailMeNot, Inc. (formerly WhaleShark Media)—This media company owns some of the most popular coupon sites in the world, including RetailMeNot and Deals2Buy.com. Companies can partner with RetailMeNot to drive coupon-based affiliate transactions through its sites, which often appear near the top of Google for any “term + coupon” search.

CONCLUSION Kris stressed that more startups should take advantage of this traction channel. As he put it:

For startups that don’t have a lot of money, where you can’t just open a PPC [pay-per-click] account and start throwing darts, affiliate marketing seems to me to be a logical place to start. There’s really no guarantee that if you spend $10,000 on Google AdWords you’ll make more than that. If you were to compare affiliate marketing and PPC, the advertiser assumes the risk in PPC. If you set up poorly written and poorly thought out campaigns on AdWords, you’re going to have to pay for the click whether or not your ads suck, or whether or not they’re converting well. With affiliate marketing, you get to define what the transaction or the conversion is, and you also have tools available to mitigate low quality. For instance, if someone refers an e-commerce transaction to you but the credit card is declined, the affiliate commission is zero. If someone submits a lead form, but the lead doesn’t follow the rules you set out (a legitimate email address, a real postal address, etc.), you don’t have to pay for that. You don’t assume the risk.

TARGETS Test using an existing affiliate network. It already has affiliates, so you can start using this traction channel immediately. Keep your payouts simple. Know how much you can spend to acquire a customer and keep it below that. As you get deeper into this channel you can test more complicated payout programs. The next place you should look for more affiliates is your customers. They already like you, and so there may be a lot of them willing to sell for you.

CHAPTER TWENTY

Existing Platforms

Existing platforms are Web sites, apps, or networks with huge numbers of users—sometimes in the hundreds of millions—that you can potentially leverage to get traction. Major platforms include the Apple and Android app stores, Mozilla and Chrome browser extensions, social platforms like Facebook, Twitter, and Pinterest, as well as newer platforms that are growing rapidly (Tumblr, Snapchat, etc.). When mobile video-sharing app Socialcam launched, it suggested users sign up with Facebook or Twitter, promoted user videos on both platforms, and encouraged people to invite their friends from each site. It went on to hit 60 million users within twelve months—that type of growth just isn’t possible through many other channels.

EXISTING PLATFORMS STRATEGY: APP STORES With the number of smartphone users well above one billion and growing every day, we’ve seen an explosion of apps reaching millions of users in short periods of time: months, rather than years. The most efficient way for an app to get discovered in the app stores is through the top app rankings and featured listings sections. These rankings group apps by category, country, popularity, and editors’ choice. The story of Trainyard illustrates the impact an App Store feature can have. Trainyard, a paid iOS game developed by Matt Rix, wasn’t growing the way he had hoped. Because free applications are downloaded at a much higher rate than paid, app developers will often release a free version and monetize those free users via in-app purchases or paid upgrades. Matt decided to try this tactic. When he released the free version of Trainyard (Trainyard Express), an editor at a popular Italian blog wrote a glowing piece on it almost immediately. 

This propelled the app to be the number-one free app in Italy—netting more than 22,000 downloads on that day alone! The app then hit the top spot in the United Kingdom and was downloaded more than 450,000 times in a week. Seven days after that, Apple decided to feature it. Everything that happened before was dwarfed by what happened next. Downloads skyrocketed by 50x and persisted at those heightened levels while the feature was live. Millions of downloads. And even after the feature passed, daily download levels remained significantly elevated compared with where they were before. Trainyard illustrates the importance of getting enough attention for your app so that it shows up in the rankings and featured sections. Mark Johnson, founder of Focused Apps LLC, wrote about how app promotions usually work: 1. Ads get the [app] somewhere into the charts. 2. Now it’s in the charts, more people see it. 3. So it gets more organic downloads. 4. Which makes it go a bit higher up in the charts. 5. Now even more people see it and it gets more organic downloads. 6. People like it and start telling their friends to get it too. 7. It goes up higher in the charts. 8. Repeat from 5. Companies use many tactics to get into the charts initially. They buy ads from places like AdMob, buy installs from companies like Tapjoy, crosspromote their apps (through cross-promotion networks or other apps they own), or even buy their way to the top of the charts through services like FreeAppADay. Other traction channels can also drive adoption of your mobile app: as Trainyard showed, the publicity and targeting blogs channels can work well. While none of these tactics are enough on their own, they can help you get the ball rolling toward a ranking or feature. However, for top rankings to happen sustainably, you need to have a compelling app that is rated highly on a regular basis. Ratings matter a lot—they influence individual choices to download an app, editors choose apps to feature based on them, and they’re often mentioned in any press coverage. That’s why you see even top apps continually asking you to rate them.

There are some tricks you can use, like asking people to rate your app right after you give them something useful, but really the base experience has to be excellent to get consistently high rankings. Even with hundreds of thousands of apps, there are shockingly few that are truly amazing user experiences. Most of the apps that are now household names—Instagram, Path, Google Maps, Pandora, Spotify—all have excellent user experiences and consistently high ratings. Browser extensions in Chrome and add-ons in Firefox are apps you can download for your Web browser. The most popular browser extension is Adblock Plus, which blocks ads on major Web sites. Other popular extensions help you download YouTube videos, save bookmarks across computers, and manage your passwords. Web users visit dozens of different sites every day; to establish yours as a site they consistently visit can be difficult. A browser add-on allows people to get value from your product without consistently returning to your site. Evernote, a memory-enhancement and productivity tool, saw a huge jump in customers when it launched its browser extensions. In its “2010 Year in Review” blog post, Evernote said Web usage went up 205 percent thanks to these extensions—and this from a company with more than 6 million users at the time! Like mobile app stores, browser extensions have dedicated portals where you download the apps, though unlike app stores, all the apps are free. These portals also have features and rankings, which you should be similarly seeking if you focus on this area.

EXISTING PLATFORMS STRATEGY: SOCIAL SITES The use of social sites is constantly shifting as people change where they communicate online. Newer social platforms like Snapchat and Vine are adding users at a dizzying pace, and we’re sure others will follow them soon. Even though keeping up with the evolution of social platforms can be challenging, they remain one of the best ways to rapidly acquire large numbers of customers. In fact, it makes sense to focus on platforms that are just taking off. Social platforms that haven’t fully matured also haven’t built all of the features they’ll eventually need; you might be able to fill in one of those gaps.

They also are less saturated, as larger brands are often slower to target up-andcoming sites. YouTube got its initial traction by filling gaps in the Myspace platform. In the mid-2000s Myspace was the most visited social networking site in the world. Video sharing on the Web wasn’t user friendly yet—it was difficult to upload videos and put them on other sites. Myspace didn’t have a native video hosting solution. YouTube stepped in and provided one that was simple: you could upload and embed a video in Myspace in a matter of minutes. Even better for YouTube, Myspace users were directed back to YouTube when they clicked on the embedded videos. This exposed many Myspace users to all of the great features and content available on YouTube, and was responsible for YouTube’s rapid early growth. Every major platform has similar stories. Bitly fulfilled the need to share shortened links on Twitter and saw most of its adoption from such use. Imgur built its image-hosting solution for reddit users, and has seen an explosion in usage as a result. This pattern repeats itself time and time again. There are thousands of other large sites and marketplaces that you can target to get customers. First, figure out where your potential customers hang out online. Then create a strategy to target potential customers on these existing platforms. Sites like Amazon, eBay, Craigslist, Tumblr, GitHub, and Behance have all helped startups build traction. Airbnb saw much of its early growth come through Craigslist. Customers who used Craigslist found that Airbnb was a much simpler and safer solution. With this knowledge, the company’s engineers developed a “Post to Craigslist” feature that would allow you to list your bed on Craigslist. Though this feature eventually was shut down, it drove tens of thousands of Craigslist users back to Airbnb to book a room. PayPal, the leading online payments platform, used a similar strategy when it targeted eBay users as its first customers. In the beginning, PayPal itself purchased goods from eBay and required that the sellers accept payment through PayPal. This worked so well that PayPal proved more popular than the payment system eBay itself was trying to implement! This single-minded focus allowed PayPal to acquire a large percentage of people within one of the few groups of buyers and sellers that dealt with online payments at the time.

CASE STUDY: EVERNOTE

Since its founding, Evernote has focused on existing platforms as its core traction channel. We talked with Alex Pachikov, on the founding team of Evernote. His company was recently valued at over one billion dollars. Evernote has made it a priority to be on every new and existing platform. It benefits from the platform’s initial marketing push and increases its chances of getting featured. As Evernote’s CEO, Phil Libin, puts it: We really killed ourselves in the first couple of years to always be in all of the App Store launches on day one. Whenever a new device or platform would come out, we would work day and night for months before that to make sure Evernote was there and supporting the new device or operating system in the App Store on the first day. . . . When iPhone launched we were one of the very first iPhone apps, so we were promoted and had a lot of visibility. When iPad launched, we were there on day one, not just with a port of our iPhone client, which a lot of other companies did. . . . [We had] a completely new designed version for the iPad even though we’d never seen an iPad before—we stood in line with everyone else. Same thing with Android devices and the Kindle Fire. Being first can open you up to the opportunity to benefit from the early marketing and promotion about the platform itself. As Alex said: Every year there’s a new platform, new device, new something, and as somebody who’s starting a company you should consider if there’s something really cool you can do on an upcoming platform. Now obviously you can’t plan if a platform is going to be successful, but you can [make some] reasonable guesses based on past experiences with a company. I think people see this as gambling. People take the “I will support this platform when it has a million users” type of approach. That’s a fine thing to do if you are EA or Adobe or something like that. And maybe for Evernote a year from now, that’s the right thing to do. But for a startup, you really aren’t in that position. When a platform is popular, it’s crowded. . . . 

A lot of people have cool apps and could do really, really well if they were to get this initial push, and that initial push is free if you do it early. But you risk that all that effort is a waste. Evernote was one of the first apps available for Android. Because it had some very cool functionality, it was featured in the Android store for six weeks straight, at a time when it was far less crowded than it is now. This gave Evernote hundreds of thousands of new customers, all because it was early and focused its engineering efforts on being first on the platform. Similarly, when Verizon picked up Android phones, Evernote benefited from the national marketing push Verizon did to promote its Android launch. This build-early strategy doesn’t work in every case, especially when the underlying platform flops. Evernote took the same approach with the Nokia, Windows, and BlackBerry smartphone platforms, none of which moved the needle. Nevertheless, Alex is very happy with the overall strategy: when it works —as with Android—it more than makes up for the failures. In the last few years, Evernote’s strategy has been to expand beyond its pure note-taking app and release many different apps for specific verticals (Evernote Food for food notes, Evernote Hello for remembering people, etc.). Because getting promoted in app stores has been its most effective growth tactic, this strategy enables Evernote to get featured and ranked in categories where Evernote’s main app does not appear. At Evernote, Alex mentioned that they think hard about what types of features or apps would stand out to editors: You have to think ahead. What types of things would Apple or Google really like? What are things that, if we were to do, Apple or Google or Microsoft would be looking for? And is there a natural fit between what we do and what they would be looking for? This thought process has led to apps like Evernote Peek. Peek was an app that allowed you to turn your media (notes, videos, or audio) into study material that you interacted with using the iPad Smart Cover. While now discontinued, it felt magical at the time because it took advantage of a new Apple technology. It was so cool that Apple itself showcased it in a commercial!

Peek was featured in Apple’s education category, and was the number-one educational app for over a month. This exposure led to more than 500,000 new Evernote users who experienced the product through Peek, and was one of the strongest growth drivers for the company during 2012. Though Evernote has seen most of its growth come through mobile channels, its platform strategy works perfectly well on nonmobile platforms. The important takeaway is that it is a good idea to focus on new and untapped platforms to generate growth. Chris Dixon, a partner at Andreessen Horowitz and the founder of Hunch before its acquisition by eBay, had this to say about platform-based growth: Some of the most successful startups grew by making bets on emerging platforms that were not yet saturated and where barriers to discovery were low. . . . Betting on new platforms means you’ll likely fail if the platform fails, but it also dramatically lowers the distribution risks described above.

TARGETS Figure out where your potential customers are hanging out online. They could be on major platforms, on niche platforms, or on some combination thereof. Then embark on a strategy to target these existing platforms. Create a feature specifically to fill a gap for that platform’s users. Large companies have been built on the back of each major social platform by filling gaps with features that the platform was not providing itself. Focus on new and untapped platforms. Or try new aspects of major platforms because there is less competition there.

CHAPTER TWENTY-ONE

Trade Shows

Trade shows offer you the opportunity to showcase your products in person. These events are often exclusive to industry insiders, and are designed to foster interactions between vendors and their prospects. Early on, you can use this traction channel to build interest in what you’re building. As you get more established, you can use trade shows as an opportunity to make a major announcement, sell big clients, seal a partnership, or as an integral part of your sales funnel. We interviewed Brian Riley of SureStop, the company behind the SureStop bike brake. His startup has used trade shows to gain traction at every phase, from preproduct to a major distribution deal with a large bike manufacturer. We also spoke with Jason Cohen, founder of WP Engine, who used trade show marketing at his first company, Smart Bear Software.

TRADE SHOW STRATEGY Almost every industry has a large number of trade shows: the tough part is deciding which ones to attend. The best way to decide whether to exhibit at an event is to visit as a guest and do a walkthrough the year before. Attending as a guest allows you to get a feel for an event without straining your budget. If this isn’t possible, the next best option is to get the opinions of people who have exhibited at previous shows: How crowded was it? How high was the quality of attendees? Would you go again? These are important questions that will help you decide if a particular trade show is right for your startup. Brad Feld, a partner at Foundry Group, suggests following these steps when deciding which events to pick:

Set your goals for attending trade shows this year. For example, are you trying to get press, lure investors, land major customers, work out significant partnerships, or something else? Your goals should drive your decisions about which events to attend and how to approach them. Write down all events in your industry. Next, evaluate each event in the context of your goals. In particular, think about the type of interactions you want and whether these interactions take place at each event. For example, if you need to have long conversations with prospects to do customer development, seek out an event with an intimate atmosphere. If your goal is to interact with as many potential customers as possible, a crowded event would be a better fit. Figure out how much you can spend per year and allocate this budget by quarter. This allows you to align events on your schedule with your budget while also giving you flexibility to reallocate in later quarters if company goals change. Finally, work backward to see if attending a particular event makes sense given your quarterly budget. For example, let’s say you are attending Traction Trade Show and your goal is to increase sales. When you receive the attendee list from the conference organizer (ask for it if it is not provided), you see that ten thousand people are going. However, you estimate that only 30 percent of those people fit the profile of a potential customer, so the total number of people you can realistically target is three thousand. If it will cost you $10,000 to attend this trade show and the price of your product is $5,000, it may make sense for you to attend. That is, your trip will be profitable around the third sale with these numbers. Then the decision comes down to what other traction opportunities you have right now. However, if you are selling a $50 product, you probably won’t sell enough to make attending this trade show worth your while. SureStop attended a few trade shows early on with prototypes on hand—no manufacturing line, no pricing, and no concrete plans to sell anything yet.

Their goal was simply to have conversations with other companies about the features they wanted to see in SureStop’s products. From these conversations, they learned what their product needed from a technical standpoint and the price points they needed to hit. Later, when they had developed a product, they increased their presence and expenses at trade shows. In other words, as their company goals changed, their actions at each show changed with them.

TRADE SHOW TACTICS Your preparation for a trade show will determine how successful you will be. This is one of the few times during the year when nearly everyone in your industry is in one place; you’ll want to be at your best. To prepare, make a list of key attendees you want to meet at the trade show. Then schedule meetings with them before you attend the event. Brian sent wellresearched emails explaining what SureStop did and how its technology could benefit the people he wanted to meet. He also attached a one-pager with more information about the company. This strategy allowed him to meet the people he wanted at every event he attended. Jason Cohen put it this way: Set up meetings. Yes, meetings! Trade shows are a rare chance to get face time with: Editors of online and offline magazines. Often overlooked, editors are your key to real press. I’ve been published in every major programming magazine; almost all of that I can directly attribute to talking with editors at trade shows! It works. Bloggers you like, especially if you wish they’d write about you. Existing customers. Potential customers currently trialing your stuff. Your vendors. Your competition. Potential partners.

Proactively set meetings. Call/email everyone you can find. It’s easy to use email titles which will be obviously non-spam such as “At [Trade Show X]: Can we chat for 5 minutes?” I try to get at least five meetings per day. Organizing dinner and/or drinks after the show is good too. If publicity is one of your goals, reach out to media that will be in attendance. Media members attend trade shows specifically to see what’s going on in an industry—give them something to write about! This could be a new product, feature, or deal with a big customer. Successful trade shows come down to the relationships you build and the impression you make on journalists, prospective customers, and potential partners. Mark Suster, partner at Upfront Ventures, suggests hosting dinners for such people to strengthen these relationships: The other secret conference trick that is orchestrated by the true Zen masters is to schedule a dinner and invite other people. It’s a great way to get to know people intimately. Start by booking a few easy-to-land friends who are interesting. Work hard to bag a “brand name” person who others will want to meet. All it takes is one. Then the rest of your invites can mention that person’s name on the guest list (name others, too . . . obviously) and you will be able to draw in some other people you’d like to meet. Another similar strategy is with customers. If you invite three to four customers and three to four prospects to a dinner with two or three employees and some other interesting guests you’ll be doing well. Potential customers always prefer to talk to existing reference customers than to talk to just your sales reps. Final tip: picking a killer venue is one of the best ways to bag high-profile people. Everybody loves to eat somewhere hot. However, sometimes a dinner can be too expensive for an earlystage company. So why not go in on the dinner with two other companies? That way you’re all extending your networks and splitting the costs.

When planning your booth, first determine where you want to be located on the show floor. If your goal is to reach many attendees (as opposed to targeting a few high-value prospects), you need visibility. That means you want a booth in a well-trafficked location and a marketing plan to get people to take notice. If your strategy is dependent on talking to just a few key partners, a great booth location and the added cost that comes with it doesn’t make as much sense. In fact, you may want to be situated in a very particular place, such as next to a specific established company. No matter what your location, you will want to put together an impressive display. Having a big banner that says what you do, nice-looking booth materials, business cards, and a compelling demo are the basics. If that seems like a lot to put together, there are many vendors that help companies create trade show materials. To attract people to his startup’s booth, Jason Cohen, founder of Smart Bear Software, would send out discount cards for its software to all attendees before the actual conference. The recipients had to come to his booth to redeem the discounts. Giveaways are an important way of getting some buzz and inbound traffic at a trade show. Coffee mugs and stress balls are tried and true, but you can get even more creative with more unique items (yo-yos, coconuts, cigar cutters) to stand out during the show. A play on the name of your company or your core value proposition gets people talking about your booth. For example, DuckDuckGo could give out duck key chains or sunglasses to showcase that they don’t track your searches. You can also be proactive on the trade show floor to bring people back to your booth. The founders of RJMetrics, a business analytics subscription service, told us about how they’ve had success starting conversations by walking up to people at trade shows: One thing was clear: it pays to have an outbound strategy. Only 28 percent of our conversations were walk-ups. This means that employing an outbound strategy allowed us to extract between three and four times as much value from the show as we would have otherwise. A proactive and inexpensive method that requires no creativity is giving away as many bags with your company’s name on it as possible. 

Most attendees travel with armloads of pamphlets, catalogs, flyers, and giveaways. Stopping each to offer them a bag to put it in gets them talking to you but, more important, gets your name displayed all over the conference area. Many companies also do something particularly engaging within their booths to get people to stay there long enough to experience their full pitch. SureStop has a funny video demo comparing its brakes with the regular brakes found on bikes. The video shows an individual using regular bike brakes speeding down a mountain and braking and promptly getting thrown over the handlebars. Then, as a comparison, the same person in the same scenario is shown using a bike with SureStop brakes and comes to a quick, safe stop. It’s a simple and compelling way to pique people’s interest in the product. When you do engage a person, each of the materials you give out should have a specific call to action (CTA). For example, if someone picked up a business card at your booth, it should have an enticing offer (e.g., download a free industry guide), along with a unique link to that download. Make sure this page is mobile optimized, as most of your visitors will be accessing the page from a mobile device. One step beyond hosting dinners is throwing a party near the show center. Like dinners, these are a great way to loosen up and chat with others at the event. You could cosponsor these with other startups to keep costs reasonable. Trade shows give you more direct interaction with customers, partners, and press in a short period of time than most other traction channels. Those connections can be especially valuable if your key customers and partners are geographically diverse, and traveling to meet each of them separately would be prohibitively expensive. This channel has the potential to move the needle in a matter of days. That’s what happened with SureStop. After a major industry trade show, they managed to form a relationship with Jamis, one of the largest players in the bike manufacturing industry. They met Jamis early on, when they had just a prototype. From that meeting, SureStop learned the necessary specifications they’d need to have if they wanted to work with Jamis. After SureStop built its product to those specs, SureStop formed a manufacturing relationship with Jamis. Its brakes are now stopping thousands of bikes nationwide, and are its biggest source of traction. And all of it started and grew through this traction channel.

TARGETS Schedule meetings and dinners ahead of time. Identify your top targets and find a way to engage them individually at the show. Investigate the efficacy of shows before committing. Attend shows this year you might want to exhibit at next year. Reach out to previous exhibitors. Have an inbound and outbound strategy for your booth. Do something proactive and creative. Include a strong call to action on every item you give out.

CHAPTER TWENTY-TWO

Offline Events

Sponsoring or running offline events—from small meetups to large conferences—can be a primary way to get traction. Twilio, a tool that makes it easy to add phone calls and text messaging to apps, attracts its customers by sponsoring hackathons, conferences, and meetups, large and small. Larger companies like Oracle and Box throw huge events to maintain their position as market leaders. Salesforce’s Dreamforce conference has more than 100,000 attendees! In phase I, offline events give you the opportunity to engage directly with potential customers about their problems. Such events are especially important when your target customers do not respond well to online advertising and do not have a natural place to congregate online. Attracting these customers to one location or going to a place where they meet in person can be the most effective way to reach them. Offline events are particularly effective for startups with long sales cycles, as is often the case with enterprise software. We’ll look at how Enservio reached decision makers and shortened its sales cycle by using this channel. You can also use offline events to build relationships with power users, as both Yelp and Evite have done successfully.

OFFLINE EVENTS STRATEGY Conferences are the biggest and most popular type of offline event. Each year hundreds of startup-related conferences and thousands of business conferences are held worldwide. You can benefit from a conference in any startup phase. In phase I, where smaller groups of people can move the needle, attending meetups and events is a prime way to do so. Tech startups in phase II can take advantage of larger tech conferences like TechCrunch Disrupt, Launch Conference, and SXSW to build on their existing traction. 

Twitter launched nine months before SXSW in 2007 and was seeing decent amounts of traction, on the order of several thousand users. Because many of its early users were headed to SXSW, Twitter saw the conference as an opportunity to accelerate its adoption. As Twitter cofounder Evan Williams said: We did two things to take advantage of the emerging critical mass: 1. We created a Twitter visualizer and negotiated with the festival to put flat panel screens in the hallways. . . . We paid $11K for this and set up the TVs ourselves. (This was about the only money Twitter’s ever spent on marketing.) 2. We created an event-specific feature where you could text “join sxsw” to 40404. Then you would show up on the screens. And, if you weren’t already a Twitter user, you’d automatically be following a half dozen or so “ambassadors,” who were Twitter users also at SXSW. We advertised this on the screens in the hallways. Thanks to this conference-specific marketing, Twitter jumped from twenty thousand tweets per day to more than sixty thousand by the end of the conference. Twitter also won the SXSW Web Award, leading to press coverage and even more awareness of its service. Eric Ries wanted to broaden the audience for the Lean Startup principles he was promoting on his blog. However, he was afraid his message would get lost at a large conference like SXSW. Instead, he organized his own conference and invited founders of successful companies to talk about how Lean principles worked in their startups. First, Eric tested demand for his conference by asking his readers if they would be interested in such an event. After a resounding yes, he sold conference tickets through his site and other popular startup blogs. Startup Lessons Learned began as a one-day conference in San Francisco with just a few speakers and panels focused on Lean Startup concepts. The short event was attractive to individuals who didn’t want to spend a lot on travel or take time off work. In addition, Eric avoided the extra cost and coordination headaches that come with arranging a multiday event: flying in speakers, hotel stays, and so on.

He made the commitment to attend as simple as possible. The result was a strong turnout and a great conference experience. While he didn’t want people to have to travel to attend, he still wanted people from out of the area to find out what was happening at the conference. To this end, Eric live-streamed the conference to meetup groups across the country. The people who attended those meetups or watched on individual live streams were instrumental in promoting his ideas to a larger audience and making his book a bestseller. Other companies have built traction by holding more lavish affairs. This was the case with Enservio, a company that sells expensive software to insurance companies. Enservio was struggling to reach top executives in the insurance industry through other traction channels. To get traction through offline events, Enservio went all out to organize the Claims Innovation Summit. They held it at the Ritz-Carlton in beautiful Dove Mountain, Arizona, for multiple days. They made sure the event didn’t feel like a sales pitch for their services. Instead, they pulled in prominent figures from major consulting firms, respected individuals in the insurance industry, and founders of hot startups to come speak. They then used this group of speakers to attract the industry executives who were their prospective customers. Not only could the executives learn from the speakers, but they could network and vacation at the same time. The event successfully attracted top decision makers and established Enservio as an industry leader overnight. It has now established this conference as an annual event in its industry. MicroConf is a smaller conference for self-funded startups that attracts hundreds of founders and sells out in days. It is run by Rob Walling of HitTail. When Rob first started MicroConf he had difficulty attracting people to a conference they’d never heard of. As he said: We struggled to sell tickets [for the first MicroConf]. . . . I ran Facebook ads and did [Google] AdWords, but neither really worked. Anything that wasn’t relational, where people hadn’t heard of the conference, didn’t get traction. . . . We also put together an e-book with quotes and articles from some of MicroConf’s speakers where you had to pay with a tweet to get it. It went pretty viral, but didn’t sell any tickets.

Some people said it was too expensive. I think for some people that was an issue, but I think it comes down to being able to prove value. Since it was unproven, they just didn’t know if they were going to spend that $500, plus airfare and hotel, and it would just be a crappy or mediocre conference. Once the first year proved itself, suddenly people were saying we need to raise the price—people have no issue with the price now. Rob spoke about the types of companies that have the potential to benefit from meetups and other offline events: Companies with customers who have shared interests, who have a kind of community or at least a need for one, I think that’s the type of company that will benefit most. I don’t know that HitTail would be a good example of a company that could throw a good conference. . . . Our customers are all over the board (real estate, doctors, startups, etc.), so throwing an SEO conference probably wouldn’t be all that helpful. Any niche where the market is online and easily reachable would be a good one, because everyone wants to go to a conference. Any niche where you have recognizable names you can go after would also be good. Instead of a conference, you may choose to connect with a target group of customers at a meetup. For example, if you’re a small SEO software company, you might hold a meetup where you discuss the latest and greatest SEO tactics. Small meetup groups are more effective than you might expect, especially in the early stages. Seth Godin used meetups when launching his book Linchpin. He organized Linchpin meetups in cities all across the country through his blog. In total, more than ten thousand people attended these events, where they connected over ideas that Seth wrote about as well as built relationships with one another. Great meetups can create lasting community connections. The meetup groups that watched the live stream of the first Lean Startup conference continue to meet years afterward: more than twenty cities still have regular “Lean Startup Circle” meetups. These events allow practitioners to continue to connect over the ideas in Eric’s book. They’ve also helped keep his book on the bestseller list.

You can start your own meetup, join an existing one, or even sponsor an event where your prospective customers will be. Meetup.com is the most popular site for doing so. Nick Pinkston, founder of automated manufacturing startup Plethora Labs and the Hardware Startup Meetup group, saw a need for a community around the budding hardware startup movement. In the Bay Area there were hundreds of events and meetups focused on software startups, but not a single one focused on the unique needs and challenges of hardware startups. Nick organized his first meetup at TechShop SF. The first meeting drew 60 people and the only expense was $70 for pizza. An event like this makes for a great test case given how easy it is to pull off. In Nick’s case, there was lots of interest from attendees—the group now has more than 2,600 members. Believe it or not, throwing a party can also be an effective way to get some traction. Evite did this when it put on one of the largest parties in the Bay Area for Internet celebrity Mahir Çağri. Evite was of course responsible for organizing and sending out all invitations. This event exposed Evite to its target customer in a memorable manner. Who doesn’t want a party invite? Attendees were then likely to use Evite when throwing their own parties. Yelp had a similar experience when trying to jump-start usage in new cities. It would throw parties where Yelp Elites (the company’s term for top Yelp users) were allowed to RSVP first, given free food and merchandise, and treated as VIPs. When other users heard about such perks, it gave them an incentive to be more active on the site.

OFFLINE EVENT TACTICS Although MicroConf has become a huge event, Rob suggested that a day-long mini-conference could be a great way for a smaller startup to get traction. It can also be an easy and cheap way to test if there’s any interest among your audience for a larger event. For example, you can select a topic relevant to your product and invite the founders of three local companies to come give short talks on the subject. You could also feature these founders on a panel about a particular topic. You might even take the “unconference” approach and have attendees suggest topics for roundtable discussion, and then allow them to vote on which discussions will take place. A local university lecture hall is a good place to hold an event like this. Often, universities are willing to open their facilities if it’s for an educational purpose and if some of their faculty or students are participating. This type of mini-conference can be done for less than $500. If your first event is a success, consider scaling up to larger events. The logistics of planning a larger event will take a lot more effort because you need more of everything. Sponsors may be interested in helping you cover the cost of the event. For MicroConf, companies with products built for startups offset the cost of putting on the conference. Rob also made a few key points about creating a great event. Keeping attendee quality as high as possible is crucial so that those who attend the conference will learn a great deal both from the speakers and from other audience members. Rob has found the best way to do this is to make the ticket price relatively high, so that individuals with successful businesses are more likely to attend than those just starting out. The structure of an event also plays a critical role in whether the experience works for you and the attendees. With MicroConf, Rob intentionally keeps it small so that attendees have a chance to meet everyone else at the event and the speakers can get to know the attendees. At larger events, speakers get mobbed after they give a talk or sit on a panel. At smaller events, each attendee can connect with every speaker personally. Rob facilitated these conversations by having the speakers sit with the attendees at lunch and participate in roundtable discussions. If you are creative and willing to try something different, throwing a successful event can be a big win. One of the reasons offline events are effective is that so few startups are doing them. As Rob said: I think the overarching thing for marketing is [startups] need to try more things, and fail faster and more quickly.

Trying all of this stuff and seeing what works is paramount. The tried and true approaches like Facebook and AdWords are so crowded now. People need to think about doing things that don’t scale. Early on when you’re trying to get those first one thousandcustomers, you have to do things that don’t scale. You have to take more risks. You can still build a business without being creative. If you don’t have creativity, you need money. You need one or the other.

TARGETS Launch at a conference. Conferences are the biggest and most popular type of offline event. Launching at a conference has been a successful phase I conference tactic. If there isn’t a conference that directly brings together your target customers, consider creating one. Test this channel first. Attend a couple conferences or host a few smaller meetups or a one-day mini-conference. Throw a party. Having meetups or parties, either alongside conferences or across many cities, is another successful strategy to attract and reward prospective customers.

CHAPTER TWENTY-THREE

Speaking Engagements

In the previous two chapters, we touched on speaking at trade shows and offline events. In this chapter, we will discuss how to land these speaking engagements and how to make them compelling. It’s relatively easy to get started in this channel. Start by giving free talks to small groups of potential customers or partners. Speaking at small events can improve your speaking ability, give you some early traction, and spread your story or message. It’s also good for personal growth if you’ve never done it before: Mark Zuckerberg has talked about how improving at public speaking has vastly improved his management ability. We recommend trying to give at least one talk even if you choose not to pursue this traction channel. Dan Martell is the founder of Clarity, an advice platform that connects founders with successful entrepreneurs. He spoke to us about getting traction through speaking engagements: Speaking is funny. You know to me, it’s the old-school concept that teaching sells. . . . Teaching is what content marketing is all about: webinars, blog posts, and the like. I look at [these] things as the future of good marketing. The opportunity to teach and be in front of a room for forty-five minutes introducing your company and your story to potential customers is time well spent. This channel works well wherever there is a group of people in a room that —if you pitched them right—would move the needle for your business. This happens to occur more with enterprise and B2B businesses because they’re often at expensive conferences, though Dan has gotten traction from talks he’s given for Clarity (a consumer-focused platform).

SPEAKING ENGAGEMENT STRATEGY You have to get the attention of event organizers to land speaking engagements. Event organizers need to fill time at their events. If you have a good idea for a talk and see an event that aligns with an area of your expertise, simply pitch your talk to the event organizers. If your ideas are solid, they will want you. This process becomes even easier as you become a recognized expert. Steve Barsh, a serial entrepreneur and former CEO of PackLate, has successfully pitched conference organizers to present many times. Rather than pitch them directly on what he wants to talk about, he contacts them and asks them about the ideal topics they want to have speakers cover at an event. Once that is known, he then crafts the perfect pitch: one that hits on key points the organizers want to cover. To determine where you want to speak, make a list of the events in your industry. Different kinds of events have different crowds and different expectations of speakers. There are a few types of events you should be aware of: Premier events are well regarded and attended national or international shows. Often, there will be only a few of these per year in an industry. These events will require much longer lead times to submit a proposal, often six to twelve months. Regional events bring together industry players within a day’s drive. Depending on the event, expect to land a speaking engagement roughly two to four months before the show. Local events draw city residents around a particular topic. As with regional shows, lead times can vary but are usually one to three months before the event. Organizers consider timing, topic, and credibility when selecting a speaker. By establishing yourself as an expert on an appropriate topic and submitting proposals far in advance, you maximize your chances of securing one of the best speaking engagements at the target show. Landing speaking engagements is far easier if you have expert credentials. After all, if you don’t “earn the right” to be onstage, the audience won’t give you the attention you deserve. For example, if you run a popular blog, it becomes much easier for organizers and attendees to find and recognize your expertise.

In addition to industry experience, conference organizers will want to see that you are a decent speaker. If you’re not well known as a speaker, they’ll be hesitant to book you, even for free. Getting valuable early speaking experience is not difficult. Start by speaking for free at coworking spaces, nonprofits, and smaller conferences or events. Use these smaller-scale appearances to refine your talks and build your speaking reputation. The world of event organizers is relatively small, and they pay special attention to who is speaking at events. As a result, you’ll find your number of engagements growing organically. As Dan told us: To become a speaker you have to speak once. If you speak and you’re good, people in the audience will ask you to speak at other events. That’s just how it happens. I’ve never marketed myself as a speaker; it’s not in my bio or anything. What happens is, you speak at a conference, people see it or talk about it, and you get invited to other ones. If you do a good job at smaller events, you can leverage them into talks at larger ones by asking for referrals and using past events as social proof.

SPEAKING ENGAGEMENT TACTICS When you start a talk, the audience is usually thinking about two questions: Why are you important enough to be the one giving a talk? What value can you offer me? These questions will be burning in their minds until you address them, so answer them immediately. For this reason, Dan told us he does his own introductions and highlights how he started and sold his two previous companies (Flowtown and Spheric) for millions. Once you’ve captured the audience’s attention, keep it with a gripping story. All successful talks tell a story. Your story is about what your startup is doing, why you’re doing it, and specifically how you got to where you are or where things are going. Of course, we have only so many captivating stories. That’s why Dan gives the same one or two core talks, only slightly modifying each to fit the audience.

He never does custom talks and always reuses his slides, so his speaking engagements are always well rehearsed and received: I usually figure out who are two target customers that I want to reach, because it’s hard to give more than two good talks. For Clarity, it’s entrepreneurs and potential partners. Try to figure out the two tracks your potential customers might be interested in and try to teach them about that. For us, it’s helping entrepreneurs get great advice and how it’s changed my life, so I have a talk about my entrepreneurial journey and why getting the right advice can change your life. Giving a limited number of talks is helpful in another way: it gives you more practice per talk, which helps you identify spots that may not be clicking with the audience. The more practiced and comfortable you are, the better your talks will be and the more you can improve them. If you want to focus on speaking engagements, we spoke with Dan about a few more advanced tactics you can use. Record your speaking engagements. If you’re at an event of 250 people and you’ve just given your best speech ever, you’ve still reached only 250 people. However, if you can record your best speech ever, then you can post clips, thereby exposing your story to thousands of people who would never have seen it otherwise. Among this group of online viewers will be conference organizers who will book you for future events. Leveraging social media to reach people outside of the conference is a similar tactic. Rand Fishkin of Moz tweets his slides before every presentation, which lets his followers find out what he’ll be talking about. Then, when he posts a video of his talk, there is already some buzz and interest in watching and sharing it. Dan Martell will even try to leverage social media during his talk. He asks for the audience’s “divided attention,” meaning he wants them to tweet and share good content from his presentation as he gives it. To facilitate this, he includes his Twitter handle on every slide and asks people to tweet at him if they really identified with something he said. This way, he can find out the content his audience enjoyed the most, while also growing his reach. On top of asking his audience to tweet and text, Dan also gives the audience a call to action at the end of his presentations. 

This is a simple request of the audience—something like asking them to sign up to a mailing list or to check out a link where they can see his slides. This tactic tells him whether or not members of the audience found the information engaging enough to act on it. We already mentioned that Dan prepares only two talks that he delivers at speaking engagements. But what if one conference asks for a twenty-minute presentation and another asks for sixty? It’s time-consuming to prepare a whole new talk: it’s more efficient to tailor your existing slides to a specific audience or event. As Dan said: The best talks I’ve ever seen are where each slide is essentially a seven-minute story with a beginning, middle, and end. Once you get good at that, and you have these canned slides, you can change a sixty-minute talk to a twenty-minute talk just by taking slides out. The slides for your presentation are an important part of any talk you’ll give. Every slide in your presentation should be engaging. As we mentioned early on, the main driver for being a speaker in the first place is to build relationships. At most conferences there is a speakers’ dinner, where presenters get to meet one another and network. If there isn’t one scheduled, Dan usually takes the liberty of scheduling one. Similar to trade shows, you can also do preparation ahead of time based upon who is likely to attend the event where you are speaking. Get a list of attendees from event organizers and contact people you would like to meet. Tell them exactly when and where you are speaking, and suggest meeting up afterward. Now that they’ve heard you talk they’ll be much more receptive to your pitch. Speaking engagements are one of the few traction channels that can quickly cement your place in an industry. If you give the right talk at the right time to the right people, it can make you a respected industry leader overnight.

TARGETS Remember that you are doing organizers a favor by presenting. Event organizers need to fill time at their events.

Submit authoritative proposals far in advance. Organizers consider timing, topic, and credibility when selecting a speaker. By establishing yourself as an expert on the right topic and submitting proposals far in advance, you maximize your chances of securing one of the best speaking engagements at the target show. Tell a story onstage. Without a story, the audience will lose interest. We suggest telling a story about why you’re doing what you’re doing, and specifically present insights only you can give through your unique position as a startup founder. Make it exciting!

CHAPTER TWENTY-FOUR

Community Building

Community building involves investing in the connections among your customers, fostering those relationships and helping them bring more people into your startup’s circle. We interviewed the founders of reddit, Wikipedia, Stack Exchange, Startup Digest, and Quibb to tell us how they created, grew, and nurtured their communities. You probably know people who won’t stop talking about how helpful Yelp is for choosing a restaurant and about the reviews they’ve submitted. These people are known as community evangelists—passionate customers who tell others about how awesome a product is. Maybe after hearing about Yelp from your friend for a third time, you used the app yourself when looking for a dinner spot on Saturday night. Then you found it so useful that you too became a Yelp evangelist. You started leaving reviews and telling people about them. That’s how evangelists spread the word about a product and help build its community further.

COMMUNITY BUILDING STRATEGY Every individual we interviewed emphasized how helpful it was to have an existing audience to jump-start their community-building efforts. For example, Wikipedia began with a small group of users from the Nupedia user group (an earlier online encyclopedia project). Stack Exchange is a network of high-quality question-and-answer sites, the most famous being Stack Overflow. Joel Spolsky and Jeff Atwood founded the company in 2008. Both were already Internet famous: Joel as the founder of Fog Creek Software, and Jeff as a writer at codinghorror.com. Thanks to their well-trafficked blogs, Jeff and Joel presented their ideas for Stack Overflow to readers who gave them feedback before the site launched.

They even had the community vote on the name for Stack Overflow, and received nearly seven thousand submissions! While this illustrates the power of an existing audience, it is an atypical startup experience. Few startups manage to get seven thousand customers after six months, much less seven thousand votes on the name of a site that doesn’t even exist. However, having an audience is not a prerequisite for building a successful community. Chris McCann started Startup Digest by emailing twenty-two friends in the Bay Area about local tech events. To grow the list more, Chris started giving twenty-second Startup Digest pitches at events he attended. His pitches proved effective: membership grew into the low thousands in a matter of months. Today there are more than 250,000 members of the Startup Digest community, and it all started with those twenty-two friends and bootstrapping off local startup meetups. People want to feel like they’re part of something bigger than themselves. You need to have a mission if you want to build an awesome community. A powerful mission gives your community a shared sense of purpose and motivates them to contribute. As Jeff Atwood said: We had a manifesto, and an idea of what we wanted to accomplish. And people bought into the vision because it was about them being awesome. . . . [It is] about creating something that helps everyone in material and specific ways. It helps you get better at your job, at something you love doing. There was an idealism that people bought into with Stack Exchange, and we were out there talking about it all that time. Being open with your community is the best way to get them to buy into your mission. Jeff and Joel solicited feedback every step of the way, and built the site their community wanted. When Stack Overflow launched, their audience was already excited and had shaped the direction of the site. This resulted in hundreds of customers in the first days, and thousands during the first month. From our interviews we also discovered that it’s critical to foster connections among your community (through forums, events, and user groups). When you encourage your customers to connect around your startup, they feel more cohesive as a community and can come up with ideas that you may not think of yourself. Jeff said that failing to initially allow cross-connections was his biggest mistake in building Stack Overflow: When people ask me what our biggest mistake was in building Stack Overflow I’m glad I don’t have to fudge around with platitudes. I can honestly and openly point to a huge, honking, ridiculously dumb mistake I made from the very first day of development on Stack Overflow. . . . I didn’t see the need for a Meta. Meta is, of course, the place where you go to discuss the place. Take a moment and think about what that means. Meta is for people who care so deeply about their community that they’re willing to go one step further, to come together and spend even more of their time deciding how to maintain and govern it. So, in a nutshell, I was telling the people who loved Stack Overflow the most of all to basically . . . f**k off and go away. Community members love to hear from other members. But they would also love to hear from you. You will want to connect with your evangelists and let them know that you value them. In reddit’s early days, any individual who wrote about reddit would get an email from cofounder Alexis Ohanian thanking them. Alexis also sent shirts, stickers, and other gifts to early users. He went so far as to coordinate an open bar tour for redditors, where redditors connected and drank on reddit’s dime. 

Sending emails and gifts is great, but nothing beats personal interaction. It’s just easier to form a lasting relationship with someone when you’re sharing a laugh, a meal, or a drink. In that way, community building works nicely with other channels like offline events and speaking engagements. These occasions present great opportunities for customers to connect with you and with one another. A challenge with community building as you scale is keeping its quality high. The meaning of quality depends on the service the startup provides. For Yelp, it might be the accuracy of its reviews; Wikipedia, the usefulness of its articles; reddit, the relevance of its links and comments. Stack Overflow wanted to create the best question-and-answer site for developers—a community that truly helped developers get better at their jobs.

Upon launch, Jeff established strict guidelines (decided on in tandem with the community) so that only practical, answerable questions would be allowed. Then he placed these guidelines on the Stack Overflow FAQ. Because these community guidelines were prominently featured on the site, users often policed the site on their own—even more aggressively than Jeff himself would have. Not only did this keep quality high, but it kept members of the community engaged and invested in the future of the site. Everyone we talked to about community building emphasized the importance of maintaining community quality. Wikipedia developed strict guidelines for everything from the types of articles to include on the site to how conflicts of interest should be handled. Startup Digest focused on content selected by community members in each of its cities. Quibb uses an invite-only model to recruit people they feel would be a positive addition to their community. Like Stack Overflow, reddit developed a karma system based on voting that determines what links and comments are displayed prominently. 

Unfortunately, a common occurrence is that the quality of communities starts out strong but gets diluted over time as evangelists either leave or get drowned out by newer community members. This decline in the overall quality of the community causes more good people to leave, which creates a downward spiral from which many communities don’t recover. To prevent this negative cycle, it is important to focus on quality early on and set standards that can be maintained as the community grows. When quality remains high, many communities become an essential asset for the managing company or organization. Consider Wikipedia: Its goal is to compile the world’s knowledge in one place. To reach this goal, it has built the largest group of knowledge contributors and editors ever assembled. Other startups like Yelp and Codecademy have built core groups of customers to accomplish their company goals. Yelp would be nothing without restaurant reviews from its users; many of Codecademy’s programming lessons are user generated as well. Both sites have worked to attract people to their vision (Yelp’s to allow people to discover their neighborhoods; Codecademy’s to teach the world to code), and have thrived by leveraging their users to help accomplish that vision. Like building an asset, your customers can also help you develop your actual product. Not only does this kind of community improve your product, but they will love you for giving them the chance to help.

For software companies, their code is the product. Some companies opensource their code, making it freely available for anyone to use, modify, or improve. Tom Preston-Werner, founder of popular code hosting site GitHub, points out that open-sourcing code generates free advertising and a lot of goodwill. GitHub is beloved by developers everywhere because it allows anyone working on an open source project to use GitHub free of charge. This drove a lot of its early adoption: when a developer wanted to work on a side project, GitHub was the first place that came to mind. Another use of community is for hiring. Everyone working at Gabriel’s startup DuckDuckGo was a member of the DuckDuckGo community first. Hires that come from your community already buy into your mission. These are people you really want on your team—community members who didn’t just believe in your mission, but also took the initiative to help you achieve it. Chris McCann (of Startup Digest) talked about the types of companies that will benefit from community building: I think building a community can be your traction. This is no small thing: it can truly get to crazy proportions on its own. That being said, there are definitely products and services that don’t lend themselves to community building. If I were doing something with advertising and retargeting, it might be hard to build a community around that. There are some businesses that lend themselves to doing this very well. Companies whose core function is the connecting of people are best set up to take advantage of community. Whether that’s a trade show thing, an investment thing, whatever: when a company’s underlying value is in bringing people together, and where people matter in the system, that’s where this community stuff can really take off.

TARGETS Cultivate and empower evangelists. Foster cross-connection among them and among community members in general. Set high standards from the start. Focus on community quality early on and set strict standards that can be maintained as the community grows. You can build tools and processes into your community to help your community police itself. Bootstrap off an existing audience. Find initial evangelists by sharing your mission with complementary communities online and at offline events.

APPENDIX: MIDDLE RING TESTS

Below are some basic middle ring traction tests to get you started in each traction channel. These tests are designed for phase I startups. As explained earlier, middle ring tests in phase I should cost less than a thousand dollars and take less than one month of time. However, please keep in mind that these may not be the best tests for you to run. You might come up with better tests in your Bullseye process. Targeting Blogs—Contact ten niche blogs and try to get them to review your product. To make it really easy for them, offer to walk them through the product (in person if you can find local bloggers or connect with them at events). You can also make the offer even more enticing by giving them the opportunity to give something away to their audience (discounts, T-shirt contest, etc.). Alternatively, you could find blogs that don’t run advertisements and ask several if you could run an advertisement on them for $100/month. Publicity—Contact five relevant local reporters about your company and try to get them to write about you. Local stories are much easier to get written since there is already local interest. Offer to meet them in person to walk through the product. Their phone numbers might be listed on their publication Web sites. Otherwise, try reaching out on Twitter or at events you know they’ll be covering. Unconventional PR—Host a contest around your product. This contest could be as simple as a cash giveaway for creative product usage or as complicated as a game constructed around your product. Once it’s set up, try a bit of both paid media (e.g., Twitter ads) and earned media (e.g., local press and blogs) to promote your contest. Alternatively, try a more creative approach with an infographic or video you think could go viral with your audience. If you have a large incumbent competitor, it could be explaining how they do something poorly in some way (and at the end how you do it better). Search Engine Marketing—Try four ads in Bing Ads (often cheaper than Google AdWords). These ads should be on keywords you’re highly confident will convert into long-term customers. 

Try some of these keywords even if they seem relatively expensive compared with keywords you’re less confident about. You want to figure out in the best-case conversion scenario whether SEM could work. Make sure before you turn them on that you have everything set up correctly to actually detect conversions (and not just clicks to your site). If you can’t automate that, then you can ask new customers how they heard of you (manually if necessary). Social and Display Ads—Try a Facebook or Twitter ad campaign. Use their targeting capabilities to target two niche audiences that you think would really convert well. You can get very specific here, and you should. On Twitter, advertise against Twitter handles you think are directly related to your product (like industry leaders, aggregators, or even competitors). For Facebook, advertise against complementary affinity groups. If there are local areas you have a hunch would work better, for example, certain cities, then restrict your ads further to those areas. Make sure you try a few different images in your ads, as the image can have a major effect on performance. 

Offline Ads—Advertise on a niche podcast. With these advertisements, the host usually reads your copy directly to his listeners. It needs to be niche enough where you think the audience would really like your offer, but still small enough where it is reasonably priced (as podcast ads can get expensive for larger audiences). Alternatively, run a few ads in local papers. Search Engine Optimization—Test a long-tail SEO strategy by making some content-rich pages. Perhaps your product can naturally produce data for these pages, or maybe you have enough data from making and researching your product. Link to these new pages right from your home page (e.g., on the footer), as that will give them the highest rankings. Let relevant people know about your content and see if they’ll repost it with a link back to the original source. Alternatively, test a fat-head SEO strategy by identifying promising fat-head keywords and then running search engine ads to see how effective the traffic may be. This is a very similar basic test to Search Engine Marketing itself, though the keywords may be different. Content Marketing—Start a company blog and write one blog post a week for a month. Promote your posts on Twitter and on link-sharing sites (e.g., reddit). If you see any significant audience growth and conversion, double down and commit to a few more months. Turn on comments for your posts and engage with any commenters. Try to write controversial or surprising posts, ideally using new data you’ve researched. Alternatively, do a couple of guest posts on other blogs.

Email Marketing—Contact ten email newsletters in your niche and advertise on at least two of them where it makes sense financially. If they don’t usually run advertisements in their emails, ask to sponsor the list for a week or month. Alternatively, develop a seven-email mini-course, where you teach something relevant to your product. Make a landing page for the course and drive some traffic to it. At the end of the mini-course, upsell prospective customers to becoming real customers of your product. Viral Marketing—Build a viral loop into your product and measure your viral coefficient and viral cycle time. See which step is the weakest in your viral loop (signup percentage, number of invites, click-through percentage). Run five tests to improve this weakest step and see how it affects your viral coefficient. If it gets near 0.5, then you might be on to something. Engineering as Marketing—Make a simple, free tool tangentially relevant to your company; for example, a calculator of some kind that would be useful to prospective customers. Put it on its own domain and name it something that people would search for. Collect contact information in exchange for using the tool. Reach out to anyone who uses your tool with a personal email about your main product. Business Development—Write down three types of companies that could be useful to yours in terms of partnerships. For example, are there companies with complementary products? Identify some smaller players and reach out to two in each category, six in total. Have conversations with as many as will have them to gauge interest. Try to strike at least one deal. Sales—List twenty local, prospective customers. Try to get warm intros to as many as possible and meet with them in person to discuss your product. Use the SPIN approach we presented in the Sales chapter. Alternatively, reach out cold over email to one hundred prospective customers who you think have a high likelihood of converting into real customers. Affiliate Programs—Register your product at the most relevant major affiliate network (there is a list at the end of the Affiliate Programs chapter). Recruit twenty affiliates from this program using a simple and attractive payout structure. Contact each affiliate personally to walk them through the product, which will greatly increase the chances they will sell effectively. Alternatively, contact existing customers you think might be well connected to prospective customers and strike affiliate deals with them. Existing Platforms—Identify the most relevant niche platform where your audience hangs out online (e.g., Craigslist, Tumblr, etc.). 

Research the best practices for promoting products on that platform and then do so with your product. Try some paid tools or advertising if available for the platform. Alternatively, make a simple browser extension and try to get featured. Trade Shows—Follow the procedure outlined in the Trade Shows chapter to list all the obviously relevant events over the next year. Dig deeper on the next few months to make sure smaller events are on your list. Ask your local startup community if anyone has been to these events. Exhibit at the one that seems most promising. Alternatively, go to a bigger event as an attendee. Offline Events—Put together a one-day mini-conference. Pull together a few regional speakers to speak during the day. Host it at a university, and leverage its resources. You may need to make a professor one of the speakers to make it work. Alternatively, sponsor several local events and ask to speak for a few minutes about what you’re working on at the beginning of the events. Speaking Engagements—Contact three local meetup group organizers relevant to your product and ask if you can speak at an upcoming event. Present your company in the context of your personal story. How did you come to be where you are today? How are you uniquely solving a problem with your product? What are your ambitious plans? Alternatively, pitch a talk at a regional conference. Community Building—Join three online forums where your customers hang out and engage on at least twenty threads on each. Do this over a month so you don’t look spammy. Similarly, don’t just plug your product directly; truly engage as a useful member of the community. Include references to your product where appropriate and in your signature. Alternatively, start putting together your own community using an online forum tool.

